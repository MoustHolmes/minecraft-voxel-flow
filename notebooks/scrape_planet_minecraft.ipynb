{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9928be80",
   "metadata": {},
   "source": [
    "# Planet Minecraft Scraper - Exploration\n",
    "\n",
    "This notebook explores scraping Minecraft maps/schematics from Planet Minecraft.\n",
    "\n",
    "## Key Differences from minecraft-schematics.com:\n",
    "- ‚úÖ **Easier**: No login required, no Cloudflare\n",
    "- ‚ùå **Harder**: No simple ID-based index system\n",
    "- üîó **Complex**: Multiple download sources (direct, MediaFire, Dropbox, Patreon, etc.)\n",
    "\n",
    "## Goals:\n",
    "1. Understand page structure and metadata\n",
    "2. Extract all relevant information (title, description, tags, category, date, etc.)\n",
    "3. Identify download link patterns\n",
    "4. Handle different download sources\n",
    "5. Build a scraper to collect metadata first, download later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe77c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Setup\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "print(\"‚úì Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a1582",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Bot Protection Detected!\n",
    "\n",
    "Planet Minecraft is blocking simple requests. We need to use Selenium to appear more human-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91458fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up browser...\n",
      "‚úì Browser ready\n"
     ]
    }
   ],
   "source": [
    "# Use Selenium instead of requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Setup Firefox with options\n",
    "options = Options()\n",
    "# Don't run headless - site might detect it\n",
    "# options.add_argument('--headless')\n",
    "\n",
    "# Anti-detection\n",
    "options.set_preference(\"dom.webdriver.enabled\", False)\n",
    "options.set_preference('useAutomationExtension', False)\n",
    "\n",
    "print(\"Setting up browser...\")\n",
    "driver = webdriver.Firefox(options=options)\n",
    "print(\"‚úì Browser ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0e574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to: https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/\n",
      "Waiting for page to load...\n",
      "Page title: [1.21.8] GK first church [Download] Minecraft Map\n",
      "‚úì Page loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetch page with Selenium\n",
    "url = example_urls['direct_download']\n",
    "print(f\"Navigating to: {url}\")\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for page to load\n",
    "print(\"Waiting for page to load...\")\n",
    "time.sleep(3)  # Give it a moment\n",
    "\n",
    "# Check if we got through\n",
    "page_title = driver.title\n",
    "print(f\"Page title: {page_title}\")\n",
    "\n",
    "# Get page source\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Check if blocked\n",
    "if \"blocked\" in page_title.lower() or \"sorry\" in page_title.lower():\n",
    "    print(\"‚ùå Still blocked - might need manual intervention\")\n",
    "else:\n",
    "    print(\"‚úì Page loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac733d",
   "metadata": {},
   "source": [
    "## Step 1: Fetch and Examine Example Pages\n",
    "\n",
    "Let's start by fetching the three example URLs you provided and examining their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43f3d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/\n",
      "Status: 403\n",
      "Content length: 5759 bytes\n",
      "‚úì Page parsed successfully\n"
     ]
    }
   ],
   "source": [
    "# Example URLs\n",
    "example_urls = {\n",
    "    'direct_download': 'https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/',\n",
    "    'external_links': 'https://www.planetminecraft.com/project/the-great-kingdoms/',\n",
    "    'paid_patreon': 'https://www.planetminecraft.com/project/fantasy-blue-house/'\n",
    "}\n",
    "\n",
    "# Fetch the first example (direct download)\n",
    "url = example_urls['direct_download']\n",
    "print(f\"Fetching: {url}\")\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Content length: {len(response.content)} bytes\")\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(f\"‚úì Page parsed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17fa5a",
   "metadata": {},
   "source": [
    "## Step 2: Extract Title and Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258f2cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: [1.21.8] GK first church [Download]\n",
      "No breadcrumb found\n",
      "Meta category: article\n"
     ]
    }
   ],
   "source": [
    "# Extract title\n",
    "title_elem = soup.find('h1')\n",
    "title = title_elem.get_text(strip=True) if title_elem else 'N/A'\n",
    "print(f\"Title: {title}\")\n",
    "\n",
    "# Extract category (breadcrumb navigation)\n",
    "breadcrumb = soup.find('ol', class_='breadcrumb')\n",
    "if breadcrumb:\n",
    "    categories = [a.get_text(strip=True) for a in breadcrumb.find_all('a')]\n",
    "    category_full = ' / '.join(categories)\n",
    "    print(f\"Category: {category_full}\")\n",
    "else:\n",
    "    print(\"No breadcrumb found\")\n",
    "    \n",
    "# Look for category in meta tags\n",
    "meta_category = soup.find('meta', {'property': 'og:type'})\n",
    "if meta_category:\n",
    "    print(f\"Meta category: {meta_category.get('content')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a277196",
   "metadata": {},
   "source": [
    "## Step 3: Extract Posted Date\n",
    "\n",
    "The date is shown as relative time (e.g., \"yesterday\", \"3 weeks ago\"). Let's find how it's stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae85939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 time elements:\n",
      "\n",
      "Found 'Published' text: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"CreativeWork\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\/\\/www.planetminecraft.com\\/project\\/1-21-8-gk-first-church-download\\/\"},\"url\":\"https:\\/\\/www.planetminecraft.com\\/project\\/1-21-8-gk-first-church-download\\/\",\"name\":\"[1.21.8] GK first church [Download] Minecraft Map\",\"description\":\"Church first Graveyard Keeper This is a small, beginner level medieval church. The inspiration came from the church in the Graveyard Keeper game. I...\",\"image\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/static.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/813\\/19208273-x_l.jpg\",\"name\":\"[1.21.8] GK first church [Download] Minecraft Map\",\"width\":\"800px\",\"height\":\"689px\"},\"thumbnailUrl\":\"https:\\/\\/www.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/813\\/19208273-x_l.jpg\",\"genre\":\"Map\",\"headline\":\"[1.21.8] GK first church [Download] Minecraft Map\",\"dateCreated\":\"2025-10-08T00:00:00-04:00\",\"datePublished\":\"2025-10-08T00:00:00-04:00\",\"dateModified\":\"2025-10-08T03:00:02-04:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Psemata\",\"image\":\"2738336_3.jpg\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/member\\/psemata\",\"sameAs\":[\"mezeskeksz\",\"https:\\/\\/www.sketchfab.com\\/Monratira\",\"https:\\/\\/www.paypal.me\\/Monratira\"]},\"publisher\":{\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"Organization\",\"@id\":\"#organization\",\"url\":\"https:\\/\\/www.planetminecraft.com\",\"name\":\"Planet Minecraft\",\"sameAs\":[\"https:\\/\\/www.twitter.com\\/PlanetMinecraft\",\"https:\\/\\/www.facebook.com\\/PlanetMinecraft\",\"https:\\/\\/www.instagram.com\\/PlanetMinecraftOfficial\"],\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/images\\/layout\\/themes\\/modern\\/planetminecraft_logo.png\",\"width\":\"222px\",\"height\":\"72px\"}},\"commentCount\":0,\"isFamilyFriendly\":true,\"keywords\":\"medieval,land-structure,download,church,temple,ruined,downloadable,graveyardkeeper\"}\n",
      "Parent element: script\n",
      "Parent text: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"CreativeWork\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\/\\/www.planetminecraft.com\\/project\\/1-21-8-gk-first-church-download\\/\"},\"url\":\"https:\\/\\/www.planetminecraft.com\\/project\\/1-21-8-gk-first-church-download\\/\",\"name\":\"[1.21.8] GK first church [Download] Minecraft Map\",\"description\":\"Church first Graveyard Keeper This is a small, beginner level medieval church. The inspiration came from the church in the Graveyard Keeper game. I...\",\"image\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/static.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/813\\/19208273-x_l.jpg\",\"name\":\"[1.21.8] GK first church [Download] Minecraft Map\",\"width\":\"800px\",\"height\":\"689px\"},\"thumbnailUrl\":\"https:\\/\\/www.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/813\\/19208273-x_l.jpg\",\"genre\":\"Map\",\"headline\":\"[1.21.8] GK first church [Download] Minecraft Map\",\"dateCreated\":\"2025-10-08T00:00:00-04:00\",\"datePublished\":\"2025-10-08T00:00:00-04:00\",\"dateModified\":\"2025-10-08T03:00:02-04:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Psemata\",\"image\":\"2738336_3.jpg\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/member\\/psemata\",\"sameAs\":[\"mezeskeksz\",\"https:\\/\\/www.sketchfab.com\\/Monratira\",\"https:\\/\\/www.paypal.me\\/Monratira\"]},\"publisher\":{\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"Organization\",\"@id\":\"#organization\",\"url\":\"https:\\/\\/www.planetminecraft.com\",\"name\":\"Planet Minecraft\",\"sameAs\":[\"https:\\/\\/www.twitter.com\\/PlanetMinecraft\",\"https:\\/\\/www.facebook.com\\/PlanetMinecraft\",\"https:\\/\\/www.instagram.com\\/PlanetMinecraftOfficial\"],\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/images\\/layout\\/themes\\/modern\\/planetminecraft_logo.png\",\"width\":\"222px\",\"height\":\"72px\"}},\"commentCount\":0,\"isFamilyFriendly\":true,\"keywords\":\"medieval,land-structure,download,church,temple,ruined,downloadable,graveyardkeeper\"}\n"
     ]
    }
   ],
   "source": [
    "# Look for time elements\n",
    "time_elements = soup.find_all('time')\n",
    "print(f\"Found {len(time_elements)} time elements:\\n\")\n",
    "\n",
    "for i, time_elem in enumerate(time_elements):\n",
    "    print(f\"Time element {i+1}:\")\n",
    "    print(f\"  Text: {time_elem.get_text(strip=True)}\")\n",
    "    print(f\"  datetime attribute: {time_elem.get('datetime')}\")\n",
    "    print(f\"  title attribute: {time_elem.get('title')}\")\n",
    "    print()\n",
    "\n",
    "# Also look for \"Published\" text\n",
    "published_text = soup.find(string=re.compile(r'Published|posted on', re.IGNORECASE))\n",
    "if published_text:\n",
    "    print(f\"Found 'Published' text: {published_text.strip()}\")\n",
    "    # Get parent element\n",
    "    parent = published_text.parent\n",
    "    print(f\"Parent element: {parent.name}\")\n",
    "    print(f\"Parent text: {parent.get_text(strip=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d1813",
   "metadata": {},
   "source": [
    "## Step 4: Extract Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40aad453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta description:\n",
      "Church first Graveyard Keeper This is a small, beginner level medieval church. The inspiration came from the church in the Graveyard Keeper game. I......\n",
      "\n",
      "Found 18 potential content divs\n",
      "\n",
      "No obvious description section found, exploring structure...\n"
     ]
    }
   ],
   "source": [
    "# Look for description in meta tags first\n",
    "meta_description = soup.find('meta', {'property': 'og:description'})\n",
    "if meta_description:\n",
    "    print(\"Meta description:\")\n",
    "    print(meta_description.get('content')[:200] + \"...\")\n",
    "    print()\n",
    "\n",
    "# Look for main content div\n",
    "content_divs = soup.find_all('div', class_=re.compile(r'content|description|body', re.IGNORECASE))\n",
    "print(f\"Found {len(content_divs)} potential content divs\")\n",
    "print()\n",
    "\n",
    "# Try to find the main project description\n",
    "# Usually in a section with class containing \"description\" or similar\n",
    "description_section = soup.find('section', class_=re.compile(r'description|content'))\n",
    "if description_section:\n",
    "    print(\"Description section found:\")\n",
    "    print(description_section.get_text(strip=True)[:300] + \"...\")\n",
    "else:\n",
    "    print(\"No obvious description section found, exploring structure...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7cffc",
   "metadata": {},
   "source": [
    "## Step 5: Extract Tags\n",
    "\n",
    "Tags are crucial metadata. Let's find where they're stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "955983d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 tag links:\n",
      "\n",
      "\n",
      "All tags: []\n",
      "\n",
      "Meta keywords: [1.21.8] GK first church [Download] Minecraft Map, Minecraft Map, medieval,land-structure,download,church,temple,ruined,downloadable,graveyardkeeper\n"
     ]
    }
   ],
   "source": [
    "# Look for tag elements\n",
    "tag_links = soup.find_all('a', class_=re.compile(r'tag', re.IGNORECASE))\n",
    "print(f\"Found {len(tag_links)} tag links:\\n\")\n",
    "\n",
    "tags = []\n",
    "for tag_link in tag_links[:15]:  # Show first 15\n",
    "    tag_text = tag_link.get_text(strip=True)\n",
    "    tag_href = tag_link.get('href', '')\n",
    "    tags.append(tag_text)\n",
    "    print(f\"  ‚Ä¢ {tag_text} ‚Üí {tag_href}\")\n",
    "\n",
    "print(f\"\\nAll tags: {tags}\")\n",
    "\n",
    "# Also check meta keywords\n",
    "meta_keywords = soup.find('meta', {'name': 'keywords'})\n",
    "if meta_keywords:\n",
    "    print(f\"\\nMeta keywords: {meta_keywords.get('content')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b20a6",
   "metadata": {},
   "source": [
    "## Step 6: Find Download Links (MOST IMPORTANT!)\n",
    "\n",
    "This is the critical part - we need to identify:\n",
    "1. Direct download links (hosted on Planet Minecraft)\n",
    "2. External links (MediaFire, Dropbox, etc.)\n",
    "3. Paid/Patreon links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ba36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links on page: 231\n",
      "\n",
      "Found 23 potential download links:\n",
      "\n",
      "1. Text: Download Schematic\n",
      "   URL: /project/1-21-8-gk-first-church-download/download/schematic/\n",
      "   Classes: branded-download tooltip tipso_style\n",
      "\n",
      "2. Text: Download\n",
      "   URL: /projects/tag/download/\n",
      "   Classes: \n",
      "\n",
      "3. Text: Downloadable\n",
      "   URL: /projects/tag/downloadable/\n",
      "   Classes: \n",
      "\n",
      "4. Text: \n",
      "   URL: /project/1-21-8-blood-eagle-download/\n",
      "   Classes: \n",
      "\n",
      "5. Text: [1.21.8] Blood Eagle [Download]\n",
      "   URL: /project/1-21-8-blood-eagle-download/\n",
      "   Classes: r-title\n",
      "\n",
      "6. Text: VIEW\n",
      "   URL: /project/1-21-8-blood-eagle-download/\n",
      "   Classes: \n",
      "\n",
      "7. Text: \n",
      "   URL: /project/1-21-8-gk-first-church-download/\n",
      "   Classes: \n",
      "\n",
      "8. Text: [1.21.8] GK first church [Download]\n",
      "   URL: /project/1-21-8-gk-first-church-download/\n",
      "   Classes: r-title\n",
      "\n",
      "9. Text: VIEW\n",
      "   URL: /project/1-21-8-gk-first-church-download/\n",
      "   Classes: \n",
      "\n",
      "10. Text: \n",
      "   URL: /project/1-20-8-rock-pack-x10-download/\n",
      "   Classes: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for all links\n",
    "all_links = soup.find_all('a', href=True)\n",
    "print(f\"Total links on page: {len(all_links)}\\n\")\n",
    "\n",
    "# Filter for download-related links\n",
    "download_links = []\n",
    "for link in all_links:\n",
    "    href = link.get('href', '')\n",
    "    text = link.get_text(strip=True).lower()\n",
    "    classes = ' '.join(link.get('class', []))\n",
    "    \n",
    "    # Look for download indicators\n",
    "    if any(keyword in text for keyword in ['download', 'get', 'access']):\n",
    "        download_links.append({\n",
    "            'text': link.get_text(strip=True),\n",
    "            'href': href,\n",
    "            'classes': classes\n",
    "        })\n",
    "    elif 'download' in href.lower():\n",
    "        download_links.append({\n",
    "            'text': link.get_text(strip=True),\n",
    "            'href': href,\n",
    "            'classes': classes\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(download_links)} potential download links:\\n\")\n",
    "for i, dl in enumerate(download_links[:10]):  # Show first 10\n",
    "    print(f\"{i+1}. Text: {dl['text']}\")\n",
    "    print(f\"   URL: {dl['href']}\")\n",
    "    print(f\"   Classes: {dl['classes']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65fb7f9",
   "metadata": {},
   "source": [
    "## Step 7: Classify Download Link Types\n",
    "\n",
    "Now let's classify the download links by their source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a734a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download links by type:\n",
      "\n",
      "other: 23 link(s)\n",
      "  ‚Ä¢ Download Schematic ‚Üí /project/1-21-8-gk-first-church-download/download/schematic/\n",
      "  ‚Ä¢ Download ‚Üí /projects/tag/download/\n",
      "  ... and 21 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify_download_link(href):\n",
    "    \"\"\"Classify download link by source.\"\"\"\n",
    "    href_lower = href.lower()\n",
    "    \n",
    "    if 'planetminecraft.com' in href_lower and '/data_' in href_lower:\n",
    "        return 'direct_pm'  # Direct Planet Minecraft hosted file\n",
    "    elif 'mediafire.com' in href_lower:\n",
    "        return 'mediafire'\n",
    "    elif 'dropbox.com' in href_lower:\n",
    "        return 'dropbox'\n",
    "    elif 'patreon.com' in href_lower:\n",
    "        return 'patreon_paid'\n",
    "    elif 'drive.google.com' in href_lower or 'docs.google.com' in href_lower:\n",
    "        return 'google_drive'\n",
    "    elif 'mega.nz' in href_lower:\n",
    "        return 'mega'\n",
    "    elif 'github.com' in href_lower:\n",
    "        return 'github'\n",
    "    elif 'planetminecraft.com' in href_lower:\n",
    "        return 'pm_redirect'  # Planet Minecraft page (might redirect)\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Classify the download links\n",
    "classified = {}\n",
    "for dl in download_links:\n",
    "    link_type = classify_download_link(dl['href'])\n",
    "    if link_type not in classified:\n",
    "        classified[link_type] = []\n",
    "    classified[link_type].append(dl)\n",
    "\n",
    "print(\"Download links by type:\\n\")\n",
    "for link_type, links in classified.items():\n",
    "    print(f\"{link_type}: {len(links)} link(s)\")\n",
    "    for link in links[:2]:  # Show first 2 of each type\n",
    "        print(f\"  ‚Ä¢ {link['text'][:50]} ‚Üí {link['href'][:80]}\")\n",
    "    if len(links) > 2:\n",
    "        print(f\"  ... and {len(links) - 2} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e17b5",
   "metadata": {},
   "source": [
    "## Step 8: Test on Other Example Pages\n",
    "\n",
    "Let's fetch and examine the other two examples to see different download patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on external links example (MediaFire/Dropbox)\n",
    "url_external = example_urls['external_links']\n",
    "print(f\"Fetching: {url_external}\\n\")\n",
    "\n",
    "response_external = requests.get(url_external, headers=headers)\n",
    "soup_external = BeautifulSoup(response_external.content, 'html.parser')\n",
    "\n",
    "# Find download links\n",
    "external_links = []\n",
    "for link in soup_external.find_all('a', href=True):\n",
    "    href = link.get('href', '')\n",
    "    text = link.get_text(strip=True).lower()\n",
    "    \n",
    "    if any(keyword in text for keyword in ['download', 'get', 'access']) or 'download' in href.lower():\n",
    "        external_links.append({\n",
    "            'text': link.get_text(strip=True),\n",
    "            'href': href,\n",
    "            'type': classify_download_link(href)\n",
    "        })\n",
    "\n",
    "print(\"Download links found:\")\n",
    "for link in external_links[:10]:\n",
    "    print(f\"  Type: {link['type']}\")\n",
    "    print(f\"  Text: {link['text'][:50]}\")\n",
    "    print(f\"  URL: {link['href'][:80]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9833f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Patreon example\n",
    "url_patreon = example_urls['paid_patreon']\n",
    "print(f\"Fetching: {url_patreon}\\n\")\n",
    "\n",
    "response_patreon = requests.get(url_patreon, headers=headers)\n",
    "soup_patreon = BeautifulSoup(response_patreon.content, 'html.parser')\n",
    "\n",
    "# Find download links\n",
    "patreon_links = []\n",
    "for link in soup_patreon.find_all('a', href=True):\n",
    "    href = link.get('href', '')\n",
    "    text = link.get_text(strip=True).lower()\n",
    "    \n",
    "    if any(keyword in text for keyword in ['download', 'get', 'access', 'patreon']) or 'download' in href.lower():\n",
    "        patreon_links.append({\n",
    "            'text': link.get_text(strip=True),\n",
    "            'href': href,\n",
    "            'type': classify_download_link(href)\n",
    "        })\n",
    "\n",
    "print(\"Download links found:\")\n",
    "for link in patreon_links[:10]:\n",
    "    print(f\"  Type: {link['type']}\")\n",
    "    print(f\"  Text: {link['text'][:50]}\")\n",
    "    print(f\"  URL: {link['href'][:80]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a31c6",
   "metadata": {},
   "source": [
    "## Step 9: Extract Additional Metadata\n",
    "\n",
    "Let's extract other useful information like file size, version, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to first example\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Look for stats/info sections\n",
    "print(\"Looking for additional metadata:\\n\")\n",
    "\n",
    "# Search for common patterns\n",
    "metadata_patterns = ['version', 'size', 'file type', 'downloads', 'views', 'favorites']\n",
    "\n",
    "for pattern in metadata_patterns:\n",
    "    # Look for labels/spans containing these words\n",
    "    elements = soup.find_all(string=re.compile(pattern, re.IGNORECASE))\n",
    "    if elements:\n",
    "        print(f\"\\n{pattern.upper()}:\")\n",
    "        for elem in elements[:3]:  # Show first 3\n",
    "            parent = elem.parent\n",
    "            # Try to get the value (next sibling or parent text)\n",
    "            if parent:\n",
    "                print(f\"  {parent.get_text(strip=True)[:100]}\")\n",
    "\n",
    "# Look for structured data (JSON-LD)\n",
    "json_ld = soup.find('script', type='application/ld+json')\n",
    "if json_ld:\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"JSON-LD Structured Data Found!\")\n",
    "    print(\"=\"*60)\n",
    "    try:\n",
    "        import json\n",
    "        data = json.loads(json_ld.string)\n",
    "        print(json.dumps(data, indent=2)[:500] + \"...\")\n",
    "    except:\n",
    "        print(\"Could not parse JSON-LD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a118088",
   "metadata": {},
   "source": [
    "## Step 10: Build a Complete Metadata Extractor Function\n",
    "\n",
    "Now let's create a function that extracts all metadata from a Planet Minecraft project page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "846436e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing metadata extractor on first example:\n",
      "\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/\",\n",
      "  \"scraped_at\": \"2025-10-09T15:10:37.492166\",\n",
      "  \"title\": \"[1.21.8] GK first church [Download]\",\n",
      "  \"category\": \"N/A\",\n",
      "  \"subcategory\": \"N/A\",\n",
      "  \"posted_date\": \"N/A\",\n",
      "  \"posted_date_relative\": \"N/A\",\n",
      "  \"description\": \"Church first Graveyard Keeper This is a small, beginner level medieval church. The inspiration came from the church in the Graveyard Keeper game. I...\",\n",
      "  \"tags\": [],\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"Download Schematic\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/download/schematic/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Download\",\n",
      "      \"url\": \"/projects/tag/download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Downloadable\",\n",
      "      \"url\": \"/projects/tag/downloadable/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-blood-eagle-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Blood Eagle [Download]\",\n",
      "      \"url\": \"/project/1-21-8-blood-eagle-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-blood-eagle-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] GK first church [Download]\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Rock Pack - x10 [Download]\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-horror-tree-with-face-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Horror Tree with face [Download]\",\n",
      "      \"url\": \"/project/1-21-8-horror-tree-with-face-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-horror-tree-with-face-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-lava-tree-s-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Lava Tree/s [Download]\",\n",
      "      \"url\": \"/project/1-21-8-lava-tree-s-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-lava-tree-s-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\\ud83c\\udf42 Fall Festival Quest Discussion \\ud83c\\udf3bbeach_access\",\n",
      "      \"url\": \"/forums/pmc/events/fall-festival-quest-discussion-700741/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Rock Pack - x10 [Download]\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"get_appInstall PMC APP\",\n",
      "      \"url\": \"#confirm\",\n",
      "      \"type\": \"other\"\n",
      "    }\n",
      "  ],\n",
      "  \"download_count\": 23,\n",
      "  \"primary_download_type\": \"other\",\n",
      "  \"has_direct_download\": false,\n",
      "  \"has_external_download\": false,\n",
      "  \"is_paid\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_project_metadata(url, soup=None):\n",
    "    \"\"\"\n",
    "    Extract all metadata from a Planet Minecraft project page.\n",
    "    \n",
    "    Args:\n",
    "        url: The project URL\n",
    "        soup: Optional pre-fetched BeautifulSoup object\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata including title, category, tags, description, download links, etc.\n",
    "    \"\"\"\n",
    "    if soup is None:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    metadata = {\n",
    "        'url': url,\n",
    "        'scraped_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Title\n",
    "    title_elem = soup.find('h1')\n",
    "    metadata['title'] = title_elem.get_text(strip=True) if title_elem else 'N/A'\n",
    "    \n",
    "    # Category from breadcrumb\n",
    "    breadcrumb = soup.find('ol', class_='breadcrumb')\n",
    "    if breadcrumb:\n",
    "        categories = [a.get_text(strip=True) for a in breadcrumb.find_all('a')]\n",
    "        metadata['category'] = ' / '.join(categories)\n",
    "        metadata['subcategory'] = categories[-1] if categories else 'N/A'\n",
    "    else:\n",
    "        metadata['category'] = 'N/A'\n",
    "        metadata['subcategory'] = 'N/A'\n",
    "    \n",
    "    # Posted date\n",
    "    time_elem = soup.find('time', {'datetime': True})\n",
    "    if time_elem:\n",
    "        metadata['posted_date'] = time_elem.get('datetime')\n",
    "        metadata['posted_date_relative'] = time_elem.get_text(strip=True)\n",
    "    else:\n",
    "        metadata['posted_date'] = 'N/A'\n",
    "        metadata['posted_date_relative'] = 'N/A'\n",
    "    \n",
    "    # Description\n",
    "    meta_description = soup.find('meta', {'property': 'og:description'})\n",
    "    if meta_description:\n",
    "        metadata['description'] = meta_description.get('content', 'N/A')\n",
    "    else:\n",
    "        metadata['description'] = 'N/A'\n",
    "    \n",
    "    # Tags\n",
    "    tag_links = soup.find_all('a', class_=re.compile(r'tag', re.IGNORECASE))\n",
    "    metadata['tags'] = [tag.get_text(strip=True) for tag in tag_links]\n",
    "    \n",
    "    # Download links\n",
    "    download_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href', '')\n",
    "        text = link.get_text(strip=True).lower()\n",
    "        \n",
    "        if any(keyword in text for keyword in ['download', 'get', 'access']) or 'download' in href.lower():\n",
    "            download_links.append({\n",
    "                'text': link.get_text(strip=True),\n",
    "                'url': href,\n",
    "                'type': classify_download_link(href)\n",
    "            })\n",
    "    \n",
    "    metadata['download_links'] = download_links\n",
    "    metadata['download_count'] = len(download_links)\n",
    "    \n",
    "    # Classify primary download type\n",
    "    if download_links:\n",
    "        types = [dl['type'] for dl in download_links]\n",
    "        metadata['primary_download_type'] = types[0] if types else 'none'\n",
    "        metadata['has_direct_download'] = 'direct_pm' in types\n",
    "        metadata['has_external_download'] = any(t in types for t in ['mediafire', 'dropbox', 'google_drive', 'mega'])\n",
    "        metadata['is_paid'] = 'patreon_paid' in types\n",
    "    else:\n",
    "        metadata['primary_download_type'] = 'none'\n",
    "        metadata['has_direct_download'] = False\n",
    "        metadata['has_external_download'] = False\n",
    "        metadata['is_paid'] = False\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Test it\n",
    "print(\"Testing metadata extractor on first example:\\n\")\n",
    "test_metadata = extract_project_metadata(example_urls['direct_download'], soup)\n",
    "\n",
    "# Pretty print the results\n",
    "import json\n",
    "print(json.dumps(test_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52068fd1",
   "metadata": {},
   "source": [
    "## ‚úÖ Key Findings Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Bot Protection**: Planet Minecraft blocks simple `requests` - **must use Selenium**\n",
    "\n",
    "2. **JSON-LD Structured Data** üéØ: \n",
    "   - The page has `<script type=\"application/ld+json\">` with ALL metadata!\n",
    "   - Contains: title, description, **datePublished**, author, **keywords (tags)**\n",
    "   - Much easier than scraping HTML!\n",
    "\n",
    "3. **Direct Download Pattern**:\n",
    "   - Format: `/project/{slug}/download/schematic/`\n",
    "   - Example: `/project/1-21-8-gk-first-church-download/download/schematic/`\n",
    "\n",
    "4. **Tags from JSON-LD**:\n",
    "   - In the `keywords` field: `\"medieval,land-structure,download,church,temple,ruined,downloadable,graveyardkeeper\"`\n",
    "   - Just split by comma!\n",
    "\n",
    "5. **Date Format**:\n",
    "   - Exact timestamp in JSON-LD: `\"datePublished\":\"2025-10-08T00:00:00-04:00\"`\n",
    "   - No need to parse relative dates!\n",
    "\n",
    "### Next Steps:\n",
    "1. Update extractor to parse JSON-LD first (easiest!)\n",
    "2. Improve download link classification\n",
    "3. Figure out how to discover all projects (no ID-based system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b39b0",
   "metadata": {},
   "source": [
    "## Step 11: Parse JSON-LD for ALL Metadata\n",
    "\n",
    "The JSON-LD has everything we need! Let's extract it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e2aa732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found JSON-LD script tag\n",
      "\n",
      "Extracted data from JSON-LD:\n",
      "Title: None\n",
      "Description: None\n",
      "Date Published: None\n",
      "Date Modified: None\n",
      "Author: None\n",
      "Genre: None\n",
      "\n",
      "Tags (1): ['']\n",
      "\n",
      "============================================================\n",
      "Full JSON-LD:\n",
      "============================================================\n",
      "{\n",
      "  \"@context\": \"http://schema.org/\",\n",
      "  \"@type\": \"BreadcrumbList\",\n",
      "  \"itemListElement\": [\n",
      "    {\n",
      "      \"@type\": \"ListItem\",\n",
      "      \"position\": 1,\n",
      "      \"item\": {\n",
      "        \"@id\": \"https://www.planetminecraft.com/projects/\",\n",
      "        \"name\": \"Minecraft Maps\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"@type\": \"ListItem\",\n",
      "      \"position\": 2,\n",
      "      \"item\": {\n",
      "        \"@id\": \"https://www.planetminecraft.com/projects/1-21-8-gk-first-church-download/\",\n",
      "        \"name\": \"[1.21.8] GK first church [Download] Minecraft Map\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Find and parse JSON-LD\n",
    "json_ld_script = soup.find('script', type='application/ld+json')\n",
    "\n",
    "if json_ld_script:\n",
    "    print(\"‚úì Found JSON-LD script tag\\n\")\n",
    "    \n",
    "    # Parse the JSON\n",
    "    json_data = json.loads(json_ld_script.string)\n",
    "    \n",
    "    # Extract all the fields\n",
    "    print(\"Extracted data from JSON-LD:\")\n",
    "    print(f\"Title: {json_data.get('name')}\")\n",
    "    print(f\"Description: {json_data.get('description')}\")\n",
    "    print(f\"Date Published: {json_data.get('datePublished')}\")\n",
    "    print(f\"Date Modified: {json_data.get('dateModified')}\")\n",
    "    print(f\"Author: {json_data.get('author', {}).get('name')}\")\n",
    "    print(f\"Genre: {json_data.get('genre')}\")  # This is the category!\n",
    "    \n",
    "    # Keywords = Tags\n",
    "    keywords = json_data.get('keywords', '')\n",
    "    tags_list = [tag.strip() for tag in keywords.split(',')]\n",
    "    print(f\"\\nTags ({len(tags_list)}): {tags_list}\")\n",
    "    \n",
    "    # Pretty print full JSON\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Full JSON-LD:\")\n",
    "    print(\"=\"*60)\n",
    "    print(json.dumps(json_data, indent=2))\n",
    "else:\n",
    "    print(\"‚ùå No JSON-LD found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1747b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 JSON-LD script tags\n",
      "\n",
      "============================================================\n",
      "JSON-LD #1:\n",
      "============================================================\n",
      "Type: BreadcrumbList\n",
      "{\n",
      "  \"@context\": \"http://schema.org/\",\n",
      "  \"@type\": \"BreadcrumbList\",\n",
      "  \"itemListElement\": [\n",
      "    {\n",
      "      \"@type\": \"ListItem\",\n",
      "      \"position\": 1,\n",
      "      \"item\": {\n",
      "        \"@id\": \"https://www.planetminecraft.com/projects/\",\n",
      "        \"name\": \"Minecraft Maps\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"@type\": \"ListItem\",\n",
      "      \"position\": 2,\n",
      "      \"item\": {\n",
      "        \"@id\": \"https://www.planetminecraft.com/projects/1-21-8-gk-first-church-download/\",\n",
      "        \"name\": \"[1.21.8] GK first church [Download] Minecraft Map\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "============================================================\n",
      "JSON-LD #2:\n",
      "============================================================\n",
      "Type: CreativeWork\n",
      "‚úì This is the one we want!\n",
      "  Title: [1.21.8] GK first church [Download] Minecraft Map\n",
      "  Published: 2025-10-08T00:00:00-04:00\n",
      "  Genre: Map\n",
      "  Keywords: medieval,land-structure,download,church,temple,rui...\n",
      "{\n",
      "  \"@context\": \"http://schema.org/\",\n",
      "  \"@type\": \"CreativeWork\",\n",
      "  \"mainEntityOfPage\": {\n",
      "    \"@type\": \"WebPage\",\n",
      "    \"@id\": \"https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/\"\n",
      "  },\n",
      "  \"url\": \"https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/\",\n",
      "  \"name\": \"[1.21.8] GK first church [Download] Minecraft Map\",\n",
      "  \"description\": \"Church first Graveyard Keeper This is a small, beginner level medieval church. The inspiration came from the church in the Graveyard Keeper game. I...\",\n",
      "  \"image\": {\n",
      "    \"@type\": \"ImageObject\",\n",
      "    \"url\": \"https://static.planetminecraft.com/files/image/minecraft/project/2025/813/19208273-x_l.jpg\",\n",
      "    \"name\": \"[1.21.8] GK first church [Download] Minecraft Map\",\n",
      "    \"width\": \"800px\",\n",
      "    \"height\": \"689px\"\n",
      "  },\n",
      "  \"thumbnailUrl\": \"https://www.planetminecraft.com/files/image/minecraft/project/2025/813/19208273-x_l.jpg\",\n",
      "  \"genre\": \"Map\",\n",
      "  \"headline\": \"[1.21.8] GK first church [Download] Minecraft Map\",\n",
      "  \"dateCreated\": \"2025-10-08T00:00:00-04:00\",\n",
      "  \"datePublished\": \"2025-10-08T00:00:00-04:00\",\n",
      "  \"dateModified\": \"2025-10-08T03:00:02-04:00\",\n",
      "  \"author\": {\n",
      "    \"@type\": \"Person\",\n",
      "    \"name\": \"Psemata\",\n",
      "    \"image\": \"2738336_3.jpg\",\n",
      "    \"url\": \"https://www.planetminecraft.com/member/psemata\",\n",
      "    \"sameAs\": [\n",
      "      \"mezeskeksz\",\n",
      "      \"https://www.sketchfab.com/Monratira\",\n",
      "      \"https://www.paypal.me/Monratira\"\n",
      "    ]\n",
      "  },\n",
      "  \"publisher\": {\n",
      "    \"@context\": \"http://schema.org/\",\n",
      "    \"@type\": \"Organization\",\n",
      "    \"@id\": \"#organization\",\n",
      "    \"url\": \"https://www.planetminecraft.com\",\n",
      "    \"name\": \"Planet Minecraft\",\n",
      "    \"sameAs\": [\n",
      "      \"https://www.twitter.com/PlanetMinecraft\",\n",
      "      \"https://www.facebook.com/PlanetMinecraft\",\n",
      "      \"https://www.instagram.com/PlanetMinecraftOfficial\"\n",
      "    ],\n",
      "    \"logo\": {\n",
      "      \"@type\": \"ImageObject\",\n",
      "      \"url\": \"https://www.planetminecraft.com/images/layout/themes/modern/planetminecraft_logo.png\",\n",
      "      \"width\": \"222px\",\n",
      "      \"height\": \"72px\"\n",
      "    }\n",
      "  },\n",
      "  \"commentCount\": 0,\n",
      "  \"isFamilyFriendly\": true,\n",
      "  \"keywords\": \"medieval,land-structure,download,church,temple,ruined,downloadable,graveyardkeeper\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find ALL JSON-LD script tags\n",
    "json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "\n",
    "print(f\"Found {len(json_ld_scripts)} JSON-LD script tags\\n\")\n",
    "\n",
    "for i, script in enumerate(json_ld_scripts, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"JSON-LD #{i}:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        json_data = json.loads(script.string)\n",
    "        json_type = json_data.get('@type')\n",
    "        \n",
    "        print(f\"Type: {json_type}\")\n",
    "        \n",
    "        # Show preview of what this contains\n",
    "        if json_type == 'CreativeWork':\n",
    "            print(f\"‚úì This is the one we want!\")\n",
    "            print(f\"  Title: {json_data.get('name')}\")\n",
    "            print(f\"  Published: {json_data.get('datePublished')}\")\n",
    "            print(f\"  Genre: {json_data.get('genre')}\")\n",
    "            print(f\"  Keywords: {json_data.get('keywords')[:50]}...\")\n",
    "        \n",
    "        # Print full JSON\n",
    "        print(json.dumps(json_data, indent=2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing: {e}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624648a",
   "metadata": {},
   "source": [
    "## Step 12: Updated Metadata Extractor with JSON-LD Parsing\n",
    "\n",
    "Now let's update the extraction function to parse JSON-LD first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19833559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "UPDATED METADATA EXTRACTION:\n",
      "============================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/\",\n",
      "  \"title\": \"[1.21.8] GK first church [Download] Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"N/A\",\n",
      "  \"posted_date\": \"2025-10-08T00:00:00-04:00\",\n",
      "  \"tags\": [\n",
      "    \"medieval\",\n",
      "    \"land-structure\",\n",
      "    \"download\",\n",
      "    \"church\",\n",
      "    \"temple\",\n",
      "    \"ruined\",\n",
      "    \"downloadable\",\n",
      "    \"graveyardkeeper\"\n",
      "  ],\n",
      "  \"description\": \"Church first Graveyard Keeper This is a small, beginner level medieval church. The inspiration came from the church in the Graveyard Keeper game. I...\",\n",
      "  \"author\": \"Psemata\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"Download Schematic\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/download/schematic/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-blood-eagle-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Blood Eagle [Download]\",\n",
      "      \"url\": \"/project/1-21-8-blood-eagle-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-blood-eagle-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] GK first church [Download]\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Rock Pack - x10 [Download]\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-horror-tree-with-face-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Horror Tree with face [Download]\",\n",
      "      \"url\": \"/project/1-21-8-horror-tree-with-face-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-horror-tree-with-face-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-21-8-lava-tree-s-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Lava Tree/s [Download]\",\n",
      "      \"url\": \"/project/1-21-8-lava-tree-s-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-21-8-lava-tree-s-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"[1.21.8] Rock Pack - x10 [Download]\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"VIEW\",\n",
      "      \"url\": \"/project/1-20-8-rock-pack-x10-download/\",\n",
      "      \"type\": \"other\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_project_metadata_v2(soup, project_url):\n",
    "    \"\"\"\n",
    "    Extract all metadata from Planet Minecraft project page\n",
    "    Uses JSON-LD first (most reliable), HTML as fallback\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"url\": project_url,\n",
    "        \"title\": \"N/A\",\n",
    "        \"category\": \"N/A\",\n",
    "        \"subcategory\": \"N/A\",\n",
    "        \"posted_date\": \"N/A\",\n",
    "        \"tags\": [],\n",
    "        \"description\": \"N/A\",\n",
    "        \"author\": \"N/A\",\n",
    "        \"download_links\": [],\n",
    "        \"has_direct_download\": False\n",
    "    }\n",
    "    \n",
    "    # Step 1: Try to extract from JSON-LD (most reliable)\n",
    "    json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    creative_work = None\n",
    "    \n",
    "    for script in json_ld_scripts:\n",
    "        try:\n",
    "            json_data = json.loads(script.string)\n",
    "            if json_data.get('@type') == 'CreativeWork':\n",
    "                creative_work = json_data\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # If we found JSON-LD, extract from it\n",
    "    if creative_work:\n",
    "        metadata['title'] = creative_work.get('name', 'N/A')\n",
    "        metadata['description'] = creative_work.get('description', 'N/A')\n",
    "        metadata['posted_date'] = creative_work.get('datePublished', 'N/A')\n",
    "        metadata['category'] = creative_work.get('genre', 'N/A')  # \"Map\", \"Skin\", \"Texture Pack\", etc.\n",
    "        \n",
    "        # Extract tags from keywords\n",
    "        keywords = creative_work.get('keywords', '')\n",
    "        if keywords:\n",
    "            metadata['tags'] = [tag.strip() for tag in keywords.split(',')]\n",
    "        \n",
    "        # Extract author\n",
    "        author_data = creative_work.get('author', {})\n",
    "        if isinstance(author_data, dict):\n",
    "            metadata['author'] = author_data.get('name', 'N/A')\n",
    "    \n",
    "    # Step 2: HTML fallback for fields not in JSON-LD\n",
    "    else:\n",
    "        # Title fallback\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            metadata['title'] = h1.get_text(strip=True)\n",
    "    \n",
    "    # Step 3: Extract download links (always from HTML)\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        text = link.get_text(strip=True)\n",
    "        \n",
    "        # Look for download-related links\n",
    "        if any(keyword in href.lower() for keyword in ['download', 'schematic', 'world']):\n",
    "            # Skip tag links and other non-download links\n",
    "            if '/tags/' in href or '/projects/' in href:\n",
    "                continue\n",
    "            \n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': classify_download_link(href)\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "            \n",
    "            # Check for direct PM download\n",
    "            if '/download/schematic/' in href or '/download/world/' in href:\n",
    "                metadata['has_direct_download'] = True\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Test it on our loaded page\n",
    "metadata_v2 = extract_project_metadata_v2(soup, url)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPDATED METADATA EXTRACTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(metadata_v2, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa15f3",
   "metadata": {},
   "source": [
    "## Step 13: Update Download Link Classifier\n",
    "\n",
    "Fix the classifier to properly recognize Planet Minecraft download patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e4cdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL METADATA EXTRACTION (with classifier fix):\n",
      "============================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/1-21-8-gk-first-church-download/\",\n",
      "  \"title\": \"[1.21.8] GK first church [Download] Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"N/A\",\n",
      "  \"posted_date\": \"2025-10-08T00:00:00-04:00\",\n",
      "  \"tags\": [\n",
      "    \"medieval\",\n",
      "    \"land-structure\",\n",
      "    \"download\",\n",
      "    \"church\",\n",
      "    \"temple\",\n",
      "    \"ruined\",\n",
      "    \"downloadable\",\n",
      "    \"graveyardkeeper\"\n",
      "  ],\n",
      "  \"description\": \"Church first Graveyard Keeper This is a small, beginner level medieval church. The inspiration came from the church in the Graveyard Keeper game. I...\",\n",
      "  \"author\": \"Psemata\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"Download Schematic\",\n",
      "      \"url\": \"/project/1-21-8-gk-first-church-download/download/schematic/\",\n",
      "      \"type\": \"direct_pm\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": true\n",
      "}\n",
      "\n",
      "‚úì All fields properly extracted from JSON-LD!\n",
      "‚úì Found 1 download link(s)\n",
      "‚úì Has direct download: True\n"
     ]
    }
   ],
   "source": [
    "def classify_download_link_v2(url, text=\"\"):\n",
    "    \"\"\"\n",
    "    Classify download links by type\n",
    "    \"\"\"\n",
    "    url_lower = url.lower()\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Direct Planet Minecraft download\n",
    "    if '/download/schematic/' in url_lower or '/download/world/' in url_lower:\n",
    "        return 'direct_pm'\n",
    "    \n",
    "    # External file hosts\n",
    "    if 'mediafire.com' in url_lower:\n",
    "        return 'mediafire'\n",
    "    if 'dropbox.com' in url_lower:\n",
    "        return 'dropbox'\n",
    "    if 'patreon.com' in url_lower:\n",
    "        return 'patreon'\n",
    "    if 'drive.google.com' in url_lower or 'docs.google.com' in url_lower:\n",
    "        return 'google_drive'\n",
    "    \n",
    "    # Skip these (not actual downloads)\n",
    "    if '/tags/' in url_lower:\n",
    "        return 'tag_link'\n",
    "    if '/projects/' in url_lower and 'download' not in url_lower:\n",
    "        return 'related_project'\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "\n",
    "# Now rebuild the extractor with updated classifier\n",
    "def extract_project_metadata_v3(soup, project_url):\n",
    "    \"\"\"\n",
    "    Extract all metadata from Planet Minecraft project page\n",
    "    Uses JSON-LD first (most reliable), HTML as fallback\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"url\": project_url,\n",
    "        \"title\": \"N/A\",\n",
    "        \"category\": \"N/A\",\n",
    "        \"subcategory\": \"N/A\",\n",
    "        \"posted_date\": \"N/A\",\n",
    "        \"tags\": [],\n",
    "        \"description\": \"N/A\",\n",
    "        \"author\": \"N/A\",\n",
    "        \"download_links\": [],\n",
    "        \"has_direct_download\": False\n",
    "    }\n",
    "    \n",
    "    # Step 1: Try to extract from JSON-LD (most reliable)\n",
    "    json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    creative_work = None\n",
    "    \n",
    "    for script in json_ld_scripts:\n",
    "        try:\n",
    "            json_data = json.loads(script.string)\n",
    "            if json_data.get('@type') == 'CreativeWork':\n",
    "                creative_work = json_data\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # If we found JSON-LD, extract from it\n",
    "    if creative_work:\n",
    "        metadata['title'] = creative_work.get('name', 'N/A')\n",
    "        metadata['description'] = creative_work.get('description', 'N/A')\n",
    "        metadata['posted_date'] = creative_work.get('datePublished', 'N/A')\n",
    "        metadata['category'] = creative_work.get('genre', 'N/A')  # \"Map\", \"Skin\", \"Texture Pack\", etc.\n",
    "        \n",
    "        # Extract tags from keywords\n",
    "        keywords = creative_work.get('keywords', '')\n",
    "        if keywords:\n",
    "            metadata['tags'] = [tag.strip() for tag in keywords.split(',')]\n",
    "        \n",
    "        # Extract author\n",
    "        author_data = creative_work.get('author', {})\n",
    "        if isinstance(author_data, dict):\n",
    "            metadata['author'] = author_data.get('name', 'N/A')\n",
    "    \n",
    "    # Step 2: HTML fallback for fields not in JSON-LD\n",
    "    else:\n",
    "        # Title fallback\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            metadata['title'] = h1.get_text(strip=True)\n",
    "    \n",
    "    # Step 3: Extract download links (always from HTML)\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        text = link.get_text(strip=True)\n",
    "        \n",
    "        # Only look for actual download links (not just anything with \"download\" in URL)\n",
    "        # Direct PM downloads have this specific pattern\n",
    "        if '/download/schematic/' in href.lower() or '/download/world/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': 'direct_pm'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "            metadata['has_direct_download'] = True\n",
    "        \n",
    "        # PM redirect links to external hosts (/download/mirror/, /download/website/)\n",
    "        elif '/download/mirror/' in href.lower() or '/download/website/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': 'pm_external_redirect'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "        \n",
    "        # Direct external download hosts\n",
    "        elif any(host in href.lower() for host in ['mediafire.com', 'dropbox.com', 'drive.google.com', 'mega.nz', 'patreon.com']):\n",
    "            link_type = classify_download_link_v2(href, text)\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': link_type\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Test the updated version\n",
    "metadata_v3 = extract_project_metadata_v3(soup, url)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL METADATA EXTRACTION (with classifier fix):\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(metadata_v3, indent=2))\n",
    "print(\"\\n‚úì All fields properly extracted from JSON-LD!\")\n",
    "print(f\"‚úì Found {len(metadata_v3['download_links'])} download link(s)\")\n",
    "print(f\"‚úì Has direct download: {metadata_v3['has_direct_download']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d0e56",
   "metadata": {},
   "source": [
    "## Step 14: Test on Other Example URLs\n",
    "\n",
    "Let's test the extractor on the other two examples to verify it handles different download types correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda45972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST #2: External Download Links (MediaFire/Dropbox)\n",
      "======================================================================\n",
      "URL: https://www.planetminecraft.com/project/the-great-kingdoms/\n",
      "\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/the-great-kingdoms/\",\n",
      "  \"title\": \"The Great Kingdoms Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"N/A\",\n",
      "  \"posted_date\": \"2025-10-09T00:00:00-04:00\",\n",
      "  \"tags\": [\n",
      "    \"redstone-device\",\n",
      "    \"kingdom\",\n",
      "    \"portals\",\n",
      "    \"commandblocks\",\n",
      "    \"dimensions\",\n",
      "    \"teleport\",\n",
      "    \"superpowers\",\n",
      "    \"other\"\n",
      "  ],\n",
      "  \"description\": \"A large scale world created by me and some buddies. Each one went out to establish his own land, his own kingdom, a place that he could call his own....\",\n",
      "  \"author\": \"PokecrafterChamp\",\n",
      "  \"download_links\": [],\n",
      "  \"has_direct_download\": false\n",
      "}\n",
      "\n",
      "‚úì Found 0 download link(s)\n",
      "‚úì Download types: []\n",
      "‚úì Has direct download: False\n"
     ]
    }
   ],
   "source": [
    "# Test #2: External links example (The Great Kingdoms - MediaFire/Dropbox)\n",
    "print(\"=\"*70)\n",
    "print(\"TEST #2: External Download Links (MediaFire/Dropbox)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "url_external = example_urls['external_links']\n",
    "print(f\"URL: {url_external}\\n\")\n",
    "\n",
    "# Navigate with Selenium\n",
    "driver.get(url_external)\n",
    "time.sleep(3)  # Wait for load\n",
    "\n",
    "# Parse the page\n",
    "page_source_external = driver.page_source\n",
    "soup_external = BeautifulSoup(page_source_external, 'html.parser')\n",
    "\n",
    "# Extract metadata\n",
    "metadata_external = extract_project_metadata_v3(soup_external, url_external)\n",
    "\n",
    "print(json.dumps(metadata_external, indent=2))\n",
    "print(f\"\\n‚úì Found {len(metadata_external['download_links'])} download link(s)\")\n",
    "print(f\"‚úì Download types: {[dl['type'] for dl in metadata_external['download_links']]}\")\n",
    "print(f\"‚úì Has direct download: {metadata_external['has_direct_download']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef692f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEBUG: Searching for ALL links that might be downloads...\n",
      "======================================================================\n",
      "\n",
      "Found 5 potential download links:\n",
      "\n",
      "1. Text: Kingdoms Download\n",
      "   URL: /project/the-great-kingdoms/download/mirror/748184/\n",
      "\n",
      "2. Text: Kingdoms Download\n",
      "   URL: /project/the-great-kingdoms/download/website/748185/\n",
      "\n",
      "3. Text: \n",
      "   URL: /project/nuestra-se-ora-de-la-sant-sima-trinidad-ship-of-the-line-free-download/\n",
      "\n",
      "4. Text: Nuestra Se√±ora de la Sant√≠sima Trinidad, ship of the line (F\n",
      "   URL: /project/nuestra-se-ora-de-la-sant-sima-trinidad-ship-of-the-line-free-download/\n",
      "\n",
      "5. Text: VIEW\n",
      "   URL: /project/nuestra-se-ora-de-la-sant-sima-trinidad-ship-of-the-line-free-download/\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's see what links are actually on this page\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEBUG: Searching for ALL links that might be downloads...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_links_external = soup_external.find_all('a', href=True)\n",
    "potential_downloads = []\n",
    "\n",
    "for link in all_links_external:\n",
    "    href = link['href']\n",
    "    text = link.get_text(strip=True)\n",
    "    \n",
    "    # Check for external hosts or download keywords\n",
    "    if any(host in href.lower() for host in ['mediafire', 'dropbox', 'drive.google', 'mega.nz', 'patreon', 'download']):\n",
    "        potential_downloads.append({\n",
    "            'text': text[:60],\n",
    "            'href': href[:100]\n",
    "        })\n",
    "\n",
    "print(f\"\\nFound {len(potential_downloads)} potential download links:\")\n",
    "for i, link in enumerate(potential_downloads[:10], 1):\n",
    "    print(f\"\\n{i}. Text: {link['text']}\")\n",
    "    print(f\"   URL: {link['href']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2879910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UPDATED EXTRACTION:\n",
      "======================================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/the-great-kingdoms/\",\n",
      "  \"title\": \"The Great Kingdoms Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"N/A\",\n",
      "  \"posted_date\": \"2025-10-09T00:00:00-04:00\",\n",
      "  \"tags\": [\n",
      "    \"redstone-device\",\n",
      "    \"kingdom\",\n",
      "    \"portals\",\n",
      "    \"commandblocks\",\n",
      "    \"dimensions\",\n",
      "    \"teleport\",\n",
      "    \"superpowers\",\n",
      "    \"other\"\n",
      "  ],\n",
      "  \"description\": \"A large scale world created by me and some buddies. Each one went out to establish his own land, his own kingdom, a place that he could call his own....\",\n",
      "  \"author\": \"PokecrafterChamp\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"Kingdoms Download\",\n",
      "      \"url\": \"/project/the-great-kingdoms/download/mirror/748184/\",\n",
      "      \"type\": \"pm_external_redirect\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Kingdoms Download\",\n",
      "      \"url\": \"/project/the-great-kingdoms/download/website/748185/\",\n",
      "      \"type\": \"pm_external_redirect\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": false\n",
      "}\n",
      "\n",
      "‚úì Found 2 download link(s)\n",
      "‚úì Download types: ['pm_external_redirect', 'pm_external_redirect']\n",
      "‚úì Has direct download: False\n"
     ]
    }
   ],
   "source": [
    "# Re-extract with updated function\n",
    "metadata_external_v2 = extract_project_metadata_v3(soup_external, url_external)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UPDATED EXTRACTION:\")\n",
    "print(\"=\"*70)\n",
    "print(json.dumps(metadata_external_v2, indent=2))\n",
    "print(f\"\\n‚úì Found {len(metadata_external_v2['download_links'])} download link(s)\")\n",
    "if metadata_external_v2['download_links']:\n",
    "    print(f\"‚úì Download types: {[dl['type'] for dl in metadata_external_v2['download_links']]}\")\n",
    "print(f\"‚úì Has direct download: {metadata_external_v2['has_direct_download']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b361d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST #3: Patreon/Paid Download\n",
      "======================================================================\n",
      "URL: https://www.planetminecraft.com/project/fantasy-blue-house/\n",
      "\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/fantasy-blue-house/\",\n",
      "  \"title\": \"Fantasy blue house Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"N/A\",\n",
      "  \"posted_date\": \"2025-09-21T00:00:00-04:00\",\n",
      "  \"tags\": [\n",
      "    \"fantasy\",\n",
      "    \"medieval\",\n",
      "    \"land-structure\",\n",
      "    \"castle\",\n",
      "    \"minecraft\",\n",
      "    \"creative\",\n",
      "    \"statue\",\n",
      "    \"build\",\n",
      "    \"house\",\n",
      "    \"modern\",\n",
      "    \"realistic\",\n",
      "    \"recreation\",\n",
      "    \"download\",\n",
      "    \"vanilla\",\n",
      "    \"idea\",\n",
      "    \"free\",\n",
      "    \"new\"\n",
      "  ],\n",
      "  \"description\": \"CONTACT ME Want a CUSTOM BUILD for your server or PROJECT DM me on DISCORD SpicyTmc All my socials Download Patreon .litematic Sub lvl Green chili...\",\n",
      "  \"author\": \"SpicyT\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"File Download\",\n",
      "      \"url\": \"/project/fantasy-blue-house/download/mirror/746905/\",\n",
      "      \"type\": \"pm_external_redirect\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": false\n",
      "}\n",
      "\n",
      "‚úì Found 1 download link(s)\n",
      "‚úì Download types: ['pm_external_redirect']\n",
      "‚úì Has direct download: False\n"
     ]
    }
   ],
   "source": [
    "# Test #3: Patreon/Paid example\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST #3: Patreon/Paid Download\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "url_patreon = example_urls['paid_patreon']\n",
    "print(f\"URL: {url_patreon}\\n\")\n",
    "\n",
    "# Navigate with Selenium\n",
    "driver.get(url_patreon)\n",
    "time.sleep(3)  # Wait for load\n",
    "\n",
    "# Parse the page\n",
    "page_source_patreon = driver.page_source\n",
    "soup_patreon = BeautifulSoup(page_source_patreon, 'html.parser')\n",
    "\n",
    "# Extract metadata\n",
    "metadata_patreon = extract_project_metadata_v3(soup_patreon, url_patreon)\n",
    "\n",
    "print(json.dumps(metadata_patreon, indent=2))\n",
    "print(f\"\\n‚úì Found {len(metadata_patreon['download_links'])} download link(s)\")\n",
    "if metadata_patreon['download_links']:\n",
    "    print(f\"‚úì Download types: {[dl['type'] for dl in metadata_patreon['download_links']]}\")\n",
    "print(f\"‚úì Has direct download: {metadata_patreon['has_direct_download']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35890c",
   "metadata": {},
   "source": [
    "## ‚úÖ Test Results Summary\n",
    "\n",
    "All three example projects extracted successfully!\n",
    "\n",
    "### Test #1: Direct Download (GK First Church)\n",
    "- **Title**: \"[1.21.8] GK first church [Download] Minecraft Map\"\n",
    "- **Category**: Map\n",
    "- **Posted**: 2025-10-08\n",
    "- **Tags**: 8 tags (medieval, land-structure, church, etc.)\n",
    "- **Download**: 1 link - `direct_pm` type ‚úì\n",
    "- **Has Direct Download**: True ‚úì\n",
    "\n",
    "### Test #2: External Links (The Great Kingdoms)\n",
    "- **Title**: \"The Great Kingdoms Minecraft Map\"\n",
    "- **Category**: Map\n",
    "- **Posted**: 2025-10-09\n",
    "- **Tags**: 8 tags (kingdom, portals, commandblocks, etc.)\n",
    "- **Download**: 2 links - both `pm_external_redirect` type ‚úì\n",
    "- **Has Direct Download**: False ‚úì\n",
    "\n",
    "### Test #3: Patreon/Paid (Fantasy Blue House)\n",
    "- **Title**: \"Fantasy blue house Minecraft Map\"\n",
    "- **Category**: Map\n",
    "- **Posted**: 2025-09-21\n",
    "- **Tags**: 17 tags (fantasy, medieval, house, etc.)\n",
    "- **Download**: 1 link - `pm_external_redirect` type ‚úì\n",
    "- **Has Direct Download**: False ‚úì\n",
    "\n",
    "### Key Findings:\n",
    "1. ‚úì JSON-LD extraction works perfectly for all projects\n",
    "2. ‚úì Direct PM downloads detected: `/download/schematic/` pattern\n",
    "3. ‚úì External redirects detected: `/download/mirror/` and `/download/website/` patterns\n",
    "4. ‚úì All metadata fields populated correctly (title, category, date, tags, author, description)\n",
    "5. ‚úì No false positives - only actual download buttons captured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097d399",
   "metadata": {},
   "source": [
    "## Step 15: Improved Extractor with Better Category/Subcategory Logic\n",
    "\n",
    "You're right - \"land-structure\" should be the subcategory! Let me explain how the data flows:\n",
    "\n",
    "### Data Extraction Process:\n",
    "\n",
    "1. **JSON-LD Script Tag** (Primary Source):\n",
    "   - Planet Minecraft embeds structured data in `<script type=\"application/ld+json\">`\n",
    "   - We parse this JSON to get: title, description, date, genre (category), keywords (tags), author\n",
    "\n",
    "2. **Genre vs Tags**:\n",
    "   - `genre`: \"Map\" (broad category)\n",
    "   - `keywords`: \"medieval,land-structure,download,church,...\" \n",
    "   - \"land-structure\" is actually a **tag**, not in a separate category field\n",
    "\n",
    "3. **Subcategory Detection**:\n",
    "   - We scan the tags for structure-type indicators\n",
    "   - Common patterns: land-structure, air-structure, castle, house, city, etc.\n",
    "   - First matching tag becomes the subcategory\n",
    "\n",
    "Let's update the extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05736621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL EXTRACTOR - Testing All Three Examples\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Testing: direct_download\n",
      "======================================================================\n",
      "Title: [1.21.8] GK first church [Download] Minecraft Map\n",
      "Category: Map\n",
      "Subcategory: land-structure ‚Üê Detected from tags!\n",
      "Posted: 2025-10-08T00:00:00-04:00\n",
      "Author: Psemata\n",
      "Tags: ['medieval', 'land-structure', 'download', 'church', 'temple']...\n",
      "Download links: 1 (['direct_pm'])\n",
      "Has direct download: True\n",
      "\n",
      "======================================================================\n",
      "Testing: external_links\n",
      "======================================================================\n",
      "Title: The Great Kingdoms Minecraft Map\n",
      "Category: Map\n",
      "Subcategory: N/A ‚Üê Detected from tags!\n",
      "Posted: 2025-10-09T00:00:00-04:00\n",
      "Author: PokecrafterChamp\n",
      "Tags: ['redstone-device', 'kingdom', 'portals', 'commandblocks', 'dimensions']...\n",
      "Download links: 2 (['pm_external_redirect', 'pm_external_redirect'])\n",
      "Has direct download: False\n",
      "\n",
      "======================================================================\n",
      "Testing: paid_patreon\n",
      "======================================================================\n",
      "Title: Fantasy blue house Minecraft Map\n",
      "Category: Map\n",
      "Subcategory: land-structure ‚Üê Detected from tags!\n",
      "Posted: 2025-09-21T00:00:00-04:00\n",
      "Author: SpicyT\n",
      "Tags: ['fantasy', 'medieval', 'land-structure', 'castle', 'minecraft']...\n",
      "Download links: 1 (['pm_external_redirect'])\n",
      "Has direct download: False\n"
     ]
    }
   ],
   "source": [
    "def extract_project_metadata_final(soup, project_url):\n",
    "    \"\"\"\n",
    "    Final version: Extract all metadata from Planet Minecraft project page\n",
    "    Uses JSON-LD first (most reliable), HTML as fallback\n",
    "    Properly detects subcategory from tags\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"url\": project_url,\n",
    "        \"title\": \"N/A\",\n",
    "        \"category\": \"N/A\",\n",
    "        \"subcategory\": \"N/A\",\n",
    "        \"posted_date\": \"N/A\",\n",
    "        \"tags\": [],\n",
    "        \"description\": \"N/A\",\n",
    "        \"author\": \"N/A\",\n",
    "        \"download_links\": [],\n",
    "        \"has_direct_download\": False\n",
    "    }\n",
    "    \n",
    "    # Step 1: Try to extract from JSON-LD (most reliable)\n",
    "    json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    creative_work = None\n",
    "    \n",
    "    for script in json_ld_scripts:\n",
    "        try:\n",
    "            json_data = json.loads(script.string)\n",
    "            if json_data.get('@type') == 'CreativeWork':\n",
    "                creative_work = json_data\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # If we found JSON-LD, extract from it\n",
    "    if creative_work:\n",
    "        metadata['title'] = creative_work.get('name', 'N/A')\n",
    "        metadata['description'] = creative_work.get('description', 'N/A')\n",
    "        metadata['posted_date'] = creative_work.get('datePublished', 'N/A')\n",
    "        \n",
    "        # Category is the broad type from genre field\n",
    "        metadata['category'] = creative_work.get('genre', 'N/A')  # \"Map\", \"Skin\", \"Texture Pack\", etc.\n",
    "        \n",
    "        # Extract tags from keywords (comma-separated string)\n",
    "        keywords = creative_work.get('keywords', '')\n",
    "        if keywords:\n",
    "            metadata['tags'] = [tag.strip() for tag in keywords.split(',')]\n",
    "            \n",
    "            # Detect subcategory from structure-related tags\n",
    "            # These are common subcategory indicators on Planet Minecraft\n",
    "            structure_tags = [\n",
    "                'land-structure', 'air-structure', 'underground-structure', \n",
    "                'water-structure', 'floating-structure',\n",
    "                'castle', 'house', 'temple', 'tower', 'city', 'village',\n",
    "                'mansion', 'fort', 'palace', 'church', 'cathedral'\n",
    "            ]\n",
    "            \n",
    "            # Find first matching structure tag\n",
    "            for tag in metadata['tags']:\n",
    "                if tag in structure_tags:\n",
    "                    metadata['subcategory'] = tag\n",
    "                    break\n",
    "        \n",
    "        # Extract author\n",
    "        author_data = creative_work.get('author', {})\n",
    "        if isinstance(author_data, dict):\n",
    "            metadata['author'] = author_data.get('name', 'N/A')\n",
    "    \n",
    "    # Step 2: HTML fallback for fields not in JSON-LD\n",
    "    else:\n",
    "        # Title fallback\n",
    "        h1 = soup.find('h1')\n",
    "        if h1:\n",
    "            metadata['title'] = h1.get_text(strip=True)\n",
    "    \n",
    "    # Step 3: Extract download links (always from HTML)\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        text = link.get_text(strip=True)\n",
    "        \n",
    "        # Only look for actual download links\n",
    "        # Direct PM downloads\n",
    "        if '/download/schematic/' in href.lower() or '/download/world/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': 'direct_pm'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "            metadata['has_direct_download'] = True\n",
    "        \n",
    "        # PM redirect links to external hosts\n",
    "        elif '/download/mirror/' in href.lower() or '/download/website/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': 'pm_external_redirect'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "        \n",
    "        # Direct external download hosts\n",
    "        elif any(host in href.lower() for host in ['mediafire.com', 'dropbox.com', 'drive.google.com', 'mega.nz', 'patreon.com']):\n",
    "            link_type = classify_download_link_v2(href, text)\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': link_type\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Test on all three examples\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL EXTRACTOR - Testing All Three Examples\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, url_test in example_urls.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get the soup (reuse if already loaded)\n",
    "    if name == 'direct_download':\n",
    "        soup_test = soup\n",
    "    elif name == 'external_links':\n",
    "        soup_test = soup_external\n",
    "    else:  # patreon\n",
    "        soup_test = soup_patreon\n",
    "    \n",
    "    result = extract_project_metadata_final(soup_test, url_test)\n",
    "    \n",
    "    # Show key fields\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Category: {result['category']}\")\n",
    "    print(f\"Subcategory: {result['subcategory']} ‚Üê Detected from tags!\")\n",
    "    print(f\"Posted: {result['posted_date']}\")\n",
    "    print(f\"Author: {result['author']}\")\n",
    "    print(f\"Tags: {result['tags'][:5]}...\" if len(result['tags']) > 5 else f\"Tags: {result['tags']}\")\n",
    "    print(f\"Download links: {len(result['download_links'])} ({[dl['type'] for dl in result['download_links']]})\")\n",
    "    print(f\"Has direct download: {result['has_direct_download']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac5024",
   "metadata": {},
   "source": [
    "## Step 16: Extract Additional Stats (Views, Downloads, Diamonds, Hearts, Updated Date)\n",
    "\n",
    "You've identified additional important metadata! Let's explore where these stats are stored on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57827292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading example with stats: https://www.planetminecraft.com/project/medieval-house-fully-decorated-interior-download-6638738\n",
      "\n",
      "‚úì Page loaded\n",
      "\n",
      "======================================================================\n",
      "Searching for stats elements...\n",
      "======================================================================\n",
      "\n",
      "VIEW:\n",
      "  Text: 1,460views,60today\n",
      "  Text: VIEW\n",
      "  Attrs: {'title': 'Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map & Project'}\n",
      "  Text: VIEW\n",
      "  Attrs: {'title': 'Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map & Project'}\n",
      "  Text: VIEW\n",
      "  Attrs: {'title': 'Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map & Project'}\n",
      "  Text: VIEW\n",
      "  Attrs: {'title': 'Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map & Project'}\n",
      "\n",
      "\n",
      "DOWNLOAD:\n",
      "  Text: Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map\n",
      "  Text: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\n",
      "  Text: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"CreativeWork\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@i\n",
      "  Text: Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map\n",
      "  Text: Medieval House ‚Äì Fully Decorated Interior | Download\n",
      "\n",
      "\n",
      "DIAMOND:\n",
      "  Text: Diamond Log\n",
      "  Text: Give diamonds to support favorite creators\n",
      "\n",
      "\n",
      "FAVORITE:\n",
      "  Text: Favorite Log\n",
      "  Text: Give diamonds to support favorite creators\n",
      "\n",
      "\n",
      "UPDATE:\n",
      "  Text: Updated on Oct 8th,yesterday|1 logsPublishedJun 6th,5 months ago\n",
      "  Text: 1 Update Logs\n",
      "  Text: Update #1\n",
      "  Text: Updated and modified the schematic file.\n",
      "  Text: window.dataLayer=window.dataLayer||[];window.gtag=window.gtag||function(){window.dataLayer.push(argu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the new example page with stats\n",
    "url_stats = \"https://www.planetminecraft.com/project/medieval-house-fully-decorated-interior-download-6638738\"\n",
    "print(f\"Loading example with stats: {url_stats}\\n\")\n",
    "\n",
    "driver.get(url_stats)\n",
    "time.sleep(3)\n",
    "\n",
    "page_source_stats = driver.page_source\n",
    "soup_stats = BeautifulSoup(page_source_stats, 'html.parser')\n",
    "\n",
    "print(\"‚úì Page loaded\\n\")\n",
    "\n",
    "# Look for stat elements\n",
    "print(\"=\"*70)\n",
    "print(\"Searching for stats elements...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Search for common stat patterns\n",
    "stat_keywords = ['view', 'download', 'diamond', 'heart', 'favorite', 'update', 'upload']\n",
    "\n",
    "for keyword in stat_keywords:\n",
    "    elements = soup_stats.find_all(string=re.compile(keyword, re.IGNORECASE))\n",
    "    if elements:\n",
    "        print(f\"\\n{keyword.upper()}:\")\n",
    "        for elem in elements[:5]:\n",
    "            parent = elem.parent\n",
    "            # Get surrounding context\n",
    "            if parent:\n",
    "                text = parent.get_text(strip=True)\n",
    "                # Also check for data attributes\n",
    "                attrs = {k: v for k, v in parent.attrs.items() if 'data' in k or 'title' in k}\n",
    "                print(f\"  Text: {text[:100]}\")\n",
    "                if attrs:\n",
    "                    print(f\"  Attrs: {attrs}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5212362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Looking for stat numbers...\n",
      "======================================================================\n",
      "\n",
      "Text: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"item\":{\"@id\":\"https:\\/\\/www.planetminecraft.com\\/projects\\/\",\"name\":\"Minecraft Maps\"}},{\"@type\":\"ListItem\",\"position\":2,\"item\":{\"@id\":\"https:\\/\\/www.planetminecraft.com\\/projects\\/medieval-house-fully-decorated-interior-download-6638738\\/\",\"name\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\"}}]}\n",
      "Parent tag: script\n",
      "Parent class: no class\n",
      "\n",
      "Text: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"CreativeWork\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\/\\/www.planetminecraft.com\\/project\\/medieval-house-fully-decorated-interior-download-6638738\\/\"},\"url\":\"https:\\/\\/www.planetminecraft.com\\/project\\/medieval-house-fully-decorated-interior-download-6638738\\/\",\"name\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\",\"description\":\"This Medieval House comes with a fully decorated interior, perfect for adding life and detail to your medieval towns or survival worlds. Download...\",\"image\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/static.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/738\\/19211527-medievalhouseb_l.jpg\",\"name\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\",\"width\":\"800px\",\"height\":\"450px\"},\"thumbnailUrl\":\"https:\\/\\/www.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/738\\/19211527-medievalhouseb_l.jpg\",\"genre\":\"Map\",\"headline\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\",\"dateCreated\":\"2025-06-06T00:00:00-04:00\",\"datePublished\":\"2025-06-06T00:00:00-04:00\",\"dateModified\":\"2025-10-08T08:11:49-04:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Raekon\",\"image\":\"3932308_1.png\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/member\\/raekon\",\"sameAs\":[\"https:\\/\\/www.instagram.com\\/raekonbuilds\",\"https:\\/\\/www.patreon.com\\/raekon\"]},\"publisher\":{\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"Organization\",\"@id\":\"#organization\",\"url\":\"https:\\/\\/www.planetminecraft.com\",\"name\":\"Planet Minecraft\",\"sameAs\":[\"https:\\/\\/www.twitter.com\\/PlanetMinecraft\",\"https:\\/\\/www.facebook.com\\/PlanetMinecraft\",\"https:\\/\\/www.instagram.com\\/PlanetMinecraftOfficial\"],\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/images\\/layout\\/themes\\/modern\\/planetminecraft_logo.png\",\"width\":\"222px\",\"height\":\"72px\"}},\"commentCount\":0,\"isFamilyFriendly\":true,\"keywords\":\"medieval,city,house,download,casa,schematics,home,schematic,rustic,interior,fully,downloadable,decorated,other,litematica,downloadablemap,litematic\"}\n",
      "Parent tag: script\n",
      "Parent class: no class\n",
      "\n",
      "Text: <a href=\"https://www.planetminecraft.com/project/medieval-house-fully-decorated-interior-download-6638738/\" title=\"Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map\"><br /><img src=\"https://static.planetminecraft.com/files/image/minecraft/project/2025/738/19211527-medievalhouseb_l.jpg\" alt=\"Medieval House ‚Äì Fully Decorated Interior | Download\" border=\"0\"/><br/>Medieval House ‚Äì Fully Decorated Interior | Download</a> by <a href=\"https://www.planetminecraft.com/member/raekon/\" title=\"Raekon Profile\">Raekon</a>\n",
      "Parent tag: textarea\n",
      "Parent class: ['auto_select']\n",
      "\n",
      "======================================================================\n",
      "Looking for dates...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "JSON-LD dates:\n",
      "======================================================================\n",
      "dateCreated: 2025-06-06T00:00:00-04:00\n",
      "datePublished: 2025-06-06T00:00:00-04:00\n",
      "dateModified: 2025-10-08T08:11:49-04:00\n"
     ]
    }
   ],
   "source": [
    "# Look for stat containers with specific patterns\n",
    "print(\"=\"*70)\n",
    "print(\"Looking for stat numbers...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find elements with stat-like text (numbers followed by keywords)\n",
    "all_text = soup_stats.find_all(string=re.compile(r'\\d+.*?(view|download|diamond|heart|favorite)', re.IGNORECASE))\n",
    "\n",
    "for text in all_text[:15]:\n",
    "    parent = text.parent\n",
    "    print(f\"\\nText: {text.strip()}\")\n",
    "    print(f\"Parent tag: {parent.name}\")\n",
    "    print(f\"Parent class: {parent.get('class', 'no class')}\")\n",
    "    \n",
    "# Also check for specific date elements\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Looking for dates...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "time_elements = soup_stats.find_all('time')\n",
    "for time_elem in time_elements:\n",
    "    print(f\"\\nTime element:\")\n",
    "    print(f\"  Text: {time_elem.get_text(strip=True)}\")\n",
    "    print(f\"  datetime: {time_elem.get('datetime')}\")\n",
    "    print(f\"  title: {time_elem.get('title')}\")\n",
    "    \n",
    "# Check JSON-LD for dateModified\n",
    "json_ld_scripts_stats = soup_stats.find_all('script', type='application/ld+json')\n",
    "for script in json_ld_scripts_stats:\n",
    "    try:\n",
    "        json_data = json.loads(script.string)\n",
    "        if json_data.get('@type') == 'CreativeWork':\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"JSON-LD dates:\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"dateCreated: {json_data.get('dateCreated')}\")\n",
    "            print(f\"datePublished: {json_data.get('datePublished')}\")\n",
    "            print(f\"dateModified: {json_data.get('dateModified')}\")\n",
    "            break\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de8187e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Looking for stat bars with numbers...\n",
      "======================================================================\n",
      "Found 31 stat-like elements\n",
      "\n",
      "Class: ['resource-statistics']\n",
      "Text: 1,460views,60today185downloads,4today\n",
      "Tag: ul\n",
      "\n",
      "======================================================================\n",
      "Looking for elements with comma-separated numbers...\n",
      "======================================================================\n",
      "Found 25 relevant stat elements:\n",
      "\n",
      "Number: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"item\":{\"@id\":\"https:\\/\\/www.planetminecraft.com\\/projects\\/\",\"name\":\"Minecraft Maps\"}},{\"@type\":\"ListItem\",\"position\":2,\"item\":{\"@id\":\"https:\\/\\/www.planetminecraft.com\\/projects\\/medieval-house-fully-decorated-interior-download-6638738\\/\",\"name\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\"}}]}\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n",
      "Number: {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"CreativeWork\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\/\\/www.planetminecraft.com\\/project\\/medieval-house-fully-decorated-interior-download-6638738\\/\"},\"url\":\"https:\\/\\/www.planetminecraft.com\\/project\\/medieval-house-fully-decorated-interior-download-6638738\\/\",\"name\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\",\"description\":\"This Medieval House comes with a fully decorated interior, perfect for adding life and detail to your medieval towns or survival worlds. Download...\",\"image\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/static.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/738\\/19211527-medievalhouseb_l.jpg\",\"name\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\",\"width\":\"800px\",\"height\":\"450px\"},\"thumbnailUrl\":\"https:\\/\\/www.planetminecraft.com\\/files\\/image\\/minecraft\\/project\\/2025\\/738\\/19211527-medievalhouseb_l.jpg\",\"genre\":\"Map\",\"headline\":\"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\",\"dateCreated\":\"2025-06-06T00:00:00-04:00\",\"datePublished\":\"2025-06-06T00:00:00-04:00\",\"dateModified\":\"2025-10-08T08:11:49-04:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Raekon\",\"image\":\"3932308_1.png\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/member\\/raekon\",\"sameAs\":[\"https:\\/\\/www.instagram.com\\/raekonbuilds\",\"https:\\/\\/www.patreon.com\\/raekon\"]},\"publisher\":{\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"Organization\",\"@id\":\"#organization\",\"url\":\"https:\\/\\/www.planetminecraft.com\",\"name\":\"Planet Minecraft\",\"sameAs\":[\"https:\\/\\/www.twitter.com\\/PlanetMinecraft\",\"https:\\/\\/www.facebook.com\\/PlanetMinecraft\",\"https:\\/\\/www.instagram.com\\/PlanetMinecraftOfficial\"],\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.planetminecraft.com\\/images\\/layout\\/themes\\/modern\\/planetminecraft_logo.png\",\"width\":\"222px\",\"height\":\"72px\"}},\"commentCount\":0,\"isFamilyFriendly\":true,\"keywords\":\"medieval,city,house,download,casa,schematics,home,schematic,rustic,interior,fully,downloadable,decorated,other,litematica,downloadablemap,litematic\"}\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n",
      "Number: (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n",
      "  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n",
      "  })(window,document,'script','dataLayer','GTM-PCJ3GW');\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n",
      "Number: .material-icons {width:18px;height:18px;overflow:hidden;display:inline-block;}\n",
      "\t\t\t.material-icons.md-14 {height:14px; width:14px;}\n",
      "\t\t\t.material-icons.md-16 {height:16px; width:16px;}\n",
      "\t\t\t.material-icons.md-18 {height:18px; width:18px;}\n",
      "\t\t\t.material-icons.md-24 {height:24px; width:24px;}\n",
      "\t\t\t.material-icons.md-36 {height:36px; width:36px;}\n",
      "\t\t\t.material-icons.md-48 {height:48px; width:48px;}\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <style class='None'>\n",
      "\n",
      "Number: window.applicationServerKey = 'BFjDsY5mgP1O32ZWL1ECAAdDdcqGMWT5kcS3odutPznpi9sv_AZu0AY3J2WAEm3YFVCE01oK_jO-1XsMXyCGtdw';\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n",
      "Number: function preloadSupported() {\n",
      "    const relList = document.createElement('link').relList;\n",
      "    return !!(relList && relList.supports && relList.supports('preload'));\n",
      "}\n",
      "\n",
      "function loadStyle(href, callback){\n",
      "    const head  = document.getElementsByTagName('head')[0];\n",
      "    const link  = document.createElement('link');\n",
      "    link.rel  = 'stylesheet';\n",
      "    link.type = 'text/css';\n",
      "    link.href = href;\n",
      "    if (callback) { link.onload = function() { callback() } }\n",
      "    head.appendChild(link);\n",
      "}\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n",
      "Number: if(!preloadSupported()) loadStyle(\"//www.planetminecraft.com/css/fonts.css?v=iAIAFChsfiOXhFo1Q2xw8K3yBUtVao3WmHpU39GNnyc7p4lg\");\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n",
      "Number: [if lte IE 9]> <script type=\"text/javascript\">lteIE9 = true;\n",
      "\t\t\t\tconsole.log('LTE IE9 detected.');</script> <![endif]\n",
      "Context: Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "       \n",
      "Parent: <head class='None'>\n",
      "\n",
      "Number: window.coreUserData = {\"mid\":0,\"mod\":false};\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n",
      "Number: self.__VM = self.__VM || [];\n",
      "\tself.__VM.push(function (admanager, scope) {\n",
      "scope.Config.get('leaderboard','leaderboard-btf').display('vp-leaderboard-btf-0'); // 728BTF\n",
      "scope.Config.get('mpu','mpu-atf').display('vp-mpu-atf'); // 300ATF\n",
      "scope.Config.get('mpu','mpu-btf').display('vp-mpu-btf-0'); // 300BTF\n",
      "scope.Config.get('desktop_takeover').display('vp-desktop_takeover'); // TITLE_TAKEOVER\n",
      "scope.Config.get('video').display('vp-video'); // VIDEO\n",
      "});\n",
      "Context:    Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map \n",
      "  \n",
      "          \n",
      "  \n",
      "          \n",
      " \n",
      "Parent: <script class='None'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for the specific stat pattern we saw: \"1,460views,60today\"\n",
    "print(\"=\"*70)\n",
    "print(\"Looking for stat bars with numbers...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find all elements with class containing \"stat\" or \"count\"\n",
    "stat_elements = soup_stats.find_all(class_=re.compile(r'stat|count|number|metric', re.IGNORECASE))\n",
    "print(f\"Found {len(stat_elements)} stat-like elements\\n\")\n",
    "\n",
    "for elem in stat_elements[:20]:\n",
    "    text = elem.get_text(strip=True)\n",
    "    if any(keyword in text.lower() for keyword in ['view', 'download', 'diamond', 'heart', 'favorite']):\n",
    "        print(f\"Class: {elem.get('class')}\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Tag: {elem.name}\")\n",
    "        print()\n",
    "\n",
    "# Also try to find by looking for number patterns\n",
    "print(\"=\"*70)\n",
    "print(\"Looking for elements with comma-separated numbers...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "number_elements = soup_stats.find_all(string=re.compile(r'\\d{1,3}(,\\d{3})*'))\n",
    "relevant_stats = []\n",
    "\n",
    "for elem in number_elements:\n",
    "    text = elem.strip()\n",
    "    parent = elem.parent\n",
    "    \n",
    "    # Check if contains stat keywords\n",
    "    siblings_text = ' '.join([s.get_text() if hasattr(s, 'get_text') else str(s) for s in parent.parent.children if s])\n",
    "    \n",
    "    if any(keyword in siblings_text.lower() for keyword in ['view', 'download', 'diamond', 'heart']):\n",
    "        relevant_stats.append({\n",
    "            'text': text,\n",
    "            'context': siblings_text[:100],\n",
    "            'parent_tag': parent.name,\n",
    "            'parent_class': parent.get('class')\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(relevant_stats)} relevant stat elements:\\n\")\n",
    "for stat in relevant_stats[:10]:\n",
    "    print(f\"Number: {stat['text']}\")\n",
    "    print(f\"Context: {stat['context']}\")\n",
    "    print(f\"Parent: <{stat['parent_tag']} class='{stat['parent_class']}'>\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0ca52a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Searching for combined stat pattern (e.g., '1,460views,60today')...\n",
      "======================================================================\n",
      "Found 0 matches:\n",
      "\n",
      "======================================================================\n",
      "Checking for title/data attributes with stats...\n",
      "======================================================================\n",
      "Title: Download from third-party: https://www.patreon.com/posts/medieval-house-130857478?utm_medium=clipboard_copy&utm_source=copyLink&utm_campaign=postshare_creator&utm_content=join_link\n",
      "Element: <a class='['third-party-download', 'branded-download']'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try regex to find the exact pattern: \"1,460views,60today\"\n",
    "print(\"=\"*70)\n",
    "print(\"Searching for combined stat pattern (e.g., '1,460views,60today')...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pattern = re.compile(r'(\\d[\\d,]*)\\s*(view|download|diamond|heart)', re.IGNORECASE)\n",
    "matches = soup_stats.find_all(string=pattern)\n",
    "\n",
    "print(f\"Found {len(matches)} matches:\\n\")\n",
    "for match in matches[:15]:\n",
    "    print(f\"Text: '{match.strip()}'\")\n",
    "    parent = match.parent\n",
    "    print(f\"Parent: <{parent.name} class='{parent.get('class')}'>\")\n",
    "    # Try to extract the numbers\n",
    "    found = pattern.findall(match)\n",
    "    if found:\n",
    "        print(f\"Extracted: {found}\")\n",
    "    print()\n",
    "\n",
    "# Alternative: Look for title or data attributes with stats\n",
    "print(\"=\"*70)\n",
    "print(\"Checking for title/data attributes with stats...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "elements_with_title = soup_stats.find_all(attrs={'title': re.compile(r'\\d')})\n",
    "for elem in elements_with_title[:10]:\n",
    "    title = elem.get('title')\n",
    "    if any(keyword in title.lower() for keyword in ['view', 'download', 'diamond', 'heart']):\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Element: <{elem.name} class='{elem.get('class')}'>\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc1b3991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Searching raw HTML for stats...\n",
      "======================================================================\n",
      "Found '1,460' in context:\n",
      "data-timeago=\"1\" data-original-date=\"2025-06-06T10:04:26-04:00\" data-timestamp=\"1749218666000\" data-timeago-list=\"0\">5 months ago</abbr>\n",
      "</div><ul class=\"resource-statistics\">\n",
      "<li>\n",
      "<span class=\"stat\">1,460</span> views, <span class=\"stat\">60</span> today</li><li>\n",
      "<span class=\"stat\">185</span> downloads, <span class=\"stat\">4</span> today</li>\n",
      "</ul></div>\n",
      "</div><div id=\"resource-options\">\n",
      "<ul class=\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Checking for aria-label or data- attributes with numbers...\n"
     ]
    }
   ],
   "source": [
    "# Search in the raw page source for the stats\n",
    "print(\"=\"*70)\n",
    "print(\"Searching raw HTML for stats...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find \"views\" in the page source\n",
    "if '1,460' in page_source_stats:\n",
    "    # Find the context around it\n",
    "    idx = page_source_stats.find('1,460')\n",
    "    context = page_source_stats[max(0, idx-200):min(len(page_source_stats), idx+200)]\n",
    "    print(\"Found '1,460' in context:\")\n",
    "    print(context)\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Look for data attributes or aria-labels\n",
    "print(\"Checking for aria-label or data- attributes with numbers...\")\n",
    "elements_with_aria = soup_stats.find_all(attrs={'aria-label': True})\n",
    "for elem in elements_with_aria:\n",
    "    aria = elem.get('aria-label')\n",
    "    if re.search(r'\\d', aria):\n",
    "        print(f\"aria-label: {aria}\")\n",
    "        print(f\"Element: <{elem.name} class='{elem.get('class')}'>\")\n",
    "        print(f\"Text: {elem.get_text(strip=True)[:50]}\")\n",
    "        print()\n",
    "\n",
    "# Check data attributes\n",
    "elements_with_data = soup_stats.find_all(attrs=lambda x: any(k.startswith('data-') for k in (x or {})))\n",
    "for elem in elements_with_data[:20]:\n",
    "    data_attrs = {k: v for k, v in elem.attrs.items() if k.startswith('data-')}\n",
    "    if any(re.search(r'\\d', str(v)) for v in data_attrs.values()):\n",
    "        text = elem.get_text(strip=True)\n",
    "        if any(keyword in text.lower() for keyword in ['view', 'download', 'diamond', 'heart', 'today']):\n",
    "            print(f\"Data attrs: {data_attrs}\")\n",
    "            print(f\"Text: {text[:100]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c35e926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Extracting stats from resource-statistics...\n",
      "======================================================================\n",
      "\n",
      "Found 4 stat values:\n",
      "  Stat 1: 1,460\n",
      "  Stat 2: 60\n",
      "  Stat 3: 185\n",
      "  Stat 4: 4\n",
      "\n",
      "Full stats text:\n",
      "  1,460views,60today185downloads,4today\n",
      "\n",
      "LI text: 1,460views,60today\n",
      "Stat values: ['1,460', '60']\n",
      "\n",
      "LI text: 185downloads,4today\n",
      "Stat values: ['185', '4']\n",
      "\n",
      "======================================================================\n",
      "Extracted Stats:\n",
      "======================================================================\n",
      "views: 1,460\n",
      "views_today: 60\n",
      "downloads: 185\n",
      "downloads_today: 4\n",
      "\n",
      "======================================================================\n",
      "Looking for diamonds and hearts...\n",
      "======================================================================\n",
      "\n",
      "Found 2 diamond mentions\n",
      "Found 2 heart/favorite mentions\n"
     ]
    }
   ],
   "source": [
    "# Perfect! Found the structure. Now extract it properly\n",
    "print(\"=\"*70)\n",
    "print(\"Extracting stats from resource-statistics...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Find the resource-statistics ul\n",
    "resource_stats = soup_stats.find('ul', class_='resource-statistics')\n",
    "\n",
    "if resource_stats:\n",
    "    # Get all stat spans\n",
    "    stat_spans = resource_stats.find_all('span', class_='stat')\n",
    "    \n",
    "    print(f\"Found {len(stat_spans)} stat values:\")\n",
    "    for i, span in enumerate(stat_spans):\n",
    "        print(f\"  Stat {i+1}: {span.get_text(strip=True)}\")\n",
    "    \n",
    "    # Get the full text to understand labels\n",
    "    print(f\"\\nFull stats text:\")\n",
    "    print(f\"  {resource_stats.get_text(strip=True)}\")\n",
    "    \n",
    "    # Parse it properly\n",
    "    li_elements = resource_stats.find_all('li')\n",
    "    stats_dict = {}\n",
    "    \n",
    "    for li in li_elements:\n",
    "        text = li.get_text(strip=True)\n",
    "        stat_values = [s.get_text(strip=True) for s in li.find_all('span', class_='stat')]\n",
    "        \n",
    "        print(f\"\\nLI text: {text}\")\n",
    "        print(f\"Stat values: {stat_values}\")\n",
    "        \n",
    "        # Determine what stat this is\n",
    "        if 'view' in text.lower():\n",
    "            if len(stat_values) >= 2:\n",
    "                stats_dict['views'] = stat_values[0]\n",
    "                stats_dict['views_today'] = stat_values[1]\n",
    "        elif 'download' in text.lower():\n",
    "            if len(stat_values) >= 2:\n",
    "                stats_dict['downloads'] = stat_values[0]\n",
    "                stats_dict['downloads_today'] = stat_values[1]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Extracted Stats:\")\n",
    "    print(\"=\"*70)\n",
    "    for key, value in stats_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå resource-statistics ul not found\")\n",
    "\n",
    "# Now look for diamonds and hearts (likely in a different location)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Looking for diamonds and hearts...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Search for elements with \"diamond\" or \"heart\"\n",
    "diamond_elements = soup_stats.find_all(string=re.compile(r'diamond', re.IGNORECASE))\n",
    "heart_elements = soup_stats.find_all(string=re.compile(r'(heart|favorite)', re.IGNORECASE))\n",
    "\n",
    "print(f\"Found {len(diamond_elements)} diamond mentions\")\n",
    "print(f\"Found {len(heart_elements)} heart/favorite mentions\")\n",
    "\n",
    "# Look for specific counters\n",
    "for elem in diamond_elements[:5]:\n",
    "    parent = elem.parent\n",
    "    # Check for numbers nearby\n",
    "    parent_text = parent.get_text(strip=True)\n",
    "    if re.search(r'\\d+', parent_text):\n",
    "        print(f\"\\nDiamond context: {parent_text[:100]}\")\n",
    "        print(f\"Parent: <{parent.name} class='{parent.get('class')}'>\")\n",
    "\n",
    "for elem in heart_elements[:5]:\n",
    "    parent = elem.parent\n",
    "    parent_text = parent.get_text(strip=True)\n",
    "    if re.search(r'\\d+', parent_text) and 'log' not in parent_text.lower():\n",
    "        print(f\"\\nHeart context: {parent_text[:100]}\")\n",
    "        print(f\"Parent: <{parent.name} class='{parent.get('class')}'>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50ca7ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Searching for diamond/heart counts in buttons/links...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Checking for social stats section...\n",
      "======================================================================\n",
      "\n",
      "Text: 51Diamond LogFavorite LogFeature on profileEmbedReport\n",
      "Tag: <ul class='['site-actions', 'r-data']'>\n",
      "\n",
      "======================================================================\n",
      "Searching raw HTML...\n",
      "======================================================================\n",
      "\n",
      "Diamond pattern matches: []\n",
      "Heart pattern matches: [('24', 'favorite')]\n"
     ]
    }
   ],
   "source": [
    "# Look for diamond and heart counts more specifically\n",
    "# They're likely in buttons or links for giving diamonds/hearts\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Searching for diamond/heart counts in buttons/links...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Find all buttons and links\n",
    "buttons = soup_stats.find_all(['button', 'a'])\n",
    "\n",
    "for btn in buttons:\n",
    "    text = btn.get_text(strip=True)\n",
    "    \n",
    "    # Check for diamond\n",
    "    if 'diamond' in text.lower():\n",
    "        # Look for numbers\n",
    "        numbers = re.findall(r'\\d+', text)\n",
    "        if numbers:\n",
    "            print(f\"Diamond button: {text}\")\n",
    "            print(f\"Numbers found: {numbers}\")\n",
    "            print(f\"Classes: {btn.get('class')}\")\n",
    "            print()\n",
    "    \n",
    "    # Check for heart/favorite\n",
    "    if any(keyword in text.lower() for keyword in ['heart', 'favorite', 'follow']):\n",
    "        numbers = re.findall(r'\\d+', text)\n",
    "        if numbers:\n",
    "            print(f\"Heart/Favorite button: {text}\")\n",
    "            print(f\"Numbers found: {numbers}\")\n",
    "            print(f\"Classes: {btn.get('class')}\")\n",
    "            print()\n",
    "\n",
    "# Also check for specific classes\n",
    "print(\"=\"*70)\n",
    "print(\"Checking for social stats section...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Look for common social stat patterns\n",
    "social_elements = soup_stats.find_all(class_=re.compile(r'social|engagement|action', re.IGNORECASE))\n",
    "\n",
    "for elem in social_elements[:10]:\n",
    "    text = elem.get_text(strip=True)\n",
    "    if re.search(r'\\d+', text) and any(keyword in text.lower() for keyword in ['diamond', 'heart', 'favorite', 'follow']):\n",
    "        print(f\"Text: {text[:100]}\")\n",
    "        print(f\"Tag: <{elem.name} class='{elem.get('class')}'>\")\n",
    "        print()\n",
    "\n",
    "# Check raw HTML for diamond/heart counts\n",
    "print(\"=\"*70)\n",
    "print(\"Searching raw HTML...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Look for patterns like \"5 diamonds\" or \"1 heart\"\n",
    "diamond_pattern = r'(\\d+)\\s*(diamond|gem)'\n",
    "heart_pattern = r'(\\d+)\\s*(heart|favorite)'\n",
    "\n",
    "diamond_matches = re.findall(diamond_pattern, page_source_stats, re.IGNORECASE)\n",
    "heart_matches = re.findall(heart_pattern, page_source_stats, re.IGNORECASE)\n",
    "\n",
    "print(f\"Diamond pattern matches: {diamond_matches[:5]}\")\n",
    "print(f\"Heart pattern matches: {heart_matches[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5da0c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Parsing social actions more carefully...\n",
      "======================================================================\n",
      "\n",
      "Found site-actions ul\n",
      "Found 11 action items:\n",
      "\n",
      "======================================================================\n",
      "Looking for specific give-diamond and give-favorite links...\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Found \"51Diamond LogFavorite LogFeature...\" - let's parse this better\n",
    "print(\"=\"*70)\n",
    "print(\"Parsing social actions more carefully...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Find the site-actions ul\n",
    "site_actions = soup_stats.find('ul', class_='site-actions')\n",
    "\n",
    "if site_actions:\n",
    "    print(\"Found site-actions ul\")\n",
    "    \n",
    "    # Get all li elements\n",
    "    li_elements = site_actions.find_all('li')\n",
    "    \n",
    "    print(f\"Found {len(li_elements)} action items:\\n\")\n",
    "    \n",
    "    for li in li_elements:\n",
    "        # Get the link/button text\n",
    "        link = li.find(['a', 'button'])\n",
    "        if link:\n",
    "            text = link.get_text(strip=True)\n",
    "            print(f\"Action: {text}\")\n",
    "            \n",
    "            # Check for numbers at the start\n",
    "            number_match = re.match(r'^(\\d+)', text)\n",
    "            if number_match:\n",
    "                count = number_match.group(1)\n",
    "                remaining_text = text[len(count):]\n",
    "                print(f\"  Count: {count}\")\n",
    "                print(f\"  Label: {remaining_text}\")\n",
    "            \n",
    "            # Check for data attributes\n",
    "            data_attrs = {k: v for k, v in link.attrs.items() if k.startswith('data-')}\n",
    "            if data_attrs:\n",
    "                print(f\"  Data: {data_attrs}\")\n",
    "            print()\n",
    "\n",
    "# Alternative: look for specific action links\n",
    "print(\"=\"*70)\n",
    "print(\"Looking for specific give-diamond and give-favorite links...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "give_diamond = soup_stats.find('a', href=re.compile(r'/give_diamond/'))\n",
    "give_favorite = soup_stats.find('a', href=re.compile(r'/give_favorite/'))\n",
    "\n",
    "if give_diamond:\n",
    "    print(f\"Give Diamond link found:\")\n",
    "    print(f\"  Text: {give_diamond.get_text(strip=True)}\")\n",
    "    print(f\"  Href: {give_diamond.get('href')}\")\n",
    "    \n",
    "if give_favorite:\n",
    "    print(f\"\\nGive Favorite link found:\")\n",
    "    print(f\"  Text: {give_favorite.get_text(strip=True)}\")\n",
    "    print(f\"  Href: {give_favorite.get('href')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "294d0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Raw HTML of site-actions...\n",
      "======================================================================\n",
      "\n",
      "<ul class=\"site-actions r-data\" data-id=\"6638738\" data-type=\"resource\">\n",
      " <li class=\"resource-score\">\n",
      "  <div class=\"c-vote\" title=\"Give diamond!\">\n",
      "   <div class=\"s-rate\">\n",
      "    <div class=\"c-icon\">\n",
      "    </div>\n",
      "   </div>\n",
      "   <span class=\"c-num-votes stat txtlrg\">\n",
      "    5\n",
      "   </span>\n",
      "  </div>\n",
      " </li>\n",
      " <li class=\"c-fav\" title=\"Favorite &amp; follow updates\">\n",
      "  <div class=\"c-icon\">\n",
      "  </div>\n",
      "  <span class=\"c-num-favs stat txtlrg\">\n",
      "   1\n",
      "  </span>\n",
      " </li>\n",
      " <li class=\"c-cmt\" title=\"Comment\">\n",
      "  <div class=\"c-icon\">\n",
      "   <i class=\"material-icons comment\">\n",
      "   </i>\n",
      "  </div>\n",
      "  <span class=\"num_comments stat txtlrg\">\n",
      "  </span>\n",
      " </li>\n",
      " <li class=\"collectable\" data-id=\"6638738\" data-subkey=\"projects\" title=\"Add to collection...\">\n",
      "  <i class=\"material-icons playlist_add\">\n",
      "  </i>\n",
      " </li>\n",
      " <li class=\"action-share submenu_trigger\" data-image=\"https://static.planetminecraft.com/files/image/minecraft/project/2025/738/19211527-medievalhouseb_l.jpg\" data-url=\"/project/medieval-house-fully-decorated-interior-download-6638738\" title=\"Share\">\n",
      "  <i class=\"material-icons share\">\n",
      "  </i>\n",
      "  <div class=\"submenu sharing hidden\">\n",
      "  </div>\n",
      " </li>\n",
      " <li class=\"more-site-actions submenu_trigger\" title=\"More actions...\">\n",
      "  <i class=\"material-icons more_vert\">\n",
      "  </i>\n",
      "  <div class=\"submenu hidden\">\n",
      "   <ul>\n",
      "    <li class=\"who_voted\" rid=\"6638738\" title=\"Who gave Diamonds\">\n",
      "     <i class=\"material-icons md-24 assignment\">\n",
      "     </i>\n",
      "     <span class=\"action-label\">\n",
      "      Diamond Log\n",
      "     </span>\n",
      "    </li>\n",
      "    <li class=\"who_faved\" r\n",
      "\n",
      "======================================================================\n",
      "All links in site-actions:\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the raw structure of site-actions\n",
    "print(\"=\"*70)\n",
    "print(\"Raw HTML of site-actions...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "if site_actions:\n",
    "    # Print the HTML\n",
    "    print(site_actions.prettify()[:1500])\n",
    "    \n",
    "    # Try different approaches\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"All links in site-actions:\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    all_links = site_actions.find_all('a')\n",
    "    for link in all_links[:15]:\n",
    "        text = link.get_text(strip=True)\n",
    "        href = link.get('href', '')\n",
    "        \n",
    "        # Extract number if at start of text\n",
    "        number_match = re.match(r'^(\\d+)', text)\n",
    "        if number_match or any(keyword in text.lower() for keyword in ['diamond', 'favorite', 'heart']):\n",
    "            print(f\"Text: '{text}'\")\n",
    "            print(f\"Href: {href}\")\n",
    "            if number_match:\n",
    "                print(f\"Number: {number_match.group(1)}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c8233",
   "metadata": {},
   "source": [
    "## Step 17: Final Complete Extractor with All Stats\n",
    "\n",
    "Now I have found all the data locations! Let me build the complete extractor:\n",
    "\n",
    "### Data Locations Summary:\n",
    "1. **Basic metadata** ‚Üí JSON-LD CreativeWork\n",
    "2. **Category** ‚Üí JSON-LD `genre` field (\"Map\")\n",
    "3. **Subcategory** ‚Üí From the fixed list, found in tags\n",
    "4. **Views/Downloads** ‚Üí `<ul class=\"resource-statistics\">` with `<span class=\"stat\">`\n",
    "5. **Diamonds** ‚Üí `.c-num-votes.stat` \n",
    "6. **Hearts/Favorites** ‚Üí `.c-num-favs.stat`\n",
    "7. **Dates** ‚Üí JSON-LD `datePublished` and `dateModified`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "382aef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPLETE EXTRACTOR - Testing with Stats Example\n",
      "======================================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/medieval-house-fully-decorated-interior-download-6638738\",\n",
      "  \"title\": \"Medieval House \\u2013 Fully Decorated Interior | Download Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"other\",\n",
      "  \"posted_date\": \"2025-06-06T00:00:00-04:00\",\n",
      "  \"updated_date\": \"2025-10-08T08:11:49-04:00\",\n",
      "  \"tags\": [\n",
      "    \"medieval\",\n",
      "    \"city\",\n",
      "    \"house\",\n",
      "    \"download\",\n",
      "    \"casa\",\n",
      "    \"schematics\",\n",
      "    \"home\",\n",
      "    \"schematic\",\n",
      "    \"rustic\",\n",
      "    \"interior\",\n",
      "    \"fully\",\n",
      "    \"downloadable\",\n",
      "    \"decorated\",\n",
      "    \"other\",\n",
      "    \"litematica\",\n",
      "    \"downloadablemap\",\n",
      "    \"litematic\"\n",
      "  ],\n",
      "  \"description\": \"This Medieval House comes with a fully decorated interior, perfect for adding life and detail to your medieval towns or survival worlds. Download...\",\n",
      "  \"author\": \"Raekon\",\n",
      "  \"views\": \"1,460\",\n",
      "  \"views_today\": \"60\",\n",
      "  \"downloads\": \"185\",\n",
      "  \"downloads_today\": \"4\",\n",
      "  \"diamonds\": \"5\",\n",
      "  \"hearts\": \"1\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"DOWNLOAD ALL MY BUILDS\",\n",
      "      \"url\": \"/project/medieval-house-fully-decorated-interior-download-6638738/download/mirror/737341/\",\n",
      "      \"type\": \"pm_external_redirect\",\n",
      "      \"file_type\": \"unknown\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\",\n",
      "      \"url\": \"https://www.patreon.com/raekon\",\n",
      "      \"type\": \"patreon\",\n",
      "      \"file_type\": \"unknown\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": false\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "Summary:\n",
      "======================================================================\n",
      "Title: Medieval House ‚Äì Fully Decorated Interior | Download Minecraft Map\n",
      "Author: Raekon\n",
      "Category: Map\n",
      "Subcategory: other\n",
      "Posted: 2025-06-06T00:00:00-04:00\n",
      "Updated: 2025-10-08T08:11:49-04:00\n",
      "Views: 1,460 (60 today)\n",
      "Downloads: 185 (4 today)\n",
      "Diamonds: 5\n",
      "Hearts: 1\n",
      "Tags: 17 tags\n",
      "Download links: 2\n"
     ]
    }
   ],
   "source": [
    "def extract_project_metadata_complete(soup, project_url):\n",
    "    \"\"\"\n",
    "    Complete extractor with all metadata and stats from Planet Minecraft\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"url\": project_url,\n",
    "        \"title\": \"N/A\",\n",
    "        \"category\": \"N/A\",\n",
    "        \"subcategory\": \"N/A\",\n",
    "        \"posted_date\": \"N/A\",\n",
    "        \"updated_date\": \"N/A\",\n",
    "        \"tags\": [],\n",
    "        \"description\": \"N/A\",\n",
    "        \"author\": \"N/A\",\n",
    "        \"views\": \"0\",\n",
    "        \"views_today\": \"0\",\n",
    "        \"downloads\": \"0\",\n",
    "        \"downloads_today\": \"0\",\n",
    "        \"diamonds\": \"0\",\n",
    "        \"hearts\": \"0\",\n",
    "        \"download_links\": [],\n",
    "        \"has_direct_download\": False\n",
    "    }\n",
    "    \n",
    "    # Step 1: Extract from JSON-LD\n",
    "    json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    creative_work = None\n",
    "    \n",
    "    for script in json_ld_scripts:\n",
    "        try:\n",
    "            json_data = json.loads(script.string)\n",
    "            if json_data.get('@type') == 'CreativeWork':\n",
    "                creative_work = json_data\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if creative_work:\n",
    "        metadata['title'] = creative_work.get('name', 'N/A')\n",
    "        metadata['description'] = creative_work.get('description', 'N/A')\n",
    "        metadata['posted_date'] = creative_work.get('datePublished', 'N/A')\n",
    "        metadata['updated_date'] = creative_work.get('dateModified', 'N/A')\n",
    "        metadata['category'] = creative_work.get('genre', 'N/A')\n",
    "        \n",
    "        # Extract tags from keywords\n",
    "        keywords = creative_work.get('keywords', '')\n",
    "        if keywords:\n",
    "            metadata['tags'] = [tag.strip() for tag in keywords.split(',')]\n",
    "            \n",
    "            # Detect subcategory from the official list\n",
    "            valid_subcategories = [\n",
    "                '3d-art', 'air-structure', 'challenge-adventure', 'complex', 'educational',\n",
    "                'enviroment-landscaping', 'land-structure', 'minecart', 'music',\n",
    "                'nether-structure', 'piston', 'pixel-art', 'redstone-device',\n",
    "                'underground-structure', 'water-structure', 'other'\n",
    "            ]\n",
    "            \n",
    "            for tag in metadata['tags']:\n",
    "                if tag in valid_subcategories:\n",
    "                    metadata['subcategory'] = tag\n",
    "                    break\n",
    "        \n",
    "        # Extract author\n",
    "        author_data = creative_work.get('author', {})\n",
    "        if isinstance(author_data, dict):\n",
    "            metadata['author'] = author_data.get('name', 'N/A')\n",
    "    \n",
    "    # Step 2: Extract views and downloads stats\n",
    "    resource_stats = soup.find('ul', class_='resource-statistics')\n",
    "    if resource_stats:\n",
    "        li_elements = resource_stats.find_all('li')\n",
    "        \n",
    "        for li in li_elements:\n",
    "            text = li.get_text(strip=True)\n",
    "            stat_values = [s.get_text(strip=True) for s in li.find_all('span', class_='stat')]\n",
    "            \n",
    "            if 'view' in text.lower() and len(stat_values) >= 2:\n",
    "                metadata['views'] = stat_values[0]\n",
    "                metadata['views_today'] = stat_values[1]\n",
    "            elif 'download' in text.lower() and len(stat_values) >= 2:\n",
    "                metadata['downloads'] = stat_values[0]\n",
    "                metadata['downloads_today'] = stat_values[1]\n",
    "    \n",
    "    # Step 3: Extract diamonds and hearts\n",
    "    diamond_elem = soup.find('span', class_='c-num-votes')\n",
    "    if diamond_elem:\n",
    "        metadata['diamonds'] = diamond_elem.get_text(strip=True)\n",
    "    \n",
    "    heart_elem = soup.find('span', class_='c-num-favs')\n",
    "    if heart_elem:\n",
    "        metadata['hearts'] = heart_elem.get_text(strip=True)\n",
    "    \n",
    "    # Step 4: Extract download links\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        text = link.get_text(strip=True)\n",
    "        \n",
    "        # Direct PM downloads - distinguish between world and schematic\n",
    "        if '/download/schematic/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': 'direct_pm_schematic',\n",
    "                'file_type': 'schematic'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "            metadata['has_direct_download'] = True\n",
    "        \n",
    "        elif '/download/world/' in href.lower() or '/download/worldmap/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': 'direct_pm_world',\n",
    "                'file_type': 'world'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "            metadata['has_direct_download'] = True\n",
    "        \n",
    "        # PM redirect links to external hosts\n",
    "        elif '/download/mirror/' in href.lower() or '/download/website/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': 'pm_external_redirect',\n",
    "                'file_type': 'unknown'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "        \n",
    "        # Direct external download hosts\n",
    "        elif any(host in href.lower() for host in ['mediafire.com', 'dropbox.com', 'drive.google.com', 'mega.nz', 'patreon.com']):\n",
    "            link_type = classify_download_link_v2(href, text)\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': link_type,\n",
    "                'file_type': 'unknown'\n",
    "            }\n",
    "            metadata['download_links'].append(download_info)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "# Test on the stats example\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE EXTRACTOR - Testing with Stats Example\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = extract_project_metadata_complete(soup_stats, url_stats)\n",
    "\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"Author: {result['author']}\")\n",
    "print(f\"Category: {result['category']}\")\n",
    "print(f\"Subcategory: {result['subcategory']}\")\n",
    "print(f\"Posted: {result['posted_date']}\")\n",
    "print(f\"Updated: {result['updated_date']}\")\n",
    "print(f\"Views: {result['views']} ({result['views_today']} today)\")\n",
    "print(f\"Downloads: {result['downloads']} ({result['downloads_today']} today)\")\n",
    "print(f\"Diamonds: {result['diamonds']}\")\n",
    "print(f\"Hearts: {result['hearts']}\")\n",
    "print(f\"Tags: {len(result['tags'])} tags\")\n",
    "print(f\"Download links: {len(result['download_links'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9cfc46",
   "metadata": {},
   "source": [
    "## Step 18: Test on New Example - The Sunken Island\n",
    "\n",
    "Let's test the extractor on another project to verify it works across different types of projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6fd3524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: https://www.planetminecraft.com/project/custom-terrain-the-sunken-island/\n",
      "\n",
      "‚úì Page loaded\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION RESULTS:\n",
      "======================================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/custom-terrain-the-sunken-island/\",\n",
      "  \"title\": \"Custom Terrain: The Sunken Island Adventure(1.2.5) Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"challenge-adventure\",\n",
      "  \"posted_date\": \"2011-08-22T00:00:00-04:00\",\n",
      "  \"updated_date\": \"2012-04-02T20:16:28-04:00\",\n",
      "  \"tags\": [\n",
      "    \"adventure\",\n",
      "    \"environment\",\n",
      "    \"landscaping\",\n",
      "    \"challenge\",\n",
      "    \"terrain\",\n",
      "    \"island\",\n",
      "    \"water\",\n",
      "    \"mountains\",\n",
      "    \"sunken\",\n",
      "    \"challenge-adventure\"\n",
      "  ],\n",
      "  \"description\": \"This started as a simple custom terrain, but has now spawned into a highly detailed, full adventure map, with objectives, story line, and ending event....\",\n",
      "  \"author\": \"inHaze\",\n",
      "  \"views\": \"1,493,309\",\n",
      "  \"views_today\": \"25\",\n",
      "  \"downloads\": \"635,293\",\n",
      "  \"downloads_today\": \"2\",\n",
      "  \"diamonds\": \"1,939\",\n",
      "  \"hearts\": \"660\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"Downloadable Map\",\n",
      "      \"url\": \"/project/custom-terrain-the-sunken-island/download/mirror/285279/\",\n",
      "      \"type\": \"pm_external_redirect\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": false\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "SUMMARY:\n",
      "======================================================================\n",
      "Title: Custom Terrain: The Sunken Island Adventure(1.2.5) Minecraft Map\n",
      "Author: inHaze\n",
      "Category: Map\n",
      "Subcategory: challenge-adventure\n",
      "Posted: 2011-08-22T00:00:00-04:00\n",
      "Updated: 2012-04-02T20:16:28-04:00\n",
      "Views: 1,493,309 (25 today)\n",
      "Downloads: 635,293 (2 today)\n",
      "Diamonds: 1,939\n",
      "Hearts: 660\n",
      "Tags: ['adventure', 'environment', 'landscaping', 'challenge', 'terrain', 'island', 'water', 'mountains', 'sunken', 'challenge-adventure']\n",
      "Download links: 1 (['pm_external_redirect'])\n",
      "Has direct download: False\n"
     ]
    }
   ],
   "source": [
    "# Load the new example\n",
    "url_sunken = \"https://www.planetminecraft.com/project/custom-terrain-the-sunken-island/\"\n",
    "print(f\"Loading: {url_sunken}\\n\")\n",
    "\n",
    "driver.get(url_sunken)\n",
    "time.sleep(3)\n",
    "\n",
    "page_source_sunken = driver.page_source\n",
    "soup_sunken = BeautifulSoup(page_source_sunken, 'html.parser')\n",
    "\n",
    "print(\"‚úì Page loaded\\n\")\n",
    "\n",
    "# Extract metadata\n",
    "result_sunken = extract_project_metadata_complete(soup_sunken, url_sunken)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTION RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "print(json.dumps(result_sunken, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Title: {result_sunken['title']}\")\n",
    "print(f\"Author: {result_sunken['author']}\")\n",
    "print(f\"Category: {result_sunken['category']}\")\n",
    "print(f\"Subcategory: {result_sunken['subcategory']}\")\n",
    "print(f\"Posted: {result_sunken['posted_date']}\")\n",
    "print(f\"Updated: {result_sunken['updated_date']}\")\n",
    "print(f\"Views: {result_sunken['views']} ({result_sunken['views_today']} today)\")\n",
    "print(f\"Downloads: {result_sunken['downloads']} ({result_sunken['downloads_today']} today)\")\n",
    "print(f\"Diamonds: {result_sunken['diamonds']}\")\n",
    "print(f\"Hearts: {result_sunken['hearts']}\")\n",
    "print(f\"Tags: {result_sunken['tags']}\")\n",
    "print(f\"Download links: {len(result_sunken['download_links'])} ({[dl['type'] for dl in result_sunken['download_links']]})\")\n",
    "print(f\"Has direct download: {result_sunken['has_direct_download']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369acec",
   "metadata": {},
   "source": [
    "## Step 19: Investigate Download Links - World vs Schematic\n",
    "\n",
    "You're right - we need to distinguish between world downloads and schematic downloads, as they have very different file sizes. Let's examine the actual download links on this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2df8d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Searching for ALL download links...\n",
      "======================================================================\n",
      "\n",
      "Found 10 potential download links:\n",
      "\n",
      "1. Text: 'Part of the starting room with map info, story and rules.'\n",
      "   URL: https://static.planetminecraft.com/files/resource_media/screenshot/1145/2011-11-06_024704_814571.jpg\n",
      "   Classes: ['rsImg']\n",
      "\n",
      "2. Text: 'Download Minecraft Map'\n",
      "   URL: /project/custom-terrain-the-sunken-island/download/worldmap/\n",
      "   Classes: ['branded-download']\n",
      "\n",
      "3. Text: 'Downloadable Map'\n",
      "   URL: /project/custom-terrain-the-sunken-island/download/mirror/285279/\n",
      "   Classes: ['third-party-download', 'branded-download']\n",
      "   ‚ö†Ô∏è Type: PM redirect (need to check destination)\n",
      "\n",
      "4. Text: 'How to install Minecraft Maps on Java Edition'\n",
      "   URL: https://www.planetminecraft.com/blog/how-to-download-and-install-minecraft-maps/\n",
      "   Classes: []\n",
      "\n",
      "5. Text: 'Hillside Manor World [1.8] 4 Year Anniversary'\n",
      "   URL: /project/hillside-manor/\n",
      "   Classes: ['r-title']\n",
      "\n",
      "6. Text: 'Landscape Maps'\n",
      "   URL: /collection/168383/landscape-maps/\n",
      "   Classes: ['collection-title']\n",
      "\n",
      "7. Text: 'Maps & Skins!'\n",
      "   URL: /collection/4883/maps-amp-skins/\n",
      "   Classes: ['collection-title']\n",
      "\n",
      "8. Text: '[Preview] Medieval Starter Map - Moonvale 1.21.8 Vanilla'\n",
      "   URL: /project/preview-medieval-starter-map-moonvale-1-21-8-vanilla/\n",
      "   Classes: ['r-title']\n",
      "\n",
      "9. Text: 'Portal Minecraft Map | Multigenre / Advanture Map'\n",
      "   URL: /project/portal-6744711/\n",
      "   Classes: ['r-title']\n",
      "\n",
      "10. Text: 'Subconscious | a solo adventure map'\n",
      "   URL: /project/subconscious-a-solo-adventure-map/\n",
      "   Classes: ['r-title']\n",
      "\n",
      "======================================================================\n",
      "Checking raw HTML for download patterns...\n",
      "======================================================================\n",
      "\n",
      "Found 1 unique download URLs:\n",
      "\n",
      "  /download/mirror/285279/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for ALL download-related links on The Sunken Island page\n",
    "print(\"=\"*70)\n",
    "print(\"Searching for ALL download links...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "all_links = soup_sunken.find_all('a', href=True)\n",
    "download_related = []\n",
    "\n",
    "for link in all_links:\n",
    "    href = link['href']\n",
    "    text = link.get_text(strip=True)\n",
    "    \n",
    "    # Look for anything download-related\n",
    "    if any(keyword in href.lower() for keyword in ['download', 'mirror', 'website']) or \\\n",
    "       any(keyword in text.lower() for keyword in ['download', 'world', 'schematic', 'map']):\n",
    "        \n",
    "        # Skip navigation/tag links\n",
    "        if '/tags/' not in href and '/projects/' not in href and text:\n",
    "            download_related.append({\n",
    "                'text': text,\n",
    "                'href': href,\n",
    "                'classes': link.get('class', [])\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(download_related)} potential download links:\\n\")\n",
    "\n",
    "for i, link in enumerate(download_related[:20], 1):\n",
    "    print(f\"{i}. Text: '{link['text']}'\")\n",
    "    print(f\"   URL: {link['href']}\")\n",
    "    print(f\"   Classes: {link['classes']}\")\n",
    "    \n",
    "    # Check if it's a direct PM hosted file\n",
    "    if '/download/schematic/' in link['href']:\n",
    "        print(f\"   ‚ö†Ô∏è Type: Direct PM SCHEMATIC download\")\n",
    "    elif '/download/world/' in link['href']:\n",
    "        print(f\"   ‚ö†Ô∏è Type: Direct PM WORLD download\")\n",
    "    elif '/download/mirror/' in link['href'] or '/download/website/' in link['href']:\n",
    "        print(f\"   ‚ö†Ô∏è Type: PM redirect (need to check destination)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Check the raw HTML around download links\n",
    "print(\"=\"*70)\n",
    "print(\"Checking raw HTML for download patterns...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "if '/download/' in page_source_sunken:\n",
    "    # Find all download URLs in the source\n",
    "    download_urls = re.findall(r'/download/\\w+/\\d+/', page_source_sunken)\n",
    "    unique_urls = list(set(download_urls))\n",
    "    \n",
    "    print(f\"Found {len(unique_urls)} unique download URLs:\\n\")\n",
    "    for url in unique_urls[:10]:\n",
    "        print(f\"  {url}\")\n",
    "        \n",
    "        # Get context around this URL\n",
    "        idx = page_source_sunken.find(url)\n",
    "        if idx > 0:\n",
    "            context = page_source_sunken[max(0, idx-150):min(len(page_source_sunken), idx+150)]\n",
    "            # Look for keywords\n",
    "            if 'world' in context.lower():\n",
    "                print(f\"    ‚Üí Contains 'world' keyword\")\n",
    "            if 'schematic' in context.lower():\n",
    "                print(f\"    ‚Üí Contains 'schematic' keyword\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64041d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Examining the /download/worldmap/ link...\n",
      "======================================================================\n",
      "\n",
      "‚úì Found worldmap download link!\n",
      "Text: 'Download Minecraft Map'\n",
      "Href: /project/custom-terrain-the-sunken-island/download/worldmap/\n",
      "Classes: ['branded-download']\n",
      "Title: Download this file for Minecraft.\n",
      "\n",
      "Parent tag: <li>\n",
      "Parent text: Download Minecraft Map\n",
      "\n",
      "======================================================================\n",
      "Checking for schematic download patterns...\n",
      "======================================================================\n",
      "\n",
      "‚ùå No schematic download link on this page (expected - it's a world map)\n",
      "\n",
      "======================================================================\n",
      "DOWNLOAD PATTERN SUMMARY:\n",
      "======================================================================\n",
      "\n",
      "Direct PM Downloads:\n",
      "  /download/worldmap/      ‚Üí Full world download (large file)\n",
      "  /download/world/         ‚Üí World download (alternative pattern)\n",
      "  /download/schematic/     ‚Üí Schematic file (smaller, structure only)\n",
      "\n",
      "External Redirects:\n",
      "  /download/mirror/XXXX/   ‚Üí Redirect to external host\n",
      "  /download/website/XXXX/  ‚Üí Redirect to creator's website\n"
     ]
    }
   ],
   "source": [
    "# Examine the worldmap download link more carefully\n",
    "print(\"=\"*70)\n",
    "print(\"Examining the /download/worldmap/ link...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "worldmap_link = soup_sunken.find('a', href=re.compile(r'/download/worldmap/'))\n",
    "\n",
    "if worldmap_link:\n",
    "    print(\"‚úì Found worldmap download link!\")\n",
    "    print(f\"Text: '{worldmap_link.get_text(strip=True)}'\")\n",
    "    print(f\"Href: {worldmap_link.get('href')}\")\n",
    "    print(f\"Classes: {worldmap_link.get('class')}\")\n",
    "    print(f\"Title: {worldmap_link.get('title')}\")\n",
    "    \n",
    "    # Get parent context\n",
    "    parent = worldmap_link.parent\n",
    "    print(f\"\\nParent tag: <{parent.name}>\")\n",
    "    print(f\"Parent text: {parent.get_text(strip=True)}\")\n",
    "else:\n",
    "    print(\"‚ùå worldmap link not found\")\n",
    "\n",
    "# Check for schematic pattern too\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Checking for schematic download patterns...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "schematic_link = soup_sunken.find('a', href=re.compile(r'/download/schematic/'))\n",
    "if schematic_link:\n",
    "    print(\"‚úì Found schematic download link!\")\n",
    "    print(f\"Text: '{schematic_link.get_text(strip=True)}'\")\n",
    "    print(f\"Href: {schematic_link.get('href')}\")\n",
    "else:\n",
    "    print(\"‚ùå No schematic download link on this page (expected - it's a world map)\")\n",
    "\n",
    "# Summary of patterns\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOAD PATTERN SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDirect PM Downloads:\")\n",
    "print(\"  /download/worldmap/      ‚Üí Full world download (large file)\")\n",
    "print(\"  /download/world/         ‚Üí World download (alternative pattern)\")\n",
    "print(\"  /download/schematic/     ‚Üí Schematic file (smaller, structure only)\")\n",
    "print(\"\\nExternal Redirects:\")\n",
    "print(\"  /download/mirror/XXXX/   ‚Üí Redirect to external host\")\n",
    "print(\"  /download/website/XXXX/  ‚Üí Redirect to creator's website\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517ea43",
   "metadata": {},
   "source": [
    "## Step 20: Test Updated Extractor with World vs Schematic Detection\n",
    "\n",
    "Now let's test the updated extractor that properly distinguishes between world files and schematic files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "593c919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "UPDATED EXTRACTION - The Sunken Island\n",
      "======================================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/custom-terrain-the-sunken-island/\",\n",
      "  \"title\": \"Custom Terrain: The Sunken Island Adventure(1.2.5) Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"challenge-adventure\",\n",
      "  \"posted_date\": \"2011-08-22T00:00:00-04:00\",\n",
      "  \"updated_date\": \"2012-04-02T20:16:28-04:00\",\n",
      "  \"tags\": [\n",
      "    \"adventure\",\n",
      "    \"environment\",\n",
      "    \"landscaping\",\n",
      "    \"challenge\",\n",
      "    \"terrain\",\n",
      "    \"island\",\n",
      "    \"water\",\n",
      "    \"mountains\",\n",
      "    \"sunken\",\n",
      "    \"challenge-adventure\"\n",
      "  ],\n",
      "  \"description\": \"This started as a simple custom terrain, but has now spawned into a highly detailed, full adventure map, with objectives, story line, and ending event....\",\n",
      "  \"author\": \"inHaze\",\n",
      "  \"views\": \"1,493,309\",\n",
      "  \"views_today\": \"25\",\n",
      "  \"downloads\": \"635,293\",\n",
      "  \"downloads_today\": \"2\",\n",
      "  \"diamonds\": \"1,939\",\n",
      "  \"hearts\": \"660\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"Downloadable Map\",\n",
      "      \"url\": \"/project/custom-terrain-the-sunken-island/download/mirror/285279/\",\n",
      "      \"type\": \"pm_external_redirect\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": false\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "DOWNLOAD LINKS DETAILS:\n",
      "======================================================================\n",
      "\n",
      "1. Downloadable Map\n",
      "   URL: /project/custom-terrain-the-sunken-island/download/mirror/285279/\n",
      "   Type: pm_external_redirect\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdl[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdl[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   File Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdl\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfile_type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result_updated[\u001b[33m'\u001b[39m\u001b[33mdownload_links\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m download link(s)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'file_type'"
     ]
    }
   ],
   "source": [
    "# Test the updated extractor on The Sunken Island\n",
    "result_updated = extract_project_metadata_complete(soup_sunken, url_sunken)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"UPDATED EXTRACTION - The Sunken Island\")\n",
    "print(\"=\"*70)\n",
    "print(json.dumps(result_updated, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOAD LINKS DETAILS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, dl in enumerate(result_updated['download_links'], 1):\n",
    "    print(f\"\\n{i}. {dl['text']}\")\n",
    "    print(f\"   URL: {dl['url']}\")\n",
    "    print(f\"   Type: {dl['type']}\")\n",
    "    print(f\"   File Type: {dl['file_type']}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úì Found {len(result_updated['download_links'])} download link(s)\")\n",
    "print(f\"‚úì Has direct download: {result_updated['has_direct_download']}\")\n",
    "\n",
    "# Also test on the first example (GK First Church) to verify it detects schematic\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"TESTING ON GK FIRST CHURCH (should have schematic)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result_church = extract_project_metadata_complete(soup, url)\n",
    "\n",
    "print(\"\\nDownload links:\")\n",
    "for i, dl in enumerate(result_church['download_links'], 1):\n",
    "    print(f\"{i}. {dl['text']}\")\n",
    "    print(f\"   Type: {dl['type']}\")\n",
    "    print(f\"   File Type: {dl['file_type']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0bf91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEBUG: Checking what links are being found...\n",
      "======================================================================\n",
      "\n",
      "World links found: 1\n",
      "  - Download Minecraft Map: /project/custom-terrain-the-sunken-island/download/worldmap/\n",
      "\n",
      "Schematic links found: 0\n",
      "\n",
      "Mirror/Website links found: 1\n",
      "  - Downloadable Map: /project/custom-terrain-the-sunken-island/download/mirror/285279/\n",
      "\n",
      "======================================================================\n",
      "The world link exists but wasn't captured by the old function!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# The function needs to be re-run. Let me check if the world link is being captured\n",
    "print(\"=\"*70)\n",
    "print(\"DEBUG: Checking what links are being found...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "all_links_check = soup_sunken.find_all('a', href=True)\n",
    "\n",
    "world_links = []\n",
    "schematic_links = []\n",
    "mirror_links = []\n",
    "\n",
    "for link in all_links_check:\n",
    "    href = link['href']\n",
    "    text = link.get_text(strip=True)\n",
    "    \n",
    "    if '/download/worldmap/' in href or '/download/world/' in href:\n",
    "        world_links.append({'text': text, 'href': href})\n",
    "    elif '/download/schematic/' in href:\n",
    "        schematic_links.append({'text': text, 'href': href})\n",
    "    elif '/download/mirror/' in href or '/download/website/' in href:\n",
    "        mirror_links.append({'text': text, 'href': href})\n",
    "\n",
    "print(f\"World links found: {len(world_links)}\")\n",
    "for wl in world_links:\n",
    "    print(f\"  - {wl['text']}: {wl['href']}\")\n",
    "\n",
    "print(f\"\\nSchematic links found: {len(schematic_links)}\")\n",
    "for sl in schematic_links:\n",
    "    print(f\"  - {sl['text']}: {sl['href']}\")\n",
    "\n",
    "print(f\"\\nMirror/Website links found: {len(mirror_links)}\")\n",
    "for ml in mirror_links:\n",
    "    print(f\"  - {ml['text']}: {ml['href']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"The world link exists but wasn't captured by the old function!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65ee5d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FIXED EXTRACTION - The Sunken Island (with world detection)\n",
      "======================================================================\n",
      "\n",
      "Total download links found: 2\n",
      "\n",
      "1. Download Minecraft Map\n",
      "   URL: /project/custom-terrain-the-sunken-island/download/worldmap/\n",
      "   Type: direct_pm_world\n",
      "   File Type: world\n",
      "\n",
      "2. Downloadable Map\n",
      "   URL: /project/custom-terrain-the-sunken-island/download/mirror/285279/\n",
      "   Type: pm_external_redirect\n",
      "   File Type: unknown\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION:\n",
      "======================================================================\n",
      "‚úì Has direct download: True\n",
      "‚úì World downloads: 1\n",
      "‚úì Schematic downloads: 0\n",
      "‚úì External/Unknown: 1\n",
      "\n",
      "======================================================================\n",
      "SUCCESS! Now capturing:\n",
      "  - Direct PM world download (/download/worldmap/)\n",
      "  - External redirect (/download/mirror/)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Re-extract with the updated function\n",
    "result_sunken_fixed = extract_project_metadata_complete(soup_sunken, url_sunken)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FIXED EXTRACTION - The Sunken Island (with world detection)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"Total download links found: {len(result_sunken_fixed['download_links'])}\\n\")\n",
    "\n",
    "for i, dl in enumerate(result_sunken_fixed['download_links'], 1):\n",
    "    print(f\"{i}. {dl['text']}\")\n",
    "    print(f\"   URL: {dl['url']}\")\n",
    "    print(f\"   Type: {dl['type']}\")\n",
    "    print(f\"   File Type: {dl['file_type']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICATION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úì Has direct download: {result_sunken_fixed['has_direct_download']}\")\n",
    "\n",
    "# Count by type\n",
    "world_count = sum(1 for dl in result_sunken_fixed['download_links'] if dl['file_type'] == 'world')\n",
    "schematic_count = sum(1 for dl in result_sunken_fixed['download_links'] if dl['file_type'] == 'schematic')\n",
    "external_count = sum(1 for dl in result_sunken_fixed['download_links'] if dl['file_type'] == 'unknown')\n",
    "\n",
    "print(f\"‚úì World downloads: {world_count}\")\n",
    "print(f\"‚úì Schematic downloads: {schematic_count}\")\n",
    "print(f\"‚úì External/Unknown: {external_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUCCESS! Now capturing:\")\n",
    "print(\"  - Direct PM world download (/download/worldmap/)\")\n",
    "print(\"  - External redirect (/download/mirror/)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e9bff",
   "metadata": {},
   "source": [
    "## Step 21: Test on \"Finally Hotel\"\n",
    "\n",
    "Let's test another project to verify the extractor works consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f079880b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: https://www.planetminecraft.com/project/finally-hotel/\n",
      "\n",
      "‚úì Page loaded\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION RESULTS - Finally Hotel\n",
      "======================================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/finally-hotel/\",\n",
      "  \"title\": \"Minecraft Hotel / Resort | 3D View | Map Download Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"land-structure\",\n",
      "  \"posted_date\": \"2013-11-01T00:00:00-04:00\",\n",
      "  \"updated_date\": \"2022-06-24T11:30:38-04:00\",\n",
      "  \"tags\": [\n",
      "    \"city\",\n",
      "    \"land-structure\",\n",
      "    \"complex\",\n",
      "    \"minecraft\",\n",
      "    \"building\",\n",
      "    \"modern\",\n",
      "    \"realistic\",\n",
      "    \"download\",\n",
      "    \"beach\",\n",
      "    \"hotel\",\n",
      "    \"real\",\n",
      "    \"pool\",\n",
      "    \"realism\",\n",
      "    \"schematic\",\n",
      "    \"casino\",\n",
      "    \"hall\",\n",
      "    \"3d\",\n",
      "    \"large\",\n",
      "    \"resort\",\n",
      "    \"sauna\",\n",
      "    \"gym\",\n",
      "    \"bar\",\n",
      "    \"megastructure\",\n",
      "    \"scheme\",\n",
      "    \"spa\",\n",
      "    \"modernhotel\",\n",
      "    \"indoorpool\",\n",
      "    \"top\",\n",
      "    \"minecrafthotel\",\n",
      "    \"minecraftresort\"\n",
      "  ],\n",
      "  \"description\": \"This is my 1st attempt at a hotel, took me about 4 months to complete. It features an indoor pool, outdoor pool, pool bar, spa, sauna, casino, gym and...\",\n",
      "  \"author\": \"NikGrin417\",\n",
      "  \"views\": \"589,541\",\n",
      "  \"views_today\": \"23\",\n",
      "  \"downloads\": \"218,260\",\n",
      "  \"downloads_today\": \"2\",\n",
      "  \"diamonds\": \"348\",\n",
      "  \"hearts\": \"309\",\n",
      "  \"download_links\": [\n",
      "    {\n",
      "      \"text\": \"Download Schematic\",\n",
      "      \"url\": \"/project/finally-hotel/download/schematic/\",\n",
      "      \"type\": \"direct_pm_schematic\",\n",
      "      \"file_type\": \"schematic\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Downloadable Map\",\n",
      "      \"url\": \"/project/finally-hotel/download/mirror/435103/\",\n",
      "      \"type\": \"pm_external_redirect\",\n",
      "      \"file_type\": \"unknown\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": true\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "SUMMARY:\n",
      "======================================================================\n",
      "Title: Minecraft Hotel / Resort | 3D View | Map Download Minecraft Map\n",
      "Author: NikGrin417\n",
      "Category: Map\n",
      "Subcategory: land-structure\n",
      "Posted: 2013-11-01T00:00:00-04:00\n",
      "Updated: 2022-06-24T11:30:38-04:00\n",
      "Views: 589,541 (23 today)\n",
      "Downloads: 218,260 (2 today)\n",
      "Diamonds: 348\n",
      "Hearts: 309\n",
      "\n",
      "Download Links (2):\n",
      "  1. Download Schematic\n",
      "     Type: direct_pm_schematic\n",
      "     File: schematic\n",
      "  2. Downloadable Map\n",
      "     Type: pm_external_redirect\n",
      "     File: unknown\n",
      "\n",
      "‚úì Has direct download: True\n"
     ]
    }
   ],
   "source": [
    "# Load the Finally Hotel project\n",
    "url_hotel = \"https://www.planetminecraft.com/project/finally-hotel/\"\n",
    "print(f\"Loading: {url_hotel}\\n\")\n",
    "\n",
    "driver.get(url_hotel)\n",
    "time.sleep(3)\n",
    "\n",
    "page_source_hotel = driver.page_source\n",
    "soup_hotel = BeautifulSoup(page_source_hotel, 'html.parser')\n",
    "\n",
    "print(\"‚úì Page loaded\\n\")\n",
    "\n",
    "# Extract metadata\n",
    "result_hotel = extract_project_metadata_complete(soup_hotel, url_hotel)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTION RESULTS - Finally Hotel\")\n",
    "print(\"=\"*70)\n",
    "print(json.dumps(result_hotel, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Title: {result_hotel['title']}\")\n",
    "print(f\"Author: {result_hotel['author']}\")\n",
    "print(f\"Category: {result_hotel['category']}\")\n",
    "print(f\"Subcategory: {result_hotel['subcategory']}\")\n",
    "print(f\"Posted: {result_hotel['posted_date']}\")\n",
    "print(f\"Updated: {result_hotel['updated_date']}\")\n",
    "print(f\"Views: {result_hotel['views']} ({result_hotel['views_today']} today)\")\n",
    "print(f\"Downloads: {result_hotel['downloads']} ({result_hotel['downloads_today']} today)\")\n",
    "print(f\"Diamonds: {result_hotel['diamonds']}\")\n",
    "print(f\"Hearts: {result_hotel['hearts']}\")\n",
    "\n",
    "print(f\"\\nDownload Links ({len(result_hotel['download_links'])}):\")\n",
    "for i, dl in enumerate(result_hotel['download_links'], 1):\n",
    "    print(f\"  {i}. {dl['text'][:40]}\")\n",
    "    print(f\"     Type: {dl['type']}\")\n",
    "    print(f\"     File: {dl['file_type']}\")\n",
    "\n",
    "print(f\"\\n‚úì Has direct download: {result_hotel['has_direct_download']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f953b",
   "metadata": {},
   "source": [
    "## Step 22: Discovering All Projects - Exploration\n",
    "\n",
    "Planet Minecraft doesn't have sequential IDs, so we need to find how to discover all projects. Let's explore:\n",
    "\n",
    "### Possible approaches:\n",
    "1. **Browse/Search pages** - Categories, tags, sort by date/popularity\n",
    "2. **Sitemap** - XML sitemaps for search engines\n",
    "3. **API** - Check if they have a public API\n",
    "4. **RSS/Feeds** - RSS feeds for new content\n",
    "5. **Pagination** - Browse pages with filters\n",
    "\n",
    "Let's start investigating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd130768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Approach 1: Browse/Search Pages\n",
      "======================================================================\n",
      "\n",
      "Loading: https://www.planetminecraft.com/projects/\n",
      "\n",
      "‚úì Page loaded\n",
      "\n",
      "Found 34 unique project links on the browse page\n",
      "\n",
      "First 10 projects:\n",
      "  /project/big-yellow-crane/\n",
      "  /project/greenspire-island-1k-custom-survival-friendly-world-1-21/\n",
      "  /project/containment-field/\n",
      "  /project/suquare-colosseum-of-rome-palace-of-italian-civilization/\n",
      "  /project/f-e-a-r/\n",
      "  /project/small-contemporary-house/\n",
      "  /project/preview-medieval-starter-map-moonvale-1-21-8-vanilla/\n",
      "  /project/brownstone-halloween-house-download/\n",
      "  /project/a-cottage-in-the-woods-4341009/\n",
      "  /project/reuterstra-e-6b-townhouse-bonn-germany/\n",
      "\n",
      "======================================================================\n",
      "Checking for pagination...\n",
      "======================================================================\n",
      "\n",
      "Found 0 pagination links\n",
      "======================================================================\n",
      "Checking for filters and sorting options...\n",
      "======================================================================\n",
      "\n",
      "Found 0 select dropdowns\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: Check the main projects browse page\n",
    "print(\"=\"*70)\n",
    "print(\"Approach 1: Browse/Search Pages\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "browse_url = \"https://www.planetminecraft.com/projects/\"\n",
    "print(f\"Loading: {browse_url}\\n\")\n",
    "\n",
    "driver.get(browse_url)\n",
    "time.sleep(3)\n",
    "\n",
    "page_source_browse = driver.page_source\n",
    "soup_browse = BeautifulSoup(page_source_browse, 'html.parser')\n",
    "\n",
    "print(\"‚úì Page loaded\\n\")\n",
    "\n",
    "# Look for project links\n",
    "project_links = []\n",
    "all_links = soup_browse.find_all('a', href=True)\n",
    "\n",
    "for link in all_links:\n",
    "    href = link['href']\n",
    "    # Project URLs have pattern /project/SLUG/\n",
    "    if re.match(r'^/project/[^/]+/$', href):\n",
    "        project_links.append(href)\n",
    "\n",
    "# Remove duplicates\n",
    "unique_projects = list(set(project_links))\n",
    "\n",
    "print(f\"Found {len(unique_projects)} unique project links on the browse page\")\n",
    "print(\"\\nFirst 10 projects:\")\n",
    "for proj in unique_projects[:10]:\n",
    "    print(f\"  {proj}\")\n",
    "\n",
    "# Look for pagination\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Checking for pagination...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Look for \"next page\" or page numbers\n",
    "pagination_links = soup_browse.find_all('a', href=re.compile(r'page=|/p\\d+'))\n",
    "print(f\"Found {len(pagination_links)} pagination links\")\n",
    "\n",
    "for pag_link in pagination_links[:5]:\n",
    "    print(f\"  Text: '{pag_link.get_text(strip=True)}'\")\n",
    "    print(f\"  URL: {pag_link.get('href')}\")\n",
    "    print()\n",
    "\n",
    "# Check for filters/sorting\n",
    "print(\"=\"*70)\n",
    "print(\"Checking for filters and sorting options...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Look for select elements or filter links\n",
    "selects = soup_browse.find_all('select')\n",
    "print(f\"Found {len(selects)} select dropdowns\")\n",
    "\n",
    "for sel in selects[:5]:\n",
    "    name = sel.get('name', 'no name')\n",
    "    print(f\"\\nSelect: {name}\")\n",
    "    options = sel.find_all('option')\n",
    "    print(f\"  Options ({len(options)}):\")\n",
    "    for opt in options[:8]:\n",
    "        print(f\"    - {opt.get('value')}: {opt.get_text(strip=True)}\")\n",
    "    if len(options) > 8:\n",
    "        print(f\"    ... and {len(options)-8} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b4b041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Examining page structure for pagination...\n",
      "======================================================================\n",
      "\n",
      "Links containing 'next': 0\n",
      "\n",
      "Links that are just numbers: 0\n",
      "\n",
      "Elements containing 'load more': 0\n",
      "\n",
      "======================================================================\n",
      "Current page URL structure\n",
      "======================================================================\n",
      "URL: https://www.planetminecraft.com/projects/\n",
      "\n",
      "======================================================================\n",
      "Checking for infinite scroll indicators...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the page more carefully for pagination\n",
    "# Often pagination is handled with JavaScript, so we need to look for different patterns\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Examining page structure for pagination...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Look for \"next\" buttons or pagination elements\n",
    "next_buttons = soup.find_all('a', string=lambda s: s and 'next' in s.lower())\n",
    "print(f\"\\nLinks containing 'next': {len(next_buttons)}\")\n",
    "for btn in next_buttons[:5]:\n",
    "    print(f\"  Text: {btn.get_text(strip=True)}, href: {btn.get('href')}\")\n",
    "\n",
    "# Look for page numbers\n",
    "page_numbers = soup.find_all('a', string=lambda s: s and s.strip().isdigit())\n",
    "print(f\"\\nLinks that are just numbers: {len(page_numbers)}\")\n",
    "for num in page_numbers[:5]:\n",
    "    print(f\"  Text: {num.get_text(strip=True)}, href: {num.get('href')}\")\n",
    "\n",
    "# Look for common pagination class names\n",
    "pagination_classes = ['pagination', 'pager', 'pages', 'page-numbers']\n",
    "for cls in pagination_classes:\n",
    "    elements = soup.find_all(class_=lambda c: c and cls in c.lower())\n",
    "    if elements:\n",
    "        print(f\"\\nFound elements with class containing '{cls}': {len(elements)}\")\n",
    "        for elem in elements[:2]:\n",
    "            print(f\"  Classes: {elem.get('class')}\")\n",
    "            print(f\"  Content preview: {elem.get_text(strip=True)[:100]}\")\n",
    "\n",
    "# Check for \"load more\" buttons\n",
    "load_more = soup.find_all(string=lambda s: s and 'load more' in s.lower())\n",
    "print(f\"\\nElements containing 'load more': {len(load_more)}\")\n",
    "\n",
    "# Check the URL structure - maybe they use query parameters\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Current page URL structure\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"URL: {driver.current_url}\")\n",
    "\n",
    "# Check if there's an infinite scroll mechanism\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Checking for infinite scroll indicators...\")\n",
    "print(\"=\" * 70)\n",
    "scripts = soup.find_all('script')\n",
    "for script in scripts:\n",
    "    if script.string and 'infinite' in script.string.lower():\n",
    "        print(\"Found script mentioning 'infinite'\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93251282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing different URL patterns...\n",
      "======================================================================\n",
      "\n",
      "Testing: https://www.planetminecraft.com/projects/?p=2\n",
      "  Final URL: https://www.planetminecraft.com/projects/?p=2\n",
      "  Found 25 unique project links\n",
      "  First project: /project/bulldozer-6736655/\n",
      "\n",
      "Testing: https://www.planetminecraft.com/projects/?page=2\n",
      "  Final URL: https://www.planetminecraft.com/projects/?page=2\n",
      "  Found 33 unique project links\n",
      "  First project: /project/big-yellow-crane/\n",
      "\n",
      "Testing: https://www.planetminecraft.com/projects/page/2/\n",
      "  Final URL: https://www.planetminecraft.com/projects/\n",
      "  Found 33 unique project links\n",
      "  First project: /project/big-yellow-crane/\n"
     ]
    }
   ],
   "source": [
    "# Try different URL patterns to see if they support pagination\n",
    "# Common patterns: /projects/?p=2, /projects/page/2/, /projects/?page=2, etc.\n",
    "\n",
    "test_urls = [\n",
    "    \"https://www.planetminecraft.com/projects/?p=2\",\n",
    "    \"https://www.planetminecraft.com/projects/?page=2\",\n",
    "    \"https://www.planetminecraft.com/projects/page/2/\",\n",
    "    \"https://www.planetminecraft.com/projects/?offset=50\",\n",
    "    \"https://www.planetminecraft.com/projects/order/rating/\",\n",
    "    \"https://www.planetminecraft.com/projects/order/popular/\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Testing different URL patterns...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for test_url in test_urls[:3]:  # Test first 3\n",
    "    print(f\"\\nTesting: {test_url}\")\n",
    "    driver.get(test_url)\n",
    "    time.sleep(2)  # Wait for page to load\n",
    "    \n",
    "    test_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    project_links = test_soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "    unique_links = set([link.get('href').split('?')[0] for link in project_links])\n",
    "    \n",
    "    print(f\"  Final URL: {driver.current_url}\")\n",
    "    print(f\"  Found {len(unique_links)} unique project links\")\n",
    "    \n",
    "    if len(unique_links) > 0:\n",
    "        first_project = list(unique_links)[0]\n",
    "        print(f\"  First project: {first_project}\")\n",
    "        \n",
    "# Go back to main page\n",
    "driver.get(\"https://www.planetminecraft.com/projects/\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92c57bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Verifying pagination with ?p=N parameter\n",
      "======================================================================\n",
      "\n",
      "Loading page 1...\n",
      "  Found 33 unique project links\n",
      "  First 3: ['/project/big-yellow-crane/', '/project/greenspire-island-1k-custom-survival-friendly-world-1-21/', '/project/space-station-end-hub/']\n",
      "\n",
      "Loading page 2...\n",
      "  Found 25 unique project links\n",
      "  First 3: ['/project/bulldozer-6736655/', '/project/iralaya-5k-custom-terrain-map-download-12-custom-biomes-1-20-java-bedrock-varuna-studios/', '/project/fnaf-fall-fest-minecraft-tutorial-series-map/']\n",
      "\n",
      "Loading page 3...\n",
      "  Found 25 unique project links\n",
      "  First 3: ['/project/big-yellow-crane/', '/project/dental-clinic-littletiles-minecraft/', '/project/basic-house-6744566/']\n",
      "\n",
      "======================================================================\n",
      "Checking for overlap between pages...\n",
      "======================================================================\n",
      "Page 1 & 2 overlap: 0 projects\n",
      "Page 2 & 3 overlap: 0 projects\n",
      "Page 1 & 3 overlap: 1 projects\n",
      "\n",
      "Total unique projects across pages 1-3: 82\n",
      "\n",
      "======================================================================\n",
      "Finding the last page...\n",
      "======================================================================\n",
      "Navigating to page 999999 redirects to: https://www.planetminecraft.com/projects/?p=999999\n",
      "Found 25 projects on that page\n"
     ]
    }
   ],
   "source": [
    "# Verify that ?p=N gives us different pages\n",
    "# Let's compare page 1, 2, and 3\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Verifying pagination with ?p=N parameter\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pages_data = {}\n",
    "\n",
    "for page_num in [1, 2, 3]:\n",
    "    url = f\"https://www.planetminecraft.com/projects/?p={page_num}\"\n",
    "    print(f\"\\nLoading page {page_num}...\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    project_links = page_soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "    unique_links = set([link.get('href').split('?')[0] for link in project_links])\n",
    "    \n",
    "    pages_data[page_num] = unique_links\n",
    "    print(f\"  Found {len(unique_links)} unique project links\")\n",
    "    print(f\"  First 3: {list(unique_links)[:3]}\")\n",
    "\n",
    "# Check for overlap\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Checking for overlap between pages...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "overlap_1_2 = pages_data[1].intersection(pages_data[2])\n",
    "overlap_2_3 = pages_data[2].intersection(pages_data[3])\n",
    "overlap_1_3 = pages_data[1].intersection(pages_data[3])\n",
    "\n",
    "print(f\"Page 1 & 2 overlap: {len(overlap_1_2)} projects\")\n",
    "print(f\"Page 2 & 3 overlap: {len(overlap_2_3)} projects\")\n",
    "print(f\"Page 1 & 3 overlap: {len(overlap_1_3)} projects\")\n",
    "\n",
    "print(f\"\\nTotal unique projects across pages 1-3: {len(pages_data[1] | pages_data[2] | pages_data[3])}\")\n",
    "\n",
    "# Try to find the last page by testing higher page numbers\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Finding the last page...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Try a very high page number\n",
    "driver.get(\"https://www.planetminecraft.com/projects/?p=999999\")\n",
    "time.sleep(2)\n",
    "print(f\"Navigating to page 999999 redirects to: {driver.current_url}\")\n",
    "\n",
    "test_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "test_links = test_soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "unique_test = set([link.get('href').split('?')[0] for link in test_links])\n",
    "print(f\"Found {len(unique_test)} projects on that page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "028c627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Checking for total project count on the page...\n",
      "======================================================================\n",
      "\n",
      "Text containing count keywords:\n",
      "  {\"@context\":\"http:\\/\\/schema.org\\/\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"item\":{\"@id\":\"https:\\/\\/www.planetminecraft.com\\/projects\\/\",\"name\":\"Minecraft Maps\"}}]}\n",
      "\n",
      "======================================================================\n",
      "Checking available filters/categories...\n",
      "======================================================================\n",
      "\n",
      "Found 53 filter/category links\n",
      "Unique filter URLs: 49\n",
      "  /projects/?monetization=0\n",
      "  /projects/?monetization=1\n",
      "  /projects/?monetization=2\n",
      "  /projects/?monetization=any\n",
      "  /projects/?order=order_downloads\n",
      "  /projects/?order=order_hot\n",
      "  /projects/?order=order_latest\n",
      "  /projects/?order=order_popularity\n",
      "  /projects/?order=order_views\n",
      "  /projects/?platform=1\n",
      "  /projects/?platform=2\n",
      "  /projects/?platform=any\n",
      "  /projects/?share=any\n",
      "  /projects/?share=schematic\n",
      "  /projects/?share=seed\n",
      "\n",
      "======================================================================\n",
      "Checking if category pages exist...\n",
      "======================================================================\n",
      "\n",
      "Trying: https://www.planetminecraft.com/projects/?category=land-structure\n",
      "  Final URL: https://www.planetminecraft.com/projects/?category=land-structure\n",
      "  Found 33 projects\n"
     ]
    }
   ],
   "source": [
    "# Check if there's a page count indicator or total projects count\n",
    "# Also check different sorting/filtering options\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Checking for total project count on the page...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "driver.get(\"https://www.planetminecraft.com/projects/\")\n",
    "time.sleep(2)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Look for text containing \"projects\" or \"results\"\n",
    "text_with_numbers = soup.find_all(string=lambda s: s and any(word in s.lower() for word in ['projects', 'results', 'total', 'showing']))\n",
    "print(\"\\nText containing count keywords:\")\n",
    "for text in text_with_numbers[:10]:\n",
    "    clean_text = ' '.join(text.strip().split())\n",
    "    if clean_text and any(char.isdigit() for char in clean_text):\n",
    "        print(f\"  {clean_text}\")\n",
    "\n",
    "# Check if there are filter options that might tell us about categories\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Checking available filters/categories...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Look for filter links\n",
    "filter_links = soup.find_all('a', href=lambda h: h and '/projects/' in h and '?' in h)\n",
    "print(f\"\\nFound {len(filter_links)} filter/category links\")\n",
    "unique_filters = set()\n",
    "for link in filter_links:\n",
    "    href = link.get('href')\n",
    "    if href.startswith('/projects/'):\n",
    "        unique_filters.add(href)\n",
    "\n",
    "print(f\"Unique filter URLs: {len(unique_filters)}\")\n",
    "for filt in sorted(unique_filters)[:15]:\n",
    "    print(f\"  {filt}\")\n",
    "\n",
    "# Also check for category-specific pages\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Checking if category pages exist...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_category_urls = [\n",
    "    \"https://www.planetminecraft.com/projects/?category=land-structure\",\n",
    "    \"https://www.planetminecraft.com/projects/land-structure/\",\n",
    "    \"https://www.planetminecraft.com/resources/land-structure/\",\n",
    "]\n",
    "\n",
    "for cat_url in test_category_urls[:1]:\n",
    "    print(f\"\\nTrying: {cat_url}\")\n",
    "    driver.get(cat_url)\n",
    "    time.sleep(2)\n",
    "    print(f\"  Final URL: {driver.current_url}\")\n",
    "    \n",
    "    cat_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    cat_links = cat_soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "    unique_cat = set([link.get('href').split('?')[0] for link in cat_links])\n",
    "    print(f\"  Found {len(unique_cat)} projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "751a1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Finding the approximate number of pages using binary search...\n",
      "======================================================================\n",
      "\n",
      "Searching for the last page...\n",
      "\n",
      "Iteration 1: Testing page 25000 (range: 1-50000)\n",
      "  ‚úì Page 25000 has projects\n",
      "\n",
      "Iteration 2: Testing page 37500 (range: 25001-50000)\n",
      "  ‚úì Page 37500 has projects\n",
      "\n",
      "Iteration 3: Testing page 43750 (range: 37501-50000)\n",
      "  ‚úì Page 43750 has projects\n",
      "\n",
      "Iteration 4: Testing page 46875 (range: 43751-50000)\n",
      "  ‚úì Page 46875 has projects\n",
      "\n",
      "Iteration 5: Testing page 48438 (range: 46876-50000)\n",
      "  ‚úì Page 48438 has projects\n",
      "\n",
      "Iteration 6: Testing page 49219 (range: 48439-50000)\n",
      "  ‚úì Page 49219 has projects\n",
      "\n",
      "Iteration 7: Testing page 49610 (range: 49220-50000)\n",
      "  ‚úì Page 49610 has projects\n",
      "\n",
      "Iteration 8: Testing page 49805 (range: 49611-50000)\n",
      "  ‚úì Page 49805 has projects\n",
      "\n",
      "Iteration 9: Testing page 49903 (range: 49806-50000)\n",
      "  ‚úì Page 49903 has projects\n",
      "\n",
      "Iteration 10: Testing page 49952 (range: 49904-50000)\n",
      "  ‚úì Page 49952 has projects\n",
      "\n",
      "Iteration 11: Testing page 49976 (range: 49953-50000)\n",
      "  ‚úì Page 49976 has projects\n",
      "\n",
      "Iteration 12: Testing page 49988 (range: 49977-50000)\n",
      "  ‚úì Page 49988 has projects\n",
      "\n",
      "Iteration 13: Testing page 49994 (range: 49989-50000)\n",
      "  ‚úì Page 49994 has projects\n",
      "\n",
      "Iteration 14: Testing page 49997 (range: 49995-50000)\n",
      "  ‚úì Page 49997 has projects\n",
      "\n",
      "Iteration 15: Testing page 49999 (range: 49998-50000)\n",
      "  ‚úì Page 49999 has projects\n",
      "\n",
      "Iteration 16: Testing page 50000 (range: 50000-50000)\n",
      "  ‚úì Page 50000 has projects\n",
      "\n",
      "======================================================================\n",
      "Approximate last page: 50000\n",
      "======================================================================\n",
      "\n",
      "Verifying pages around 50000...\n",
      "  Page 49999: ‚úì HAS PROJECTS\n",
      "  Page 50000: ‚úì HAS PROJECTS\n",
      "  Page 50001: ‚úì HAS PROJECTS\n",
      "  Page 50002: ‚úì HAS PROJECTS\n"
     ]
    }
   ],
   "source": [
    "# Let's do a binary search to find approximately how many pages exist\n",
    "# Start with a reasonable range\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Finding the approximate number of pages using binary search...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def has_projects(page_num):\n",
    "    \"\"\"Check if a page number has any projects\"\"\"\n",
    "    url = f\"https://www.planetminecraft.com/projects/?p={page_num}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1.5)  # Shorter delay for searching\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    project_links = soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "    \n",
    "    # Filter to actual project pages (not just any /project/ link)\n",
    "    actual_projects = [link for link in project_links \n",
    "                      if link.get('href', '').startswith('/project/') \n",
    "                      and len(link.get('href', '').split('/')) >= 3]\n",
    "    \n",
    "    return len(actual_projects) > 0\n",
    "\n",
    "# Binary search\n",
    "low, high = 1, 50000  # Assuming max 50k pages (wild guess)\n",
    "last_valid = 1\n",
    "\n",
    "print(\"\\nSearching for the last page...\")\n",
    "iteration = 0\n",
    "max_iterations = 20  # Prevent infinite loop\n",
    "\n",
    "while low <= high and iteration < max_iterations:\n",
    "    mid = (low + high) // 2\n",
    "    iteration += 1\n",
    "    \n",
    "    print(f\"\\nIteration {iteration}: Testing page {mid} (range: {low}-{high})\")\n",
    "    \n",
    "    has_proj = has_projects(mid)\n",
    "    \n",
    "    if has_proj:\n",
    "        print(f\"  ‚úì Page {mid} has projects\")\n",
    "        last_valid = mid\n",
    "        low = mid + 1  # Try higher\n",
    "    else:\n",
    "        print(f\"  ‚úó Page {mid} is empty\")\n",
    "        high = mid - 1  # Try lower\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Approximate last page: {last_valid}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify by testing a few pages around the found number\n",
    "print(f\"\\nVerifying pages around {last_valid}...\")\n",
    "for test_page in [last_valid - 1, last_valid, last_valid + 1, last_valid + 2]:\n",
    "    if test_page > 0:\n",
    "        has_proj = has_projects(test_page)\n",
    "        status = \"‚úì HAS PROJECTS\" if has_proj else \"‚úó EMPTY\"\n",
    "        print(f\"  Page {test_page}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2388166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Extending search to find the actual limit...\n",
      "======================================================================\n",
      "\n",
      "Quick spot checks on high page numbers:\n",
      "  Page 100,000: ‚úì HAS PROJECTS (75 project links)\n",
      "  Page 200,000: ‚úì HAS PROJECTS (75 project links)\n",
      "  Page 500,000: ‚úì HAS PROJECTS (75 project links)\n",
      "  Page 1,000,000: ‚úì HAS PROJECTS (75 project links)\n",
      "\n",
      "======================================================================\n",
      "Analysis:\n",
      "======================================================================\n",
      "It appears Planet Minecraft might not have a hard page limit,\n",
      "or the limit is very high (>1M pages).\n",
      "\n",
      "For practical scraping, we have a few options:\n",
      "1. Scrape pages sequentially until we hit empty pages\n",
      "2. Use filters (?share=schematic, ?category=X) to narrow scope\n",
      "3. Use ?order=order_latest to get newest first, then stop when we\n",
      "   reach projects we've already seen (for incremental scraping)\n",
      "4. Estimate based on projects per page: if ~25-30 projects/page\n",
      "   and they show 50k+ pages, that's 1.25M+ projects total\n"
     ]
    }
   ],
   "source": [
    "# Let's try with a much higher upper bound\n",
    "print(\"=\" * 70)\n",
    "print(\"Extending search to find the actual limit...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test some very high page numbers directly\n",
    "test_pages = [100000, 200000, 500000, 1000000]\n",
    "\n",
    "print(\"\\nQuick spot checks on high page numbers:\")\n",
    "for page in test_pages:\n",
    "    url = f\"https://www.planetminecraft.com/projects/?p={page}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1.5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    project_links = soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "    actual_projects = [link for link in project_links \n",
    "                      if link.get('href', '').startswith('/project/') \n",
    "                      and len(link.get('href', '').split('/')) >= 3]\n",
    "    \n",
    "    status = \"‚úì HAS PROJECTS\" if len(actual_projects) > 0 else \"‚úó EMPTY\"\n",
    "    print(f\"  Page {page:,}: {status} ({len(actual_projects)} project links)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"It appears Planet Minecraft might not have a hard page limit,\")\n",
    "print(\"or the limit is very high (>1M pages).\")\n",
    "print(\"\\nFor practical scraping, we have a few options:\")\n",
    "print(\"1. Scrape pages sequentially until we hit empty pages\")\n",
    "print(\"2. Use filters (?share=schematic, ?category=X) to narrow scope\")\n",
    "print(\"3. Use ?order=order_latest to get newest first, then stop when we\")\n",
    "print(\"   reach projects we've already seen (for incremental scraping)\")\n",
    "print(\"4. Estimate based on projects per page: if ~25-30 projects/page\")\n",
    "print(\"   and they show 50k+ pages, that's 1.25M+ projects total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f3fd7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Checking if high page numbers are cycling content...\n",
      "======================================================================\n",
      "\n",
      "Sampling pages:\n",
      "  Page       1: 33 unique projects\n",
      "    First: /project/fanmade-erebor-the-lonely-mountain\n",
      "  Page     100: 25 unique projects\n",
      "    First: /project/fnaf-movie-2-withered-bonnie-the-bunny\n",
      "  Page   1,000: 25 unique projects\n",
      "    First: /project/dream-world-6729795\n",
      "  Page  10,000: 25 unique projects\n",
      "    First: /project/hamburg-building-6-by-tim0fei\n",
      "  Page 100,000: 25 unique projects\n",
      "    First: /project/brandi-modern-mansion-full-interior\n",
      "  Page 500,000: 25 unique projects\n",
      "    First: /project/messi-6689544\n",
      "\n",
      "======================================================================\n",
      "Checking for duplicates across sampled pages...\n",
      "======================================================================\n",
      "Page 1 & 1,000: 1 shared projects\n",
      "Page 1 & 10,000: 2 shared projects\n",
      "Page 1 & 100,000: 2 shared projects\n",
      "Page 1 & 500,000: 2 shared projects\n",
      "\n",
      "Total unique projects across all sampled pages: 151\n",
      "\n",
      "======================================================================\n",
      "Conclusion:\n",
      "======================================================================\n",
      "Based on the overlap analysis, we can determine if:\n",
      "- High page numbers show unique content ‚Üí need to scrape many pages\n",
      "- High page numbers repeat content ‚Üí pagination wraps around\n",
      "- Few/no overlaps ‚Üí sequential scraping will work well\n"
     ]
    }
   ],
   "source": [
    "# Check if high page numbers are just showing duplicates/cycling\n",
    "print(\"=\" * 70)\n",
    "print(\"Checking if high page numbers are cycling content...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def get_projects_from_page(page_num):\n",
    "    \"\"\"Get list of unique project URLs from a page\"\"\"\n",
    "    url = f\"https://www.planetminecraft.com/projects/?p={page_num}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1.5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    project_links = soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "    \n",
    "    # Get unique project URLs\n",
    "    projects = set()\n",
    "    for link in project_links:\n",
    "        href = link.get('href', '')\n",
    "        if href.startswith('/project/') and len(href.split('/')) >= 3:\n",
    "            # Clean URL (remove query params and trailing slash)\n",
    "            clean_href = href.split('?')[0].rstrip('/')\n",
    "            projects.add(clean_href)\n",
    "    \n",
    "    return projects\n",
    "\n",
    "# Compare different page ranges\n",
    "test_pages = [1, 100, 1000, 10000, 100000, 500000]\n",
    "all_projects = {}\n",
    "\n",
    "print(\"\\nSampling pages:\")\n",
    "for page in test_pages:\n",
    "    projects = get_projects_from_page(page)\n",
    "    all_projects[page] = projects\n",
    "    print(f\"  Page {page:>7,}: {len(projects):>2} unique projects\")\n",
    "    if len(projects) > 0:\n",
    "        print(f\"    First: {list(projects)[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Checking for duplicates across sampled pages...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check overlaps\n",
    "pages_list = list(all_projects.keys())\n",
    "for i in range(len(pages_list)):\n",
    "    for j in range(i+1, len(pages_list)):\n",
    "        page1, page2 = pages_list[i], pages_list[j]\n",
    "        overlap = all_projects[page1].intersection(all_projects[page2])\n",
    "        if overlap:\n",
    "            print(f\"Page {page1:,} & {page2:,}: {len(overlap)} shared projects\")\n",
    "\n",
    "# Count total unique\n",
    "all_unique = set()\n",
    "for projects in all_projects.values():\n",
    "    all_unique.update(projects)\n",
    "\n",
    "print(f\"\\nTotal unique projects across all sampled pages: {len(all_unique)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Conclusion:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Based on the overlap analysis, we can determine if:\")\n",
    "print(\"- High page numbers show unique content ‚Üí need to scrape many pages\")\n",
    "print(\"- High page numbers repeat content ‚Üí pagination wraps around\")\n",
    "print(\"- Few/no overlaps ‚Üí sequential scraping will work well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0645ca",
   "metadata": {},
   "source": [
    "## Summary: Planet Minecraft Discovery Strategy\n",
    "\n",
    "### What We Found:\n",
    "1. **Pagination works with `?p=N` parameter**\n",
    "   - Example: `https://www.planetminecraft.com/projects/?p=2`\n",
    "   - Each page has ~25-33 unique projects\n",
    "   - Pages show mostly unique content (minimal overlap)\n",
    "\n",
    "2. **Very high page limit**\n",
    "   - Page 1,000,000 still returns projects (though might be cycling at that point)\n",
    "   - Practical range is probably 1-100,000 pages\n",
    "   - Estimated 1-3 million total projects\n",
    "\n",
    "3. **Available filters:**\n",
    "   - `?share=schematic` - schematic files only\n",
    "   - `?share=seed` - seed only\n",
    "   - `?category=land-structure` - filter by subcategory\n",
    "   - `?order=order_latest` - sort by newest\n",
    "   - `?order=order_popularity` - sort by popular\n",
    "   - `?order=order_downloads` - sort by downloads\n",
    "   - `?order=order_views` - sort by views\n",
    "   - `?platform=1` - Java Edition\n",
    "   - `?platform=2` - Bedrock Edition\n",
    "\n",
    "### Recommended Scraping Strategy:\n",
    "\n",
    "**Option 1: Full Sequential Scraping**\n",
    "- Start at page 1, scrape sequentially\n",
    "- Continue until hitting empty pages (or X consecutive empty pages)\n",
    "- Store URLs in a set to avoid duplicates\n",
    "- Estimated time: Days to weeks (depending on rate limiting)\n",
    "\n",
    "**Option 2: Filtered Scraping**\n",
    "- Use `?share=schematic` to get only schematic files\n",
    "- Reduces total pages significantly\n",
    "- Better for our use case (voxel models)\n",
    "\n",
    "**Option 3: Incremental/Recent Scraping**\n",
    "- Use `?order=order_latest` to get newest first\n",
    "- Scrape until reaching projects already in database\n",
    "- Good for keeping dataset updated\n",
    "\n",
    "**Option 4: Category-Based Scraping**\n",
    "- Iterate through each subcategory\n",
    "- Scrape each category separately\n",
    "- Easier to resume if interrupted\n",
    "\n",
    "### Next Steps:\n",
    "1. Build URL collection function that iterates through pages\n",
    "2. Add rate limiting (delays between requests)\n",
    "3. Implement duplicate detection\n",
    "4. Add progress saving/resuming capability\n",
    "5. Combine with our metadata extraction function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf36cd",
   "metadata": {},
   "source": [
    "## Step 23: Build the Complete Scraper\n",
    "\n",
    "Now we'll create a full scraper that:\n",
    "1. Collects project URLs from paginated browse pages\n",
    "2. Extracts metadata from each project\n",
    "3. Saves results to CSV\n",
    "4. Supports optional search parameters (category, share type, order, etc.)\n",
    "5. Implements progress saving and resuming\n",
    "6. Adds rate limiting to be polite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bb489c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Collection Function - Scrape project URLs from browse pages\n",
    "def collect_project_urls(driver, start_page=1, max_pages=None, \n",
    "                        category=None, share=None, order=None, \n",
    "                        platform=None, delay=2.0, max_empty_pages=5):\n",
    "    \"\"\"\n",
    "    Collect project URLs from Planet Minecraft browse pages.\n",
    "    \n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance\n",
    "    - start_page: Page number to start from (default: 1)\n",
    "    - max_pages: Maximum number of pages to scrape (None = unlimited)\n",
    "    - category: Filter by category (e.g., 'land-structure', 'challenge-adventure')\n",
    "    - share: Filter by share type ('schematic', 'seed', or None for all)\n",
    "    - order: Sort order ('order_latest', 'order_popularity', 'order_downloads', 'order_views', 'order_hot')\n",
    "    - platform: Filter by platform (1=Java, 2=Bedrock, None=any)\n",
    "    - delay: Delay between page requests in seconds\n",
    "    - max_empty_pages: Stop after this many consecutive empty pages\n",
    "    \n",
    "    Returns:\n",
    "    - set: Unique project URLs\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = \"https://www.planetminecraft.com/projects/\"\n",
    "    all_project_urls = set()\n",
    "    empty_page_count = 0\n",
    "    current_page = start_page\n",
    "    \n",
    "    # Build query parameters\n",
    "    params = []\n",
    "    if category:\n",
    "        params.append(f\"category={category}\")\n",
    "    if share:\n",
    "        params.append(f\"share={share}\")\n",
    "    if order:\n",
    "        params.append(f\"order={order}\")\n",
    "    if platform:\n",
    "        params.append(f\"platform={platform}\")\n",
    "    \n",
    "    param_string = \"&\".join(params) if params else \"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Starting URL collection from page {start_page}\")\n",
    "    if param_string:\n",
    "        print(f\"Filters: {param_string}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    while True:\n",
    "        # Check if we've reached max_pages\n",
    "        if max_pages and (current_page - start_page + 1) > max_pages:\n",
    "            print(f\"\\nReached max_pages limit ({max_pages})\")\n",
    "            break\n",
    "        \n",
    "        # Build URL with page number\n",
    "        if param_string:\n",
    "            url = f\"{base_url}?{param_string}&p={current_page}\"\n",
    "        else:\n",
    "            url = f\"{base_url}?p={current_page}\"\n",
    "        \n",
    "        print(f\"\\nPage {current_page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            project_links = soup.find_all('a', href=lambda h: h and '/project/' in h)\n",
    "            \n",
    "            # Extract unique project URLs\n",
    "            page_projects = set()\n",
    "            for link in project_links:\n",
    "                href = link.get('href', '')\n",
    "                if href.startswith('/project/') and len(href.split('/')) >= 3:\n",
    "                    clean_href = href.split('?')[0].rstrip('/')\n",
    "                    page_projects.add(clean_href)\n",
    "            \n",
    "            # Check if page has projects\n",
    "            if len(page_projects) == 0:\n",
    "                empty_page_count += 1\n",
    "                print(f\"  ‚ö† Empty page (count: {empty_page_count}/{max_empty_pages})\")\n",
    "                \n",
    "                if empty_page_count >= max_empty_pages:\n",
    "                    print(f\"\\n  Stopping: {max_empty_pages} consecutive empty pages\")\n",
    "                    break\n",
    "            else:\n",
    "                empty_page_count = 0  # Reset counter\n",
    "                new_projects = page_projects - all_project_urls\n",
    "                all_project_urls.update(page_projects)\n",
    "                print(f\"  ‚úì Found {len(page_projects)} projects ({len(new_projects)} new)\")\n",
    "                print(f\"  Total unique: {len(all_project_urls)}\")\n",
    "            \n",
    "            current_page += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error on page {current_page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Collection complete: {len(all_project_urls)} unique project URLs\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return all_project_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7732956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full scraper with progress saving\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_planet_minecraft(driver, \n",
    "                            output_csv='planet_minecraft_projects.csv',\n",
    "                            progress_file='scraping_progress.json',\n",
    "                            start_page=1, \n",
    "                            max_pages=None,\n",
    "                            max_projects=None,\n",
    "                            category=None, \n",
    "                            share=None, \n",
    "                            order=None,\n",
    "                            platform=None,\n",
    "                            page_delay=2.0,\n",
    "                            project_delay=1.0,\n",
    "                            resume=True):\n",
    "    \"\"\"\n",
    "    Complete scraper for Planet Minecraft projects.\n",
    "    \n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance\n",
    "    - output_csv: Path to output CSV file\n",
    "    - progress_file: Path to progress tracking JSON file\n",
    "    - start_page: Page number to start URL collection\n",
    "    - max_pages: Maximum pages to scrape for URLs (None = unlimited)\n",
    "    - max_projects: Maximum projects to scrape metadata for (None = unlimited)\n",
    "    - category: Filter by category (e.g., 'land-structure')\n",
    "    - share: Filter by share type ('schematic', 'seed', None)\n",
    "    - order: Sort order ('order_latest', 'order_popularity', etc.)\n",
    "    - platform: Platform filter (1=Java, 2=Bedrock, None=any)\n",
    "    - page_delay: Delay between page requests (seconds)\n",
    "    - project_delay: Delay between project metadata requests (seconds)\n",
    "    - resume: Whether to resume from progress file if it exists\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Scraping statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize progress tracking\n",
    "    progress = {\n",
    "        'scraped_urls': set(),\n",
    "        'failed_urls': {},\n",
    "        'total_collected': 0,\n",
    "        'total_scraped': 0,\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'filters': {\n",
    "            'category': category,\n",
    "            'share': share,\n",
    "            'order': order,\n",
    "            'platform': platform\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load existing progress if resuming\n",
    "    if resume and Path(progress_file).exists():\n",
    "        print(f\"Loading progress from {progress_file}...\")\n",
    "        with open(progress_file, 'r') as f:\n",
    "            saved_progress = json.load(f)\n",
    "            progress['scraped_urls'] = set(saved_progress.get('scraped_urls', []))\n",
    "            progress['failed_urls'] = saved_progress.get('failed_urls', {})\n",
    "            print(f\"  Resuming: {len(progress['scraped_urls'])} already scraped\")\n",
    "    \n",
    "    # Check if output CSV exists and load existing URLs to avoid duplicates\n",
    "    csv_exists = Path(output_csv).exists()\n",
    "    if csv_exists and resume:\n",
    "        print(f\"Loading existing data from {output_csv}...\")\n",
    "        with open(output_csv, 'r', newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                if 'url' in row:\n",
    "                    progress['scraped_urls'].add(row['url'])\n",
    "        print(f\"  Found {len(progress['scraped_urls'])} existing projects\")\n",
    "    \n",
    "    # Step 1: Collect project URLs\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 1: Collecting project URLs\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    project_urls = collect_project_urls(\n",
    "        driver=driver,\n",
    "        start_page=start_page,\n",
    "        max_pages=max_pages,\n",
    "        category=category,\n",
    "        share=share,\n",
    "        order=order,\n",
    "        platform=platform,\n",
    "        delay=page_delay\n",
    "    )\n",
    "    \n",
    "    progress['total_collected'] = len(project_urls)\n",
    "    \n",
    "    # Filter out already scraped URLs\n",
    "    urls_to_scrape = [url for url in project_urls if url not in progress['scraped_urls']]\n",
    "    \n",
    "    if max_projects:\n",
    "        urls_to_scrape = urls_to_scrape[:max_projects]\n",
    "    \n",
    "    print(f\"\\nURLs to scrape: {len(urls_to_scrape)} (out of {len(project_urls)} collected)\")\n",
    "    \n",
    "    if len(urls_to_scrape) == 0:\n",
    "        print(\"No new URLs to scrape!\")\n",
    "        return progress\n",
    "    \n",
    "    # Step 2: Extract metadata from each project\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 2: Extracting project metadata\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Prepare CSV file\n",
    "    csv_headers = [\n",
    "        'title', 'url', 'author', 'category', 'subcategory', 'description',\n",
    "        'tags', 'posted_date', 'updated_date', 'views', 'views_today',\n",
    "        'downloads', 'downloads_today', 'diamonds', 'hearts',\n",
    "        'download_links', 'download_types', 'file_types'\n",
    "    ]\n",
    "    \n",
    "    # Open CSV file in append mode if resuming, write mode if new\n",
    "    file_mode = 'a' if (csv_exists and resume) else 'w'\n",
    "    with open(output_csv, file_mode, newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_headers)\n",
    "        \n",
    "        # Write header only if new file\n",
    "        if file_mode == 'w':\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Process each project\n",
    "        for idx, project_url in enumerate(urls_to_scrape, 1):\n",
    "            full_url = f\"https://www.planetminecraft.com{project_url}\"\n",
    "            \n",
    "            print(f\"\\n[{idx}/{len(urls_to_scrape)}] {project_url}\")\n",
    "            \n",
    "            try:\n",
    "                # Load project page\n",
    "                driver.get(full_url)\n",
    "                time.sleep(project_delay)\n",
    "                \n",
    "                # Extract metadata\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                metadata = extract_project_metadata_complete(soup, full_url)\n",
    "                \n",
    "                if metadata and metadata.get('title'):\n",
    "                    # Prepare CSV row\n",
    "                    csv_row = {\n",
    "                        'title': metadata.get('title', ''),\n",
    "                        'url': full_url,\n",
    "                        'author': metadata.get('author', ''),\n",
    "                        'category': metadata.get('category', ''),\n",
    "                        'subcategory': metadata.get('subcategory', ''),\n",
    "                        'description': metadata.get('description', '')[:500],  # Truncate\n",
    "                        'tags': ', '.join(metadata.get('tags', [])),\n",
    "                        'posted_date': metadata.get('posted_date', ''),\n",
    "                        'updated_date': metadata.get('updated_date', ''),\n",
    "                        'views': metadata.get('views', ''),\n",
    "                        'views_today': metadata.get('views_today', ''),\n",
    "                        'downloads': metadata.get('downloads', ''),\n",
    "                        'downloads_today': metadata.get('downloads_today', ''),\n",
    "                        'diamonds': metadata.get('diamonds', ''),\n",
    "                        'hearts': metadata.get('hearts', ''),\n",
    "                        'download_links': ' | '.join([d['url'] for d in metadata.get('download_links', [])]),\n",
    "                        'download_types': ' | '.join([d['type'] for d in metadata.get('download_links', [])]),\n",
    "                        'file_types': ' | '.join([d.get('file_type', 'unknown') for d in metadata.get('download_links', [])])\n",
    "                    }\n",
    "                    \n",
    "                    writer.writerow(csv_row)\n",
    "                    csvfile.flush()  # Ensure data is written immediately\n",
    "                    \n",
    "                    progress['scraped_urls'].add(project_url)\n",
    "                    progress['total_scraped'] += 1\n",
    "                    \n",
    "                    print(f\"  ‚úì {metadata.get('title')} - {metadata.get('subcategory', 'N/A')}\")\n",
    "                    print(f\"    Stats: {metadata.get('views', 0)} views, {metadata.get('downloads', 0)} downloads\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö† No metadata extracted\")\n",
    "                    progress['failed_urls'][project_url] = \"No metadata\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error: {e}\")\n",
    "                progress['failed_urls'][project_url] = str(e)\n",
    "            \n",
    "            # Save progress every 10 projects\n",
    "            if idx % 10 == 0:\n",
    "                with open(progress_file, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'scraped_urls': list(progress['scraped_urls']),\n",
    "                        'failed_urls': progress['failed_urls'],\n",
    "                        'total_collected': progress['total_collected'],\n",
    "                        'total_scraped': progress['total_scraped'],\n",
    "                        'start_time': progress['start_time'],\n",
    "                        'last_update': datetime.now().isoformat(),\n",
    "                        'filters': progress['filters']\n",
    "                    }, f, indent=2)\n",
    "                print(f\"  üìù Progress saved ({progress['total_scraped']} scraped)\")\n",
    "    \n",
    "    # Final progress save\n",
    "    progress['end_time'] = datetime.now().isoformat()\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'scraped_urls': list(progress['scraped_urls']),\n",
    "            'failed_urls': progress['failed_urls'],\n",
    "            'total_collected': progress['total_collected'],\n",
    "            'total_scraped': progress['total_scraped'],\n",
    "            'start_time': progress['start_time'],\n",
    "            'end_time': progress['end_time'],\n",
    "            'filters': progress['filters']\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SCRAPING COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total URLs collected: {progress['total_collected']}\")\n",
    "    print(f\"Successfully scraped: {progress['total_scraped']}\")\n",
    "    print(f\"Failed: {len(progress['failed_urls'])}\")\n",
    "    print(f\"Output saved to: {output_csv}\")\n",
    "    print(f\"Progress saved to: {progress_file}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f2569",
   "metadata": {},
   "source": [
    "### Example Usage - Test with Small Sample\n",
    "\n",
    "Let's test the scraper with a small sample first (5 pages, schematics only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ade72a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading progress from ../data/planet_minecraft/test_progress.json...\n",
      "  Resuming: 0 already scraped\n",
      "Loading existing data from ../data/planet_minecraft/test_schematics.csv...\n",
      "  Found 0 existing projects\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Collecting project URLs\n",
      "======================================================================\n",
      "======================================================================\n",
      "Starting URL collection from page 1\n",
      "Filters: share=schematic&order=order_latest\n",
      "======================================================================\n",
      "\n",
      "Page 1: https://www.planetminecraft.com/projects/?share=schematic&order=order_latest&p=1\n",
      "  ‚úì Found 25 projects (25 new)\n",
      "  Total unique: 25\n",
      "\n",
      "Page 2: https://www.planetminecraft.com/projects/?share=schematic&order=order_latest&p=2\n",
      "  ‚úì Found 25 projects (25 new)\n",
      "  Total unique: 50\n",
      "\n",
      "Page 3: https://www.planetminecraft.com/projects/?share=schematic&order=order_latest&p=3\n",
      "  ‚úì Found 25 projects (25 new)\n",
      "  Total unique: 75\n",
      "\n",
      "Reached max_pages limit (3)\n",
      "\n",
      "======================================================================\n",
      "Collection complete: 75 unique project URLs\n",
      "======================================================================\n",
      "\n",
      "URLs to scrape: 10 (out of 75 collected)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Extracting project metadata\n",
      "======================================================================\n",
      "\n",
      "[1/10] /project/piraeus-tower-heliopolis\n",
      "  ‚úì Piraeus tower Heliopolis Minecraft Map - land-structure\n",
      "    Stats: 237 views, 75 downloads\n",
      "\n",
      "[2/10] /project/water-tower-1-21-1-litematica-grotiva\n",
      "  ‚úì Water tower | 1.21.1 | litematica | Grotiva | Minecraft Map - other\n",
      "    Stats: 133 views, 18 downloads\n",
      "\n",
      "[3/10] /project/hamburg-building-6-by-tim0fei\n",
      "  ‚úì Hamburg Building 6 by tim0fei Minecraft Map - other\n",
      "    Stats: 38 views, 7 downloads\n",
      "\n",
      "[4/10] /project/fractured-femur-mining-defence-training-world-v2\n",
      "  ‚úì Fractured Femur- Mining defence training world v2 Minecraft Map - underground-structure\n",
      "    Stats: 45 views, 2 downloads\n",
      "\n",
      "[5/10] /project/halloween-dropper\n",
      "  ‚úì Halloween Dropper Minecraft Map - other\n",
      "    Stats: 99 views, 25 downloads\n",
      "\n",
      "[6/10] /project/redstonefy\n",
      "  ‚úì RedstoneFy Minecraft Map - redstone-device\n",
      "    Stats: 26 views, 1 downloads\n",
      "\n",
      "[7/10] /project/lighting-tower\n",
      "  ‚úì Lighting tower | Download Minecraft Map - other\n",
      "    Stats: 59 views, 9 downloads\n",
      "\n",
      "[8/10] /project/north-america-survival-portuguese\n",
      "  ‚úì North America Survival (Brazilian Portuguese) Minecraft Map - other\n",
      "    Stats: 67 views, 8 downloads\n",
      "\n",
      "[9/10] /project/fnaf-universe-bedrock\n",
      "  ‚úì FNAF: UNIVERSE - BEDROCK Minecraft Map - other\n",
      "    Stats: 51 views, 6 downloads\n",
      "\n",
      "[10/10] /project/the-emperor-s-castle-6744437\n",
      "  ‚úì The Emperor&#039;s castle Minecraft Map - other\n",
      "    Stats: 135 views, 36 downloads\n",
      "  üìù Progress saved (10 scraped)\n",
      "\n",
      "======================================================================\n",
      "SCRAPING COMPLETE\n",
      "======================================================================\n",
      "Total URLs collected: 75\n",
      "Successfully scraped: 10\n",
      "Failed: 10\n",
      "Output saved to: ../data/planet_minecraft/test_schematics.csv\n",
      "Progress saved to: ../data/planet_minecraft/test_progress.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test scraper with small sample\n",
    "# Scrape only 3 pages of schematic files\n",
    "\n",
    "test_output_dir = Path('../data/planet_minecraft/')\n",
    "test_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "test_stats = scrape_planet_minecraft(\n",
    "    driver=driver,\n",
    "    output_csv=str(test_output_dir / 'test_schematics.csv'),\n",
    "    progress_file=str(test_output_dir / 'test_progress.json'),\n",
    "    start_page=1,\n",
    "    max_pages=3,  # Only 3 pages for testing\n",
    "    max_projects=10,  # Max 10 projects for quick test\n",
    "    share='schematic',  # Only schematic files\n",
    "    order='order_latest',  # Get newest first\n",
    "    page_delay=2.0,\n",
    "    project_delay=1.5,\n",
    "    resume=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3d8e9",
   "metadata": {},
   "source": [
    "### Other Usage Examples\n",
    "\n",
    "Here are examples for different scraping scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Scrape all land-structure category projects\n",
    "# Uncomment to run:\n",
    "\"\"\"\n",
    "scrape_planet_minecraft(\n",
    "    driver=driver,\n",
    "    output_csv='../data/planet_minecraft/land_structures.csv',\n",
    "    progress_file='../data/planet_minecraft/land_structures_progress.json',\n",
    "    category='land-structure',\n",
    "    order='order_popularity',  # Get most popular first\n",
    "    page_delay=2.0,\n",
    "    project_delay=1.5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Scrape all schematic files (no category filter)\n",
    "# Uncomment to run:\n",
    "\"\"\"\n",
    "scrape_planet_minecraft(\n",
    "    driver=driver,\n",
    "    output_csv='../data/planet_minecraft/all_schematics.csv',\n",
    "    progress_file='../data/planet_minecraft/all_schematics_progress.json',\n",
    "    share='schematic',\n",
    "    order='order_latest',  # Get newest first\n",
    "    max_pages=1000,  # Limit to 1000 pages\n",
    "    page_delay=2.0,\n",
    "    project_delay=1.5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Scrape Java Edition projects only\n",
    "# Uncomment to run:\n",
    "\"\"\"\n",
    "scrape_planet_minecraft(\n",
    "    driver=driver,\n",
    "    output_csv='../data/planet_minecraft/java_projects.csv',\n",
    "    progress_file='../data/planet_minecraft/java_progress.json',\n",
    "    platform=1,  # 1 = Java Edition\n",
    "    max_pages=500,\n",
    "    page_delay=2.0,\n",
    "    project_delay=1.5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Example 4: Scrape challenge/adventure maps with schematics\n",
    "# Uncomment to run:\n",
    "\"\"\"\n",
    "scrape_planet_minecraft(\n",
    "    driver=driver,\n",
    "    output_csv='../data/planet_minecraft/challenge_schematics.csv',\n",
    "    progress_file='../data/planet_minecraft/challenge_progress.json',\n",
    "    category='challenge-adventure',\n",
    "    share='schematic',\n",
    "    order='order_downloads',  # Get most downloaded first\n",
    "    page_delay=2.0,\n",
    "    project_delay=1.5\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Example 5: Full scrape (no filters) - WARNING: This will take a VERY long time!\n",
    "# Uncomment to run:\n",
    "\"\"\"\n",
    "scrape_planet_minecraft(\n",
    "    driver=driver,\n",
    "    output_csv='../data/planet_minecraft/all_projects.csv',\n",
    "    progress_file='../data/planet_minecraft/all_progress.json',\n",
    "    order='order_latest',\n",
    "    max_pages=10000,  # Limit to prevent extremely long scraping\n",
    "    page_delay=2.5,  # Slower to be more polite\n",
    "    project_delay=2.0,\n",
    "    resume=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Examples ready to use!\")\n",
    "print(\"\\nAvailable filter options:\")\n",
    "print(\"  category: land-structure, challenge-adventure, 3d-art, etc.\")\n",
    "print(\"  share: 'schematic', 'seed', or None (all)\")\n",
    "print(\"  order: 'order_latest', 'order_popularity', 'order_downloads', 'order_views', 'order_hot'\")\n",
    "print(\"  platform: 1 (Java), 2 (Bedrock), None (any)\")\n",
    "print(\"\\nTip: Start with small tests (max_pages=3-5) to verify everything works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4d5d5",
   "metadata": {},
   "source": [
    "## Step 24: Fix Download Detection and Switch to JSON\n",
    "\n",
    "Let's investigate the missing downloads and improve the scraper to use JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7973bd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Investigating: https://www.planetminecraft.com/project/halloween-dropper\n",
      "======================================================================\n",
      "\n",
      "Found 2 download candidates:\n",
      "\n",
      "  1. Text: Download Bedrock mcworld\n",
      "     Href: /project/halloween-dropper/download/mcworld/\n",
      "     Class: ['branded-download', 'tooltip', 'tipso_style']\n",
      "\n",
      "  2. Text: Nuestra Se√±ora de la Sant√≠sima Trinidad, ship of the line (Free Download)\n",
      "     Href: /project/nuestra-se-ora-de-la-sant-sima-trinidad-ship-of-the-line-free-download/\n",
      "     Class: ['r-title']\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Investigating: https://www.planetminecraft.com/project/fnaf-universe-bedrock\n",
      "======================================================================\n",
      "\n",
      "Found 3 download candidates:\n",
      "\n",
      "  1. Text: Download Bedrock mcworld\n",
      "     Href: /project/fnaf-universe-bedrock/download/mcworld/\n",
      "     Class: ['branded-download', 'tooltip', 'tipso_style']\n",
      "\n",
      "  2. Text: JAVA\n",
      "     Href: /project/fnaf-universe-bedrock/download/mirror/748193/\n",
      "     Class: ['third-party-download', 'branded-download']\n",
      "\n",
      "  3. Text: Nuestra Se√±ora de la Sant√≠sima Trinidad, ship of the line (Free Download)\n",
      "     Href: /project/nuestra-se-ora-de-la-sant-sima-trinidad-ship-of-the-line-free-download/\n",
      "     Class: ['r-title']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate the problematic projects\n",
    "test_urls = [\n",
    "    \"https://www.planetminecraft.com/project/halloween-dropper\",\n",
    "    \"https://www.planetminecraft.com/project/fnaf-universe-bedrock\"\n",
    "]\n",
    "\n",
    "for url in test_urls:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Investigating: {url}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all links with 'download' in text or href\n",
    "    download_candidates = []\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href', '')\n",
    "        text = link.get_text(strip=True).lower()\n",
    "        \n",
    "        # Check if it's download-related\n",
    "        if 'download' in text or '/download/' in href.lower():\n",
    "            download_candidates.append({\n",
    "                'text': link.get_text(strip=True),\n",
    "                'href': href,\n",
    "                'class': link.get('class', [])\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nFound {len(download_candidates)} download candidates:\")\n",
    "    for idx, dl in enumerate(download_candidates, 1):\n",
    "        print(f\"\\n  {idx}. Text: {dl['text']}\")\n",
    "        print(f\"     Href: {dl['href']}\")\n",
    "        print(f\"     Class: {dl['class']}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc39e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated metadata extractor with better download detection and JSON structure\n",
    "def extract_project_metadata_v2(soup, project_url):\n",
    "    \"\"\"\n",
    "    Enhanced extractor with complete download detection and JSON-friendly structure\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"url\": project_url,\n",
    "        \"title\": \"N/A\",\n",
    "        \"category\": \"N/A\",\n",
    "        \"subcategory\": \"N/A\",\n",
    "        \"posted_date\": \"N/A\",\n",
    "        \"updated_date\": \"N/A\",\n",
    "        \"tags\": [],\n",
    "        \"description\": \"N/A\",\n",
    "        \"author\": \"N/A\",\n",
    "        \"stats\": {\n",
    "            \"views\": 0,\n",
    "            \"views_today\": 0,\n",
    "            \"downloads\": 0,\n",
    "            \"downloads_today\": 0,\n",
    "            \"diamonds\": 0,\n",
    "            \"hearts\": 0\n",
    "        },\n",
    "        \"downloads\": [],  # Array of download objects\n",
    "        \"has_direct_download\": False\n",
    "    }\n",
    "    \n",
    "    # Step 1: Extract from JSON-LD\n",
    "    json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    creative_work = None\n",
    "    \n",
    "    for script in json_ld_scripts:\n",
    "        try:\n",
    "            json_data = json.loads(script.string)\n",
    "            if json_data.get('@type') == 'CreativeWork':\n",
    "                creative_work = json_data\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if creative_work:\n",
    "        metadata['title'] = creative_work.get('name', 'N/A')\n",
    "        metadata['description'] = creative_work.get('description', 'N/A')\n",
    "        metadata['posted_date'] = creative_work.get('datePublished', 'N/A')\n",
    "        metadata['updated_date'] = creative_work.get('dateModified', 'N/A')\n",
    "        metadata['category'] = creative_work.get('genre', 'N/A')\n",
    "        \n",
    "        # Extract tags from keywords\n",
    "        keywords = creative_work.get('keywords', '')\n",
    "        if keywords:\n",
    "            metadata['tags'] = [tag.strip() for tag in keywords.split(',')]\n",
    "            \n",
    "            # Detect subcategory from the official list\n",
    "            valid_subcategories = [\n",
    "                '3d-art', 'air-structure', 'challenge-adventure', 'complex', 'educational',\n",
    "                'enviroment-landscaping', 'land-structure', 'minecart', 'music',\n",
    "                'nether-structure', 'piston', 'pixel-art', 'redstone-device',\n",
    "                'underground-structure', 'water-structure', 'other'\n",
    "            ]\n",
    "            \n",
    "            for tag in metadata['tags']:\n",
    "                if tag in valid_subcategories:\n",
    "                    metadata['subcategory'] = tag\n",
    "                    break\n",
    "        \n",
    "        # Extract author\n",
    "        author_data = creative_work.get('author', {})\n",
    "        if isinstance(author_data, dict):\n",
    "            metadata['author'] = author_data.get('name', 'N/A')\n",
    "    \n",
    "    # Step 2: Extract views and downloads stats\n",
    "    resource_stats = soup.find('ul', class_='resource-statistics')\n",
    "    if resource_stats:\n",
    "        li_elements = resource_stats.find_all('li')\n",
    "        \n",
    "        for li in li_elements:\n",
    "            text = li.get_text(strip=True)\n",
    "            stat_values = [s.get_text(strip=True) for s in li.find_all('span', class_='stat')]\n",
    "            \n",
    "            if 'view' in text.lower() and len(stat_values) >= 2:\n",
    "                try:\n",
    "                    metadata['stats']['views'] = int(stat_values[0].replace(',', ''))\n",
    "                    metadata['stats']['views_today'] = int(stat_values[1].replace(',', ''))\n",
    "                except:\n",
    "                    pass\n",
    "            elif 'download' in text.lower() and len(stat_values) >= 2:\n",
    "                try:\n",
    "                    metadata['stats']['downloads'] = int(stat_values[0].replace(',', ''))\n",
    "                    metadata['stats']['downloads_today'] = int(stat_values[1].replace(',', ''))\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # Step 3: Extract diamonds and hearts\n",
    "    diamond_elem = soup.find('span', class_='c-num-votes')\n",
    "    if diamond_elem:\n",
    "        try:\n",
    "            metadata['stats']['diamonds'] = int(diamond_elem.get_text(strip=True).replace(',', ''))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    heart_elem = soup.find('span', class_='c-num-favs')\n",
    "    if heart_elem:\n",
    "        try:\n",
    "            metadata['stats']['hearts'] = int(heart_elem.get_text(strip=True).replace(',', ''))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Step 4: Enhanced download link detection\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        text = link.get_text(strip=True)\n",
    "        classes = link.get('class', [])\n",
    "        \n",
    "        download_info = None\n",
    "        \n",
    "        # Direct PM downloads - Java schematic\n",
    "        if '/download/schematic/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text or 'Download Schematic',\n",
    "                'url': href,\n",
    "                'type': 'direct_pm_schematic',\n",
    "                'file_type': 'schematic',\n",
    "                'platform': 'java'\n",
    "            }\n",
    "            metadata['has_direct_download'] = True\n",
    "        \n",
    "        # Direct PM downloads - Java world\n",
    "        elif '/download/world/' in href.lower() or '/download/worldmap/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text or 'Download World',\n",
    "                'url': href,\n",
    "                'type': 'direct_pm_world',\n",
    "                'file_type': 'world',\n",
    "                'platform': 'java'\n",
    "            }\n",
    "            metadata['has_direct_download'] = True\n",
    "        \n",
    "        # Direct PM downloads - Bedrock mcworld (NEW!)\n",
    "        elif '/download/mcworld/' in href.lower():\n",
    "            download_info = {\n",
    "                'text': text or 'Download Bedrock World',\n",
    "                'url': href,\n",
    "                'type': 'direct_pm_mcworld',\n",
    "                'file_type': 'world',\n",
    "                'platform': 'bedrock'\n",
    "            }\n",
    "            metadata['has_direct_download'] = True\n",
    "        \n",
    "        # PM redirect links to external hosts\n",
    "        elif '/download/mirror/' in href.lower() or '/download/website/' in href.lower():\n",
    "            # Try to detect platform from text\n",
    "            platform = 'unknown'\n",
    "            if 'java' in text.lower():\n",
    "                platform = 'java'\n",
    "            elif 'bedrock' in text.lower():\n",
    "                platform = 'bedrock'\n",
    "            \n",
    "            download_info = {\n",
    "                'text': text or 'External Download',\n",
    "                'url': href,\n",
    "                'type': 'pm_external_redirect',\n",
    "                'file_type': 'unknown',\n",
    "                'platform': platform\n",
    "            }\n",
    "        \n",
    "        # Direct external download hosts\n",
    "        elif any(host in href.lower() for host in ['mediafire.com', 'dropbox.com', 'drive.google.com', 'mega.nz', 'patreon.com']):\n",
    "            link_type = classify_download_link_v2(href, text) if 'classify_download_link_v2' in dir() else 'external'\n",
    "            download_info = {\n",
    "                'text': text,\n",
    "                'url': href,\n",
    "                'type': link_type,\n",
    "                'file_type': 'unknown',\n",
    "                'platform': 'unknown'\n",
    "            }\n",
    "        \n",
    "        # Add download if we found one and it's not a duplicate\n",
    "        if download_info and not any(d['url'] == download_info['url'] for d in metadata['downloads']):\n",
    "            metadata['downloads'].append(download_info)\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f4beff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved extractor...\n",
      "======================================================================\n",
      "\n",
      "https://www.planetminecraft.com/project/halloween-dropper\n",
      "  Title: Halloween Dropper Minecraft Map\n",
      "  Downloads found: 1\n",
      "    1. [bedrock] direct_pm_mcworld: /project/halloween-dropper/download/mcworld/\n",
      "       Text: Download Bedrock mcworld\n",
      "\n",
      "\n",
      "https://www.planetminecraft.com/project/fnaf-universe-bedrock\n",
      "  Title: FNAF: UNIVERSE - BEDROCK Minecraft Map\n",
      "  Downloads found: 2\n",
      "    1. [bedrock] direct_pm_mcworld: /project/fnaf-universe-bedrock/download/mcworld/\n",
      "       Text: Download Bedrock mcworld\n",
      "    2. [java] pm_external_redirect: /project/fnaf-universe-bedrock/download/mirror/748193/\n",
      "       Text: JAVA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the improved extractor on the problematic projects\n",
    "print(\"Testing improved extractor...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for url in test_urls:\n",
    "    print(f\"\\n{url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    metadata = extract_project_metadata_v2(soup, url)\n",
    "    \n",
    "    print(f\"  Title: {metadata['title']}\")\n",
    "    print(f\"  Downloads found: {len(metadata['downloads'])}\")\n",
    "    \n",
    "    for idx, dl in enumerate(metadata['downloads'], 1):\n",
    "        print(f\"    {idx}. [{dl['platform']}] {dl['type']}: {dl['url']}\")\n",
    "        print(f\"       Text: {dl['text']}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63d36486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON-based scraper (better for variable-length arrays and nested data)\n",
    "def scrape_planet_minecraft_json(driver, \n",
    "                                 output_json='planet_minecraft_projects.json',\n",
    "                                 progress_file='scraping_progress.json',\n",
    "                                 start_page=1, \n",
    "                                 max_pages=None,\n",
    "                                 max_projects=None,\n",
    "                                 category=None, \n",
    "                                 share=None, \n",
    "                                 order=None,\n",
    "                                 platform=None,\n",
    "                                 page_delay=2.0,\n",
    "                                 project_delay=1.0,\n",
    "                                 resume=True):\n",
    "    \"\"\"\n",
    "    Complete scraper for Planet Minecraft - JSON output version.\n",
    "    \n",
    "    Same parameters as CSV version, but outputs to JSON with better structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize progress tracking\n",
    "    progress = {\n",
    "        'scraped_urls': set(),\n",
    "        'failed_urls': {},\n",
    "        'total_collected': 0,\n",
    "        'total_scraped': 0,\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'filters': {\n",
    "            'category': category,\n",
    "            'share': share,\n",
    "            'order': order,\n",
    "            'platform': platform\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load existing progress if resuming\n",
    "    if resume and Path(progress_file).exists():\n",
    "        print(f\"Loading progress from {progress_file}...\")\n",
    "        with open(progress_file, 'r') as f:\n",
    "            saved_progress = json.load(f)\n",
    "            progress['scraped_urls'] = set(saved_progress.get('scraped_urls', []))\n",
    "            progress['failed_urls'] = saved_progress.get('failed_urls', {})\n",
    "            print(f\"  Resuming: {len(progress['scraped_urls'])} already scraped\")\n",
    "    \n",
    "    # Load existing data if resuming\n",
    "    existing_projects = []\n",
    "    if Path(output_json).exists() and resume:\n",
    "        print(f\"Loading existing data from {output_json}...\")\n",
    "        with open(output_json, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            existing_projects = data.get('projects', [])\n",
    "            for project in existing_projects:\n",
    "                progress['scraped_urls'].add(project['url'])\n",
    "        print(f\"  Found {len(existing_projects)} existing projects\")\n",
    "    \n",
    "    # Step 1: Collect project URLs\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 1: Collecting project URLs\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    project_urls = collect_project_urls(\n",
    "        driver=driver,\n",
    "        start_page=start_page,\n",
    "        max_pages=max_pages,\n",
    "        category=category,\n",
    "        share=share,\n",
    "        order=order,\n",
    "        platform=platform,\n",
    "        delay=page_delay\n",
    "    )\n",
    "    \n",
    "    progress['total_collected'] = len(project_urls)\n",
    "    \n",
    "    # Filter out already scraped URLs\n",
    "    urls_to_scrape = [url for url in project_urls if url not in progress['scraped_urls']]\n",
    "    \n",
    "    if max_projects:\n",
    "        urls_to_scrape = urls_to_scrape[:max_projects]\n",
    "    \n",
    "    print(f\"\\nURLs to scrape: {len(urls_to_scrape)} (out of {len(project_urls)} collected)\")\n",
    "    \n",
    "    if len(urls_to_scrape) == 0:\n",
    "        print(\"No new URLs to scrape!\")\n",
    "        return progress\n",
    "    \n",
    "    # Step 2: Extract metadata from each project\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 2: Extracting project metadata\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    newly_scraped = []\n",
    "    \n",
    "    for idx, project_url in enumerate(urls_to_scrape, 1):\n",
    "        full_url = f\"https://www.planetminecraft.com{project_url}\"\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(urls_to_scrape)}] {project_url}\")\n",
    "        \n",
    "        try:\n",
    "            # Load project page\n",
    "            driver.get(full_url)\n",
    "            time.sleep(project_delay)\n",
    "            \n",
    "            # Extract metadata\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            metadata = extract_project_metadata_v2(soup, full_url)\n",
    "            \n",
    "            if metadata and metadata.get('title') != 'N/A':\n",
    "                newly_scraped.append(metadata)\n",
    "                \n",
    "                progress['scraped_urls'].add(project_url)\n",
    "                progress['total_scraped'] += 1\n",
    "                \n",
    "                print(f\"  ‚úì {metadata.get('title')} - {metadata.get('subcategory', 'N/A')}\")\n",
    "                print(f\"    Stats: {metadata['stats']['views']} views, {metadata['stats']['downloads']} downloads\")\n",
    "                print(f\"    Downloads: {len(metadata['downloads'])} link(s)\")\n",
    "            else:\n",
    "                print(f\"  ‚ö† No metadata extracted\")\n",
    "                progress['failed_urls'][project_url] = \"No metadata\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "            progress['failed_urls'][project_url] = str(e)\n",
    "        \n",
    "        # Save progress every 10 projects\n",
    "        if idx % 10 == 0:\n",
    "            # Combine existing + newly scraped\n",
    "            all_projects = existing_projects + newly_scraped\n",
    "            \n",
    "            # Save JSON\n",
    "            output_data = {\n",
    "                'metadata': {\n",
    "                    'scraped_at': datetime.now().isoformat(),\n",
    "                    'total_projects': len(all_projects),\n",
    "                    'filters': progress['filters']\n",
    "                },\n",
    "                'projects': all_projects\n",
    "            }\n",
    "            \n",
    "            with open(output_json, 'w', encoding='utf-8') as f:\n",
    "                json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Save progress\n",
    "            with open(progress_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'scraped_urls': list(progress['scraped_urls']),\n",
    "                    'failed_urls': progress['failed_urls'],\n",
    "                    'total_collected': progress['total_collected'],\n",
    "                    'total_scraped': progress['total_scraped'],\n",
    "                    'start_time': progress['start_time'],\n",
    "                    'last_update': datetime.now().isoformat(),\n",
    "                    'filters': progress['filters']\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"  üìù Progress saved ({len(all_projects)} total projects)\")\n",
    "    \n",
    "    # Final save\n",
    "    all_projects = existing_projects + newly_scraped\n",
    "    \n",
    "    output_data = {\n",
    "        'metadata': {\n",
    "            'scraped_at': datetime.now().isoformat(),\n",
    "            'total_projects': len(all_projects),\n",
    "            'filters': progress['filters'],\n",
    "            'scrape_stats': {\n",
    "                'total_collected': progress['total_collected'],\n",
    "                'successfully_scraped': progress['total_scraped'],\n",
    "                'failed': len(progress['failed_urls'])\n",
    "            }\n",
    "        },\n",
    "        'projects': all_projects\n",
    "    }\n",
    "    \n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Final progress save\n",
    "    progress['end_time'] = datetime.now().isoformat()\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'scraped_urls': list(progress['scraped_urls']),\n",
    "            'failed_urls': progress['failed_urls'],\n",
    "            'total_collected': progress['total_collected'],\n",
    "            'total_scraped': progress['total_scraped'],\n",
    "            'start_time': progress['start_time'],\n",
    "            'end_time': progress['end_time'],\n",
    "            'filters': progress['filters']\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SCRAPING COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total URLs collected: {progress['total_collected']}\")\n",
    "    print(f\"Successfully scraped: {progress['total_scraped']}\")\n",
    "    print(f\"Failed: {len(progress['failed_urls'])}\")\n",
    "    print(f\"Output saved to: {output_json}\")\n",
    "    print(f\"Progress saved to: {progress_file}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d920f",
   "metadata": {},
   "source": [
    "### Test JSON Scraper\n",
    "\n",
    "Now let's test with the same parameters but JSON output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f58e8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: Collecting project URLs\n",
      "======================================================================\n",
      "======================================================================\n",
      "Starting URL collection from page 1\n",
      "Filters: share=schematic&order=order_latest\n",
      "======================================================================\n",
      "\n",
      "Page 1: https://www.planetminecraft.com/projects/?share=schematic&order=order_latest&p=1\n",
      "  ‚úì Found 25 projects (25 new)\n",
      "  Total unique: 25\n",
      "\n",
      "Page 2: https://www.planetminecraft.com/projects/?share=schematic&order=order_latest&p=2\n",
      "  ‚úì Found 25 projects (25 new)\n",
      "  Total unique: 50\n",
      "\n",
      "Reached max_pages limit (2)\n",
      "\n",
      "======================================================================\n",
      "Collection complete: 50 unique project URLs\n",
      "======================================================================\n",
      "\n",
      "URLs to scrape: 5 (out of 50 collected)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Extracting project metadata\n",
      "======================================================================\n",
      "\n",
      "[1/5] /project/toulouse-house\n",
      "  ‚úì Toulouse house Minecraft Map - other\n",
      "    Stats: 87 views, 12 downloads\n",
      "    Downloads: 1 link(s)\n",
      "\n",
      "[2/5] /project/hamburg-building-6-by-tim0fei\n",
      "  ‚úì Hamburg Building 6 by tim0fei Minecraft Map - other\n",
      "    Stats: 55 views, 7 downloads\n",
      "    Downloads: 1 link(s)\n",
      "\n",
      "[3/5] /project/redstonefy\n",
      "  ‚úì RedstoneFy Minecraft Map - redstone-device\n",
      "    Stats: 43 views, 1 downloads\n",
      "    Downloads: 1 link(s)\n",
      "\n",
      "[4/5] /project/estate-6745324\n",
      "  ‚úì estate Minecraft Map - land-structure\n",
      "    Stats: 20 views, 4 downloads\n",
      "    Downloads: 1 link(s)\n",
      "\n",
      "[5/5] /project/five-nights-at-tails-tails-amp-friends-mapa\n",
      "  ‚úì Five Nights at Tails | Tails &amp; Friends Mapa Minecraft Map - challenge-adventure\n",
      "    Stats: 45 views, 2 downloads\n",
      "    Downloads: 1 link(s)\n",
      "\n",
      "======================================================================\n",
      "SCRAPING COMPLETE\n",
      "======================================================================\n",
      "Total URLs collected: 50\n",
      "Successfully scraped: 5\n",
      "Failed: 0\n",
      "Output saved to: ../data/planet_minecraft/test_schematics.json\n",
      "Progress saved to: ../data/planet_minecraft/test_progress_json.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test JSON scraper with small sample\n",
    "test_stats_json = scrape_planet_minecraft_json(\n",
    "    driver=driver,\n",
    "    output_json=str(test_output_dir / 'test_schematics.json'),\n",
    "    progress_file=str(test_output_dir / 'test_progress_json.json'),\n",
    "    start_page=1,\n",
    "    max_pages=2,  # Only 2 pages\n",
    "    max_projects=5,  # Only 5 projects for quick test\n",
    "    share='schematic',\n",
    "    order='order_latest',\n",
    "    page_delay=2.0,\n",
    "    project_delay=1.5,\n",
    "    resume=False  # Fresh start for JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a469df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "JSON Structure:\n",
      "======================================================================\n",
      "\n",
      "Metadata:\n",
      "  Scraped at: 2025-10-10T13:48:55.113189\n",
      "  Total projects: 5\n",
      "  Filters: {'category': None, 'share': 'schematic', 'order': 'order_latest', 'platform': None}\n",
      "\n",
      "\n",
      "First project (full structure):\n",
      "======================================================================\n",
      "{\n",
      "  \"url\": \"https://www.planetminecraft.com/project/toulouse-house\",\n",
      "  \"title\": \"Toulouse house Minecraft Map\",\n",
      "  \"category\": \"Map\",\n",
      "  \"subcategory\": \"other\",\n",
      "  \"posted_date\": \"2025-10-09T00:00:00-04:00\",\n",
      "  \"updated_date\": \"2025-10-09T14:59:47-04:00\",\n",
      "  \"tags\": [\n",
      "    \"city\",\n",
      "    \"build\",\n",
      "    \"house\",\n",
      "    \"french\",\n",
      "    \"red\",\n",
      "    \"street\",\n",
      "    \"toulouse\",\n",
      "    \"other\"\n",
      "  ],\n",
      "  \"description\": \"Here are some typical houses in the city of Toulouse, France. Nicknamed the pink city, Toulouse is the third largest city in France and the city where...\",\n",
      "  \"author\": \"Mineraf7\",\n",
      "  \"stats\": {\n",
      "    \"views\": 87,\n",
      "    \"views_today\": 87,\n",
      "    \"downloads\": 12,\n",
      "    \"downloads_today\": 12,\n",
      "    \"diamonds\": 4,\n",
      "    \"hearts\": 2\n",
      "  },\n",
      "  \"downloads\": [\n",
      "    {\n",
      "      \"text\": \"Download Minecraft Map\",\n",
      "      \"url\": \"/project/toulouse-house/download/worldmap/\",\n",
      "      \"type\": \"direct_pm_world\",\n",
      "      \"file_type\": \"world\",\n",
      "      \"platform\": \"java\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_direct_download\": true\n",
      "}\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Download links summary:\n",
      "======================================================================\n",
      "\n",
      "Toulouse house Minecraft Map\n",
      "  Subcategory: other\n",
      "  Downloads: 1 link(s)\n",
      "    - [java] direct_pm_world: /project/toulouse-house/download/worldmap/\n",
      "  Tags: city, build, house, french, red...\n",
      "\n",
      "Hamburg Building 6 by tim0fei Minecraft Map\n",
      "  Subcategory: other\n",
      "  Downloads: 1 link(s)\n",
      "    - [java] direct_pm_schematic: /project/hamburg-building-6-by-tim0fei/download/schematic/\n",
      "  Tags: city, minecraft, building, architecture, modern...\n",
      "\n",
      "RedstoneFy Minecraft Map\n",
      "  Subcategory: redstone-device\n",
      "  Downloads: 1 link(s)\n",
      "    - [java] direct_pm_schematic: /project/redstonefy/download/schematic/\n",
      "  Tags: redstone, redstone-device, music, c418, technoblade...\n",
      "\n",
      "estate Minecraft Map\n",
      "  Subcategory: land-structure\n",
      "  Downloads: 1 link(s)\n",
      "    - [java] direct_pm_schematic: /project/estate-6745324/download/schematic/\n",
      "  Tags: land-structure...\n",
      "\n",
      "Five Nights at Tails | Tails &amp; Friends Mapa Minecraft Map\n",
      "  Subcategory: challenge-adventure\n",
      "  Downloads: 1 link(s)\n",
      "    - [java] direct_pm_world: /project/five-nights-at-tails-tails-amp-friends-mapa/download/worldmap/\n",
      "  Tags: aventure, sonicthehedgehog, fnafroleplay, challenge-adventure, fnafmap...\n"
     ]
    }
   ],
   "source": [
    "# Load and display the JSON structure\n",
    "json_file = test_output_dir / 'test_schematics.json'\n",
    "\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"JSON Structure:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  Scraped at: {data['metadata']['scraped_at']}\")\n",
    "print(f\"  Total projects: {data['metadata']['total_projects']}\")\n",
    "print(f\"  Filters: {data['metadata']['filters']}\")\n",
    "\n",
    "print(f\"\\n\\nFirst project (full structure):\")\n",
    "print(\"=\" * 70)\n",
    "first_project = data['projects'][0]\n",
    "print(json.dumps(first_project, indent=2))\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Download links summary:\")\n",
    "print(\"=\" * 70)\n",
    "for project in data['projects']:\n",
    "    print(f\"\\n{project['title']}\")\n",
    "    print(f\"  Subcategory: {project['subcategory']}\")\n",
    "    print(f\"  Downloads: {len(project['downloads'])} link(s)\")\n",
    "    for dl in project['downloads']:\n",
    "        print(f\"    - [{dl['platform']}] {dl['type']}: {dl['url']}\")\n",
    "    print(f\"  Tags: {', '.join(project['tags'][:5])}...\")  # First 5 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c65fce",
   "metadata": {},
   "source": [
    "## Summary: JSON vs CSV Format\n",
    "\n",
    "### ‚úÖ JSON Format Advantages (RECOMMENDED)\n",
    "\n",
    "**Better data structure:**\n",
    "- Native arrays for tags (no comma escaping issues)\n",
    "- Native arrays for downloads (each with platform, type, file_type)\n",
    "- Nested stats object keeps related data together\n",
    "- No need for pipe separators or string concatenation\n",
    "\n",
    "**Better for variable-length data:**\n",
    "- Projects can have 1-20+ tags without formatting issues\n",
    "- Projects can have 1-10+ download links cleanly stored\n",
    "- Easy to add new fields without breaking parsing\n",
    "\n",
    "**Better for programmatic use:**\n",
    "- Direct parsing in Python/JavaScript/etc\n",
    "- Easy filtering (e.g., `downloads.platform == 'java'`)\n",
    "- Easy aggregation and analysis\n",
    "- Can be loaded into pandas: `pd.read_json()`\n",
    "\n",
    "**Example JSON structure:**\n",
    "```json\n",
    "{\n",
    "  \"title\": \"Project Name\",\n",
    "  \"tags\": [\"land-structure\", \"modern\", \"city\"],\n",
    "  \"downloads\": [\n",
    "    {\n",
    "      \"url\": \"/download/schematic/\",\n",
    "      \"type\": \"direct_pm_schematic\",\n",
    "      \"platform\": \"java\",\n",
    "      \"file_type\": \"schematic\",\n",
    "      \"text\": \"Download Schematic\"\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"/download/mcworld/\",\n",
    "      \"type\": \"direct_pm_mcworld\",\n",
    "      \"platform\": \"bedrock\",\n",
    "      \"file_type\": \"world\",\n",
    "      \"text\": \"Download Bedrock mcworld\"\n",
    "    }\n",
    "  ],\n",
    "  \"stats\": {\n",
    "    \"views\": 1234,\n",
    "    \"downloads\": 567\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### CSV Format (Still available)\n",
    "\n",
    "**When to use:**\n",
    "- Need to open in Excel/Google Sheets\n",
    "- Simpler analysis tools\n",
    "- Flat structure preferred\n",
    "\n",
    "**Limitations:**\n",
    "- Uses `|` separators for multi-value fields\n",
    "- All downloads concatenated into strings\n",
    "- Harder to filter by platform or download type\n",
    "- Tag limits or escaping issues\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "**Use JSON format for:**\n",
    "- Machine learning / data science workflows\n",
    "- Building download scripts\n",
    "- Filtering by platform (Java vs Bedrock)\n",
    "- Projects with multiple downloads\n",
    "- Long-term data collection\n",
    "\n",
    "**Use CSV format for:**\n",
    "- Quick Excel analysis\n",
    "- Simple spreadsheet workflows\n",
    "- When JSON parsing not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fbfe5",
   "metadata": {},
   "source": [
    "### Verify Fix: Test the problematic projects\n",
    "\n",
    "Let's verify the Halloween Dropper and FNAF Universe projects now have all downloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6fd4bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Verification Test: Problematic Projects\n",
      "======================================================================\n",
      "\n",
      "Scraping: /project/halloween-dropper\n",
      "  ‚úì Halloween Dropper Minecraft Map\n",
      "  Downloads found: 1\n",
      "    - [bedrock] direct_pm_mcworld\n",
      "      URL: /project/halloween-dropper/download/mcworld/\n",
      "      Text: Download Bedrock mcworld\n",
      "\n",
      "Scraping: /project/fnaf-universe-bedrock\n",
      "  ‚úì FNAF: UNIVERSE - BEDROCK Minecraft Map\n",
      "  Downloads found: 2\n",
      "    - [bedrock] direct_pm_mcworld\n",
      "      URL: /project/fnaf-universe-bedrock/download/mcworld/\n",
      "      Text: Download Bedrock mcworld\n",
      "    - [java] pm_external_redirect\n",
      "      URL: /project/fnaf-universe-bedrock/download/mirror/748193/\n",
      "      Text: JAVA\n",
      "\n",
      "‚úì Verification results saved to: ../data/planet_minecraft/verification_test.json\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION RESULT:\n",
      "======================================================================\n",
      "\n",
      "‚úì Halloween Dropper: 1 download(s)\n",
      "  ‚úÖ PASS: Has Bedrock mcworld download\n",
      "\n",
      "‚úì FNAF Universe: 2 download(s)\n",
      "  ‚úÖ PASS: Has both Bedrock mcworld and Java external link\n"
     ]
    }
   ],
   "source": [
    "# Create a mini scrape of just the two problematic projects\n",
    "verification_urls = [\n",
    "    '/project/halloween-dropper',\n",
    "    '/project/fnaf-universe-bedrock'\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Verification Test: Problematic Projects\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "verification_projects = []\n",
    "\n",
    "for url in verification_urls:\n",
    "    full_url = f\"https://www.planetminecraft.com{url}\"\n",
    "    print(f\"\\nScraping: {url}\")\n",
    "    \n",
    "    driver.get(full_url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    metadata = extract_project_metadata_v2(soup, full_url)\n",
    "    \n",
    "    verification_projects.append(metadata)\n",
    "    \n",
    "    print(f\"  ‚úì {metadata['title']}\")\n",
    "    print(f\"  Downloads found: {len(metadata['downloads'])}\")\n",
    "    for dl in metadata['downloads']:\n",
    "        print(f\"    - [{dl['platform']}] {dl['type']}\")\n",
    "        print(f\"      URL: {dl['url']}\")\n",
    "        print(f\"      Text: {dl['text']}\")\n",
    "\n",
    "# Save verification results\n",
    "verification_output = test_output_dir / 'verification_test.json'\n",
    "with open(verification_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'metadata': {\n",
    "            'test_purpose': 'Verify multi-download detection',\n",
    "            'tested_at': datetime.now().isoformat()\n",
    "        },\n",
    "        'projects': verification_projects\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úì Verification results saved to: {verification_output}\")\n",
    "\n",
    "# Check if both have multiple downloads\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION RESULT:\")\n",
    "print(\"=\" * 70)\n",
    "halloween = verification_projects[0]\n",
    "fnaf = verification_projects[1]\n",
    "\n",
    "print(f\"\\n‚úì Halloween Dropper: {len(halloween['downloads'])} download(s)\")\n",
    "if len(halloween['downloads']) >= 1:\n",
    "    print(\"  ‚úÖ PASS: Has Bedrock mcworld download\")\n",
    "else:\n",
    "    print(\"  ‚ùå FAIL: Missing downloads\")\n",
    "\n",
    "print(f\"\\n‚úì FNAF Universe: {len(fnaf['downloads'])} download(s)\")\n",
    "if len(fnaf['downloads']) >= 2:\n",
    "    print(\"  ‚úÖ PASS: Has both Bedrock mcworld and Java external link\")\n",
    "else:\n",
    "    print(\"  ‚ùå FAIL: Missing downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914609a",
   "metadata": {},
   "source": [
    "## ‚úÖ Complete Planet Minecraft Scraper - READY TO USE\n",
    "\n",
    "### What's Fixed:\n",
    "1. ‚úÖ **Download detection improved** - Now detects:\n",
    "   - `/download/schematic/` (Java schematic files)\n",
    "   - `/download/world/` or `/download/worldmap/` (Java world files)\n",
    "   - `/download/mcworld/` (Bedrock world files) **‚Üê NEW!**\n",
    "   - `/download/mirror/` or `/download/website/` (External redirects)\n",
    "   - Direct external hosts (MediaFire, Dropbox, etc.)\n",
    "\n",
    "2. ‚úÖ **Multiple downloads per project** - Correctly captures projects with multiple download options (Java + Bedrock, schematic + world, etc.)\n",
    "\n",
    "3. ‚úÖ **Platform detection** - Each download tagged with platform: `java`, `bedrock`, or `unknown`\n",
    "\n",
    "4. ‚úÖ **JSON format** - Clean nested structure for variable-length data (tags, downloads)\n",
    "\n",
    "5. ‚úÖ **Progress tracking** - Resume capability if scraping is interrupted\n",
    "\n",
    "### Available Functions:\n",
    "\n",
    "**For JSON output (RECOMMENDED):**\n",
    "```python\n",
    "scrape_planet_minecraft_json(\n",
    "    driver=driver,\n",
    "    output_json='planet_minecraft_projects.json',\n",
    "    start_page=1,\n",
    "    max_pages=100,  # Or None for unlimited\n",
    "    category='land-structure',  # Optional filter\n",
    "    share='schematic',  # Optional: 'schematic', 'seed', None\n",
    "    order='order_latest',  # Optional sort\n",
    "    platform=1,  # Optional: 1=Java, 2=Bedrock\n",
    "    page_delay=2.0,\n",
    "    project_delay=1.5,\n",
    "    resume=True\n",
    ")\n",
    "```\n",
    "\n",
    "**For CSV output:**\n",
    "```python\n",
    "scrape_planet_minecraft(  # Original CSV version\n",
    "    driver=driver,\n",
    "    output_csv='planet_minecraft_projects.csv',\n",
    "    # ... same parameters\n",
    ")\n",
    "```\n",
    "\n",
    "### Ready for Production!\n",
    "\n",
    "The scraper is now ready for full-scale scraping. You can:\n",
    "- Scrape all projects (millions of pages)\n",
    "- Filter by category, platform, or file type\n",
    "- Resume if interrupted\n",
    "- Get clean, structured data with all download links and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc501299",
   "metadata": {},
   "source": [
    "### Loading and Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bf7022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loaded 10 projects from ../data/planet_minecraft/test_schematics.csv\n",
      "======================================================================\n",
      "\n",
      "Dataset Overview:\n",
      "  Columns: ['title', 'url', 'author', 'category', 'subcategory', 'description', 'tags', 'posted_date', 'updated_date', 'views', 'views_today', 'downloads', 'downloads_today', 'diamonds', 'hearts', 'download_links', 'download_types', 'file_types']\n",
      "  Date range: 2025-10-06T00:00:00-04:00 to 2025-10-09T00:00:00-04:00\n",
      "\n",
      "Subcategory Distribution:\n",
      "subcategory\n",
      "other                    7\n",
      "land-structure           1\n",
      "underground-structure    1\n",
      "redstone-device          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 by Views:\n",
      "                                                          title             author  views  downloads           subcategory\n",
      "                         Piraeus tower Heliopolis Minecraft Map     HeliopolisCity    237         75        land-structure\n",
      "                        The Emperor&#039;s castle Minecraft Map Dimitrius von Brit    135         36                 other\n",
      "    Water tower | 1.21.1 | litematica | Grotiva | Minecraft Map   outsider-Grotiva    133         18                 other\n",
      "                                Halloween Dropper Minecraft Map       CristionWolf     99         25                 other\n",
      "    North America Survival (Brazilian Portuguese) Minecraft Map            tognozi     67          8                 other\n",
      "                        Lighting tower | Download Minecraft Map         AssasinBet     59          9                 other\n",
      "                         FNAF: UNIVERSE - BEDROCK Minecraft Map     Sir Microwaven     51          6                 other\n",
      "Fractured Femur- Mining defence training world v2 Minecraft Map      Laborleiter42     45          2 underground-structure\n",
      "                    Hamburg Building 6 by tim0fei Minecraft Map            tim0fei     38          7                 other\n",
      "                                       RedstoneFy Minecraft Map            menemen     26          1       redstone-device\n",
      "\n",
      "Top 10 by Downloads:\n",
      "                                                          title             author  views  downloads           subcategory\n",
      "                         Piraeus tower Heliopolis Minecraft Map     HeliopolisCity    237         75        land-structure\n",
      "                        The Emperor&#039;s castle Minecraft Map Dimitrius von Brit    135         36                 other\n",
      "                                Halloween Dropper Minecraft Map       CristionWolf     99         25                 other\n",
      "    Water tower | 1.21.1 | litematica | Grotiva | Minecraft Map   outsider-Grotiva    133         18                 other\n",
      "                        Lighting tower | Download Minecraft Map         AssasinBet     59          9                 other\n",
      "    North America Survival (Brazilian Portuguese) Minecraft Map            tognozi     67          8                 other\n",
      "                    Hamburg Building 6 by tim0fei Minecraft Map            tim0fei     38          7                 other\n",
      "                         FNAF: UNIVERSE - BEDROCK Minecraft Map     Sir Microwaven     51          6                 other\n",
      "Fractured Femur- Mining defence training world v2 Minecraft Map      Laborleiter42     45          2 underground-structure\n",
      "                                       RedstoneFy Minecraft Map            menemen     26          1       redstone-device\n",
      "\n",
      "File Type Distribution:\n",
      "  world: 5\n",
      "  schematic: 2\n",
      "  unknown: 2\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze scraped data\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Load the test results\n",
    "csv_file = test_output_dir / 'test_schematics.csv'\n",
    "\n",
    "if csv_file.exists():\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Loaded {len(df)} projects from {csv_file}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Date range: {df['posted_date'].min()} to {df['posted_date'].max()}\")\n",
    "    \n",
    "    print(\"\\nSubcategory Distribution:\")\n",
    "    print(df['subcategory'].value_counts())\n",
    "    \n",
    "    print(\"\\nTop 10 by Views:\")\n",
    "    top_views = df.nlargest(10, 'views')[['title', 'author', 'views', 'downloads', 'subcategory']]\n",
    "    print(top_views.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nTop 10 by Downloads:\")\n",
    "    top_downloads = df.nlargest(10, 'downloads')[['title', 'author', 'views', 'downloads', 'subcategory']]\n",
    "    print(top_downloads.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nFile Type Distribution:\")\n",
    "    # Count file types from the file_types column\n",
    "    file_type_counts = {}\n",
    "    for types_str in df['file_types'].dropna():\n",
    "        for file_type in types_str.split(' | '):\n",
    "            file_type_counts[file_type] = file_type_counts.get(file_type, 0) + 1\n",
    "    \n",
    "    for file_type, count in sorted(file_type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {file_type}: {count}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"CSV file not found: {csv_file}\")\n",
    "    print(\"Run the test scraper first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc876641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/moustholmes/miniconda3/envs/minecraft_voxel_flow/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/moustholmes/miniconda3/envs/minecraft_voxel_flow/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/moustholmes/miniconda3/envs/minecraft_voxel_flow/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f0f9f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>views</th>\n",
       "      <th>views_today</th>\n",
       "      <th>downloads</th>\n",
       "      <th>downloads_today</th>\n",
       "      <th>diamonds</th>\n",
       "      <th>hearts</th>\n",
       "      <th>download_links</th>\n",
       "      <th>download_types</th>\n",
       "      <th>file_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Piraeus tower Heliopolis Minecraft Map</td>\n",
       "      <td>https://www.planetminecraft.com/project/piraeu...</td>\n",
       "      <td>HeliopolisCity</td>\n",
       "      <td>Map</td>\n",
       "      <td>land-structure</td>\n",
       "      <td>Piraeus tower modern office skyscraper Origina...</td>\n",
       "      <td>city, land-structure, architecture, modern, sk...</td>\n",
       "      <td>2025-10-07T00:00:00-04:00</td>\n",
       "      <td>2025-10-07T10:44:05-04:00</td>\n",
       "      <td>237</td>\n",
       "      <td>71</td>\n",
       "      <td>75</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>/project/piraeus-tower-heliopolis/download/wor...</td>\n",
       "      <td>direct_pm_world</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Water tower | 1.21.1 | litematica | Grotiva | ...</td>\n",
       "      <td>https://www.planetminecraft.com/project/water-...</td>\n",
       "      <td>outsider-Grotiva</td>\n",
       "      <td>Map</td>\n",
       "      <td>other</td>\n",
       "      <td>Hello everyone, today I present to you a water...</td>\n",
       "      <td>build, building, download, buildings, buitiful...</td>\n",
       "      <td>2025-10-06T00:00:00-04:00</td>\n",
       "      <td>2025-10-06T16:23:58-04:00</td>\n",
       "      <td>133</td>\n",
       "      <td>37</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/project/water-tower-1-21-1-litematica-grotiva...</td>\n",
       "      <td>direct_pm_world</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hamburg Building 6 by tim0fei Minecraft Map</td>\n",
       "      <td>https://www.planetminecraft.com/project/hambur...</td>\n",
       "      <td>tim0fei</td>\n",
       "      <td>Map</td>\n",
       "      <td>other</td>\n",
       "      <td>Hamburg Building 6 by tim0fei. Subscribe and l...</td>\n",
       "      <td>city, minecraft, building, architecture, moder...</td>\n",
       "      <td>2025-10-08T00:00:00-04:00</td>\n",
       "      <td>2025-10-08T18:02:23-04:00</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>/project/hamburg-building-6-by-tim0fei/downloa...</td>\n",
       "      <td>direct_pm_schematic</td>\n",
       "      <td>schematic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fractured Femur- Mining defence training world...</td>\n",
       "      <td>https://www.planetminecraft.com/project/fractu...</td>\n",
       "      <td>Laborleiter42</td>\n",
       "      <td>Map</td>\n",
       "      <td>underground-structure</td>\n",
       "      <td>You might get any item you think you are able ...</td>\n",
       "      <td>underground-structure</td>\n",
       "      <td>2025-10-08T00:00:00-04:00</td>\n",
       "      <td>2025-10-08T01:28:30-04:00</td>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/project/fractured-femur-mining-defence-traini...</td>\n",
       "      <td>direct_pm_world</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Halloween Dropper Minecraft Map</td>\n",
       "      <td>https://www.planetminecraft.com/project/hallow...</td>\n",
       "      <td>CristionWolf</td>\n",
       "      <td>Map</td>\n",
       "      <td>other</td>\n",
       "      <td>Happy Halloween 4 levels theme for halloween y...</td>\n",
       "      <td>challenge, puzzle, dropper, halloween, other</td>\n",
       "      <td>2025-10-07T00:00:00-04:00</td>\n",
       "      <td>2025-10-07T05:09:11-04:00</td>\n",
       "      <td>99</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0             Piraeus tower Heliopolis Minecraft Map   \n",
       "1  Water tower | 1.21.1 | litematica | Grotiva | ...   \n",
       "2        Hamburg Building 6 by tim0fei Minecraft Map   \n",
       "3  Fractured Femur- Mining defence training world...   \n",
       "4                    Halloween Dropper Minecraft Map   \n",
       "\n",
       "                                                 url            author  \\\n",
       "0  https://www.planetminecraft.com/project/piraeu...    HeliopolisCity   \n",
       "1  https://www.planetminecraft.com/project/water-...  outsider-Grotiva   \n",
       "2  https://www.planetminecraft.com/project/hambur...           tim0fei   \n",
       "3  https://www.planetminecraft.com/project/fractu...     Laborleiter42   \n",
       "4  https://www.planetminecraft.com/project/hallow...      CristionWolf   \n",
       "\n",
       "  category            subcategory  \\\n",
       "0      Map         land-structure   \n",
       "1      Map                  other   \n",
       "2      Map                  other   \n",
       "3      Map  underground-structure   \n",
       "4      Map                  other   \n",
       "\n",
       "                                         description  \\\n",
       "0  Piraeus tower modern office skyscraper Origina...   \n",
       "1  Hello everyone, today I present to you a water...   \n",
       "2  Hamburg Building 6 by tim0fei. Subscribe and l...   \n",
       "3  You might get any item you think you are able ...   \n",
       "4  Happy Halloween 4 levels theme for halloween y...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  city, land-structure, architecture, modern, sk...   \n",
       "1  build, building, download, buildings, buitiful...   \n",
       "2  city, minecraft, building, architecture, moder...   \n",
       "3                              underground-structure   \n",
       "4       challenge, puzzle, dropper, halloween, other   \n",
       "\n",
       "                 posted_date               updated_date  views  views_today  \\\n",
       "0  2025-10-07T00:00:00-04:00  2025-10-07T10:44:05-04:00    237           71   \n",
       "1  2025-10-06T00:00:00-04:00  2025-10-06T16:23:58-04:00    133           37   \n",
       "2  2025-10-08T00:00:00-04:00  2025-10-08T18:02:23-04:00     38           38   \n",
       "3  2025-10-08T00:00:00-04:00  2025-10-08T01:28:30-04:00     45           13   \n",
       "4  2025-10-07T00:00:00-04:00  2025-10-07T05:09:11-04:00     99           26   \n",
       "\n",
       "   downloads  downloads_today  diamonds  hearts  \\\n",
       "0         75               22         6       4   \n",
       "1         18                8         1       1   \n",
       "2          7                7         4       0   \n",
       "3          2                0         1       0   \n",
       "4         25                8         2       1   \n",
       "\n",
       "                                      download_links       download_types  \\\n",
       "0  /project/piraeus-tower-heliopolis/download/wor...      direct_pm_world   \n",
       "1  /project/water-tower-1-21-1-litematica-grotiva...      direct_pm_world   \n",
       "2  /project/hamburg-building-6-by-tim0fei/downloa...  direct_pm_schematic   \n",
       "3  /project/fractured-femur-mining-defence-traini...      direct_pm_world   \n",
       "4                                                NaN                  NaN   \n",
       "\n",
       "  file_types  \n",
       "0      world  \n",
       "1      world  \n",
       "2  schematic  \n",
       "3      world  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b1d0071",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Halloween Dropper Minecraft Map'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/minecraft_voxel_flow/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Halloween Dropper Minecraft Map'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHalloween Dropper Minecraft Map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/minecraft_voxel_flow/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/minecraft_voxel_flow/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Halloween Dropper Minecraft Map'"
     ]
    }
   ],
   "source": [
    "df[\"Halloween Dropper Minecraft Map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56fa2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minecraft_voxel_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
